[
{"path": "create_secret_block.py", "encoding": "utf-8", "content": "import sys
from prefect.blocks.system import Secret
import argparse
import os

# --- Configuration ---
default_block_name = \"github-access-token\"

# --- Argument Parser ---
parser = argparse.ArgumentParser(description=\"Cria ou atualiza um Bloco Prefect do tipo Secret.\")
parser.add_argument(\"-n\", \"--name\", default=default_block_name,
                    help=f\"Nome para o Bloco Secret (padr\u00e3o: {default_block_name})\")
parser.add_argument(\"token\", help=\"O valor do token secreto a ser armazenado\")

args = parser.parse_args()

block_name = args.name
secret_value = args.token

print(f\"Tentando criar/atualizar Bloco Secret chamado '{block_name}'...\")

try:
    secret_block = Secret(value=secret_value)

    block_doc_id = secret_block.save(name=block_name, overwrite=True)

    print(f\"Bloco Secret '{block_name}' salvo com sucesso!\")
    print(f\"ID do Documento do Bloco: {block_doc_id}\")

    print(\"\n--- Pr\u00f3ximo Passo ---\")
    print(\"Verifique se o seu arquivo 'prefect.yaml' est\u00e1 referenciando o nome de bloco correto:\")
    print(f\"access_token: '{{{{ prefect.blocks.secret.{block_name} }}}}'\")
    print(\"Certifique-se de que esta linha est\u00e1 correta em TODOS os deployments que precisam do token.\")


except Exception as e:
    print(f\"\nErro ao salvar o bloco: {e}\")
    print(\"\nPoss\u00edveis causas:\")
    print(\"- O servidor Prefect Orion n\u00e3o est\u00e1 acess\u00edvel.\")
    print(\"- A vari\u00e1vel de ambiente PREFECT_API_URL n\u00e3o est\u00e1 definida ou est\u00e1 incorreta.\")
    print(\"  (Certifique-se que aponta para: http://localhost:4200/api)\")
    sys.exit(1)"}
,
{"path": "Dockerfile", "encoding": "utf-8", "content": "# ---- Etapa 1: Builder ----
    FROM python:3.12-slim as builder

    WORKDIR /app
    ENV INTERNAL_ENV=${INTERNAL_ENV} \
        PYTHONFAULTHANDLER=1 \
        PYTHONUNBUFFERED=1 \
        PYTHONHASHSEED=random \
        PYTHONDONTWRITEBYTECODE=1 \
        PIP_NO_CACHE_DIR=off \
        PIP_DISABLE_PIP_VERSION_CHECK=on \
        PIP_DEFAULT_TIMEOUT=100 \
        POETRY_NO_INTERACTION=1 \
        POETRY_VIRTUALENVS_CREATE=false \
        POETRY_CACHE_DIR='/var/cache/pypoetry' \
        POETRY_HOME='/usr/local' \
        POETRY_VERSION=2.1.1
    
    RUN apt-get update && \
        apt-get install -y curl gcc libpq-dev netcat-openbsd git && \
        rm -rf /var/lib/apt/lists/*
    
    # Instalar o Poetry e configur\u00e1-lo para n\u00e3o criar virtualenv
    RUN curl -sSL https://install.python-poetry.org | python3 -
    ENV PATH=\"$POETRY_HOME/bin:$PATH\"
    RUN poetry config virtualenvs.create false
    
    # Copiar os arquivos de depend\u00eancias e instalar os pacotes
    COPY pyproject.toml poetry.lock* ./
    RUN poetry install --no-root --no-interaction --no-ansi && \
    poetry run pip install prefect-sqlalchemy --no-cache-dir

    # Copiar o c\u00f3digo-fonte completo
    COPY . .
    
    # ---- Etapa 2: Imagem Final ----
    FROM python:3.12-slim
    
    WORKDIR /app
    ENV INTERNAL_ENV_FINAL=${INTERNAL_ENV_FINAL} \
    PYTHONPATH=/app \
    PYTHONFAULTHANDLER=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONHASHSEED=random \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=off \
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    POETRY_NO_INTERACTION=1 \
    POETRY_VIRTUALENVS_CREATE=false
    
    RUN apt-get update && \
        apt-get install -y curl gcc libpq-dev netcat-openbsd git && \
        rm -rf /var/lib/apt/lists/*
    
    # Copiar os pacotes instalados e o c\u00f3digo da etapa builder
    COPY --from=builder /usr/local /usr/local
    COPY --from=builder /app /app
    
    # Garantir que os scripts tenham permiss\u00e3o de execu\u00e7\u00e3o
    COPY infrastructure/k8s/wait-for-it.sh /app/wait-for-it.sh
    COPY infrastructure/k8s/entrypoint.sh /app/entrypoint.sh
    RUN chmod +x /app/wait-for-it.sh
    RUN chmod +x /app/entrypoint.sh
    
    # Comando de entrada
    CMD [\"/app/entrypoint.sh\"]
    "}
,
{"path": "infrastructure/monitoring/metrics_server.py", "encoding": "utf-8", "content": "from prometheus_client import start_http_server
import time
import os
import structlog

log = structlog.get_logger(__name__)

DEFAULT_PORT = 8082

def start_metrics_server(port: int = DEFAULT_PORT):
    \"\"\"Starts the Prometheus metrics HTTP server.\"\"\"
    actual_port = int(os.environ.get(\"APP_METRICS_PORT\", port))
    log.info(f\"Attempting to start Prometheus metrics server on port {actual_port}...\")
    try:

        start_http_server(actual_port)
        log.info(f\"Prometheus metrics server started successfully on port {actual_port}.\")
        while True:
            time.sleep(60)
    except OSError as e:
        log.error(f\"Failed to start metrics server on port {actual_port}. Port likely in use.\", exc_info=True)
        raise
    except Exception as e:
        log.error(\"Metrics server encountered an unexpected error\", exc_info=True)
        raise


if __name__ == \"__main__\":
    start_metrics_server()"}
,
{"path": "infrastructure/monitoring/metrics.py", "encoding": "utf-8", "content": "import os
import structlog
import psutil
from prometheus_client import (
    Counter,
    Gauge,
    Histogram,
    Summary,
    REGISTRY,
    push_to_gateway as prometheus_push,
    PROCESS_COLLECTOR,
    GC_COLLECTOR,
    PLATFORM_COLLECTOR,
)

log = structlog.get_logger(__name__)
PUSHGATEWAY_ADDRESS = os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\")
push_log = structlog.get_logger(\"push_metrics\")

# --- Counters ---
etl_counter = Counter(\"pipedrive_etl_runs_total\", \"Total ETL executions initiated\", [\"flow_type\"])
etl_failure_counter = Counter(\"pipedrive_etl_failures_total\", \"Total ETL executions that failed critically\", [\"flow_type\"])
records_processed_counter = Counter(\"pipedrive_etl_records_processed_total\", \"Total number of records successfully processed and loaded/upserted\", [\"flow_type\"])
etl_empty_batches_total = Counter(\"pipedrive_etl_empty_batches_total\", \"Total ETL batches that had no data\", [\"flow_type\"])
etl_skipped_batches_total = Counter(\"etl_skipped_batches_total\", \"Total batches skipped during ETL\", [\"flow_type\"])
etl_batch_validation_errors_total = Counter(\"pipedrive_etl_batch_validation_errors_total\", \"Total validation or transformation errors per batch\", [\"flow_type\", \"error_type\"])
etl_data_quality_issues_total = Counter(\"etl_data_quality_issues_total\", \"Data quality issues detected\", [\"flow_type\", \"field_name\", \"issue_type\"])
etl_final_column_mismatch_total = Counter(\"etl_final_column_mismatch_total\", \"Total times ETL found column mismatch and fixed schema dynamically\", [\"flow_type\"])
etl_extract_failures_total = Counter(\"etl_extract_failures_total\", \"Failures during ETL extraction phase\", [\"flow_type\"])
etl_transform_failures_total = Counter(\"etl_transform_failures_total\", \"Failures during ETL transformation phase\", [\"flow_type\"])
etl_load_failures_total = Counter(\"etl_load_failures_total\", \"Failures during ETL load phase\", [\"flow_type\"])

# API related
pipedrive_api_call_total       = Counter(\"pipedrive_api_call_total\",       \"Total Pipedrive API calls\", [\"endpoint\", \"method\", \"status_code\"])
pipedrive_api_token_cost_total = Counter(\"pipedrive_api_token_cost_total\", \"Estimated total token cost consumed for Pipedrive API calls\", [\"endpoint\"])
api_errors_counter             = Counter(\"pipedrive_api_errors_total\",     \"Total API errors by type\", [\"endpoint\", \"error_type\", \"status_code\"])
pipedrive_api_cache_hit_total  = Counter(\"pipedrive_api_cache_hit_total\",  \"Cache hit rate for Pipedrive lookups\", [\"entity\", \"source\"])
pipedrive_api_cache_miss_total = Counter(\"pipedrive_api_cache_miss_total\", \"Cache miss rate for Pipedrive lookups\", [\"entity\", \"source\"])
pipedrive_api_retries_total    = Counter(\"pipedrive_api_retries_total\",    \"Total retries attempted for Pipedrive API calls\", [\"endpoint\"])
pipedrive_api_retry_failures_total = Counter(\"pipedrive_api_retry_failures_total\", \"Failures after retry attempts for Pipedrive API calls\", [\"endpoint\"])

# --- Gauges ---
memory_usage_gauge                          = Gauge(\"pipedrive_etl_process_memory_mbytes\",      \"Peak memory usage of the ETL process run in Megabytes\", [\"flow_type\"])
etl_heartbeat                               = Gauge(\"etl_heartbeat\",                            \"Timestamp da \u00faltima execu\u00e7\u00e3o do flow\", [\"flow_type\"])
etl_cpu_usage_percent                       = Gauge(\"etl_cpu_usage_percent\",                      \"CPU usage percentage of ETL process\", [\"flow_type\"])
etl_thread_count                            = Gauge(\"etl_thread_count\",                          \"Number of active threads in ETL process\", [\"flow_type\"])
etl_disk_usage_bytes                        = Gauge(\"etl_disk_usage_bytes\",                       \"Disk usage in bytes for ETL storage\", [\"mount_point\"])
etl_pushgateway_up                          = Gauge(\"etl_pushgateway_up\",                         \"Status of pushgateway success (1 if ok)\", [\"instance\"])
db_active_connections                       = Gauge(\"db_active_connections\",                      \"N\u00famero de conex\u00f5es ativas no pool de banco\")
db_idle_connections                         = Gauge(\"db_idle_connections\",                        \"N\u00famero de conex\u00f5es ociosas no pool de banco\")
backfill_deals_remaining_gauge              = Gauge(\"pipedrive_backfill_deals_remaining_estimated\",\"Estimated number of deals remaining for stage history backfill\")
etl_last_successful_run_timestamp           = Gauge(\"etl_last_successful_run_timestamp\",          \"Timestamp (UNIX) da \u00faltima execu\u00e7\u00e3o bem-sucedida do ETL\", [\"flow_type\"])
etl_transformation_error_rate               = Gauge(\"etl_transformation_error_rate\",              \"Taxa de erro durante transforma\u00e7\u00e3o Pydantic + transforma\u00e7\u00e3o pandas\", [\"flow_type\"])
batch_size_gauge                            = Gauge(\"pipedrive_etl_batch_size\",                    \"Number of records in the current processing batch\", [\"flow_type\"])
pipedrive_api_rate_limit_remaining          = Gauge(\"pipedrive_api_rate_limit_remaining\",          \"Remaining API quota before hitting rate limit\", [\"endpoint\"])
pipedrive_api_rate_limit_reset_seconds      = Gauge(\"pipedrive_api_rate_limit_reset_seconds\",      \"Seconds until API rate limit resets\", [\"endpoint\"])

# --- Histograms ---
etl_duration_hist               = Histogram(\"pipedrive_etl_duration_seconds\",     \"Total ETL processing time\", [\"flow_type\"], buckets=[10,30,60,120,300,600,1800,3600,7200,10800])
hist_extract                    = Histogram(\"etl_extract_seconds\",              \"Tempo de extra\u00e7\u00e3o no ETL\", [\"flow_type\"])
hist_transform                  = Histogram(\"etl_transform_seconds\",            \"Tempo de transforma\u00e7\u00e3o no ETL\", [\"flow_type\"])
hist_load                       = Histogram(\"etl_load_seconds\",                 \"Tempo de carga no ETL\", [\"flow_type\"])
db_operation_duration_hist      = Histogram(\"pipedrive_db_operation_duration_seconds\",\"DB operation durations\", [\"operation\"], buckets=[0.1,0.5,1,5,10,30,60,120])
etl_loaded_records_per_batch    = Histogram(\"etl_loaded_records_per_batch\", \"Distribui\u00e7\u00e3o do n\u00famero de registros carregados por batch\", [\"flow_type\"], buckets=[0, 10, 50, 100, 200, 500, 1000, 2000])
api_request_duration_hist       = Histogram(\"pipedrive_api_request_duration_seconds\", \"Pipedrive API request durations\", [\"endpoint\", \"method\", \"status_code\"], buckets=[0.1, 0.5, 1, 2, 5, 10, 20, 30, 45, 60])

# --- Summaries ---
transform_duration_summary = Summary(\"pipedrive_transform_batch_duration_seconds\", \"Time spent transforming a batch of data\", [\"flow_type\"])

# --- Experiment Counters / Gauges ---
batch_experiment_counter     = Counter(\"pipedrive_batch_experiment_runs_total\",   \"Batch experiment executions\", [\"experiment\",\"batch_size\",\"flow_run_id\"])
batch_experiment_best_score  = Gauge(\"batch_experiment_best_score\",  \"Melhor score encontrado no experimento de batch\", [\"flow_run_id\",\"metric\"])
batch_experiment_success_rate= Gauge(\"batch_experiment_success_rate\",\"Taxa de sucesso dos experimentos por batch size\", [\"batch_size\",\"flow_run_id\"])

# Sync flows
sync_counter            = Counter(\"pipedrive_aux_sync_runs_total\", \"Total de auxiliary syncs executados\", [\"entity_type\"])
sync_failure_counter    = Counter(\"pipedrive_aux_sync_failures_total\", \"Total de auxiliary syncs que falharam\", [\"entity_type\"])
records_synced_counter  = Counter(\"pipedrive_aux_sync_records_synced_total\", \"Total de registros sincronizados\", [\"entity_type\"])

# --- Fun\u00e7\u00e3o de Push ---
def push_metrics_to_gateway(job_name=\"pipedrive_etl_job\", grouping_key=None):
    try:
        push_log.info(\"Attempting to push metrics to Pushgateway...\", address=PUSHGATEWAY_ADDRESS, job=job_name, grouping_key=grouping_key)
        prometheus_push(gateway=PUSHGATEWAY_ADDRESS, job=job_name, registry=REGISTRY, grouping_key=grouping_key)
        etl_pushgateway_up.labels(instance=PUSHGATEWAY_ADDRESS).set(1)
        push_log.info(\"Successfully pushed metrics to Pushgateway.\")
    except Exception as e:
        etl_pushgateway_up.labels(instance=PUSHGATEWAY_ADDRESS).set(0)
        push_log.error(\"Failed to push metrics to Pushgateway\", error=str(e), address=PUSHGATEWAY_ADDRESS, job=job_name, exc_info=True)
"}
,
{"path": "infrastructure/monitoring/__init__.py", "encoding": "utf-8", "content": "from .metrics import *"}
,
{"path": "infrastructure/config/settings.py", "encoding": "utf-8", "content": "import os
from dotenv import load_dotenv

load_dotenv()

class Settings:
    \"\"\"
    Configura\u00e7\u00f5es da aplica\u00e7\u00e3o, lidas de vari\u00e1veis de ambiente.
    \"\"\"
    PIPEDRIVE_API_KEY = os.getenv(\"PIPEDRIVE_API_KEY\")

    POSTGRES_DB = os.getenv(\"POSTGRES_DB\")
    POSTGRES_USER = os.getenv(\"POSTGRES_USER\")
    POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")
    POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\")
    POSTGRES_PORT = os.getenv(\"POSTGRES_PORT\")
    DATABASE_URL = os.getenv(\"DATABASE_URL\")

    REDIS_URL = os.getenv(\"REDIS_URL\")
    
    BATCH_OPTIMIZER_CONFIG = {
        'memory_threshold': 0.8,
        'reduce_factor': 0.7,
        'duration_threshold': 30,
        'increase_factor': 1.2,
        'history_window': 5
    }

settings = Settings()
"}
,
{"path": "infrastructure/config/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/db.py", "encoding": "utf-8", "content": "import psycopg2
from infrastructure.config.settings import settings

def get_db_connection():
    return psycopg2.connect(f\"postgresql://{settings.POSTGRES_USER}:{settings.POSTGRES_PASSWORD}@{settings.POSTGRES_HOST}:{settings.POSTGRES_PORT}/{settings.POSTGRES_DB}\")
"}
,
{"path": "infrastructure/db_pool.py", "encoding": "utf-8", "content": "import psycopg2.pool

class DBConnectionPool:
    def __init__(self, dsn: str, minconn: int = 1, maxconn: int = 10):
        self.pool = psycopg2.pool.SimpleConnectionPool(
            minconn=minconn,
            maxconn=maxconn,
            dsn=dsn
        )

    def get_connection(self):
        return self.pool.getconn()

    def release_connection(self, conn):
        self.pool.putconn(conn)

    def closeall(self):
        self.pool.closeall()

    def num_active(self) -> int:
        \"\"\"
        Retorna o n\u00famero de conex\u00f5es atualmente emprestadas (ativas).
        \"\"\"
        try:
            return len(self.pool._used)
        except Exception:
            return 0

    def num_idle(self) -> int:
        \"\"\"
        Retorna o n\u00famero de conex\u00f5es dispon\u00edveis (ociosas) no pool.
        \"\"\"
        try:
            return len(self.pool._pool)
        except Exception:
            return 0
"}
,
{"path": "infrastructure/repository_impl/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/repository_impl/pipedrive_repository.py", "encoding": "utf-8", "content": "import json
import csv
import random
import time as py_time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Set, Tuple
from io import StringIO

import numpy as np
import pandas as pd
from psycopg2 import sql, extras
from psycopg2.extensions import cursor as DbCursor
import structlog

from application.ports.data_repository_port import DataRepositoryPort
from application.utils.column_utils import normalize_column_name
from infrastructure.db_pool import DBConnectionPool

from infrastructure.monitoring.metrics import (
    db_active_connections,
    db_idle_connections,
    db_operation_duration_hist,
)


log = structlog.get_logger(__name__)

# Colunas da tabela principal de deals
BASE_COLUMNS = [
    \"id\", \"titulo\", \"creator_user_id\", \"creator_user_name\", \"person_id\",
    \"person_name\", \"stage_id\", \"stage_name\", \"pipeline_id\", \"pipeline_name\",
    \"owner_id\", \"owner_name\", \"status\", \"value\", \"currency\",
    \"add_time\", \"update_time\",
    \"org_id\", \"org_name\", \"lost_reason\", \"visible_to\", \"close_time\", \"won_time\", \"lost_time\",
    \"first_won_time\", \"expected_close_date\", \"probability\", \"label\",
    \"local_do_acidente\",
    \"local_do_acidente_numero_da_casa\",
    \"local_do_acidente_nome_da_rua\",
    \"local_do_acidente_distrito_sub_localidade\",
    \"local_do_acidente_cidade_municipio_vila_localidade\",
    \"local_do_acidente_estado\",
    \"local_do_acidente_regiao\",
    \"local_do_acidente_pais\",
    \"local_do_acidente_cep_codigo_postal\",
    \"local_do_acidente_latitude\",
    \"local_do_acidente_longitude\",
    \"proposta_endereco\",
    \"proposta_endereco_numero_da_casa\",
    \"proposta_endereco_nome_da_rua\",
]

NAME_COLUMNS_TO_PRESERVE = {
    \"creator_user_name\", \"person_name\", \"stage_name\", \"pipeline_name\",
    \"owner_name\", \"org_name\"
}

COLUMN_TYPES = {
    \"id\": \"TEXT PRIMARY KEY\", \"titulo\": \"TEXT\", \"creator_user_id\": \"INTEGER\",
    \"creator_user_name\": \"TEXT\", \"person_id\": \"INTEGER\", \"person_name\": \"TEXT\",
    \"stage_id\": \"INTEGER\", \"stage_name\": \"TEXT\", \"pipeline_id\": \"INTEGER\",
    \"pipeline_name\": \"TEXT\", \"owner_id\": \"INTEGER\", \"owner_name\": \"TEXT\",
    \"status\": \"TEXT\", \"value\": \"NUMERIC(18, 2)\", \"currency\": \"VARCHAR(10)\",
    \"add_time\": \"TIMESTAMPTZ\", \"update_time\": \"TIMESTAMPTZ\",
    \"org_id\": \"INTEGER\", \"org_name\": \"TEXT\", \"lost_reason\": \"TEXT\", \"visible_to\": \"TEXT\", 
    \"close_time\": \"TIMESTAMPTZ\", \"won_time\": \"TIMESTAMPTZ\", \"lost_time\": \"TIMESTAMPTZ\",
    \"first_won_time\": \"TIMESTAMPTZ\", \"expected_close_date\": \"DATE\", \"probability\": \"NUMERIC(5,2)\",
    \"label\": \"TEXT\",  \"local_do_acidente\": \"TEXT\",
    \"local_do_acidente_numero_da_casa\": \"TEXT\",
    \"local_do_acidente_nome_da_rua\": \"TEXT\",
    \"local_do_acidente_distrito_sub_localidade\": \"TEXT\",
    \"local_do_acidente_cidade_municipio_vila_localidade\": \"TEXT\",
    \"local_do_acidente_estado\": \"TEXT\",
    \"local_do_acidente_regiao\": \"TEXT\",
    \"local_do_acidente_pais\": \"TEXT\",
    \"local_do_acidente_cep_codigo_postal\": \"TEXT\",
    \"local_do_acidente_latitude\": \"TEXT\", # Ou NUMERIC se preferir
    \"local_do_acidente_longitude\": \"TEXT\", # Ou NUMERIC se preferir
     # --- Tipos para Endere\u00e7o de Proposta ---
    \"proposta_endereco\": \"TEXT\",
    \"proposta_endereco_numero_da_casa\": \"TEXT\",
    \"proposta_endereco_nome_da_rua\": \"TEXT\",
}

# Nomes das tabelas de lookup persistentes
LOOKUP_TABLE_USERS = \"pipedrive_users\"
LOOKUP_TABLE_PERSONS = \"pipedrive_persons\"
LOOKUP_TABLE_STAGES = \"pipedrive_stages\"
LOOKUP_TABLE_PIPELINES = \"pipedrive_pipelines\"
LOOKUP_TABLE_ORGANIZATIONS = \"pipedrive_organizations\"
UNKNOWN_NAME = \"Desconhecido\"

class PipedriveRepository(DataRepositoryPort):
    TABLE_NAME = \"pipedrive_data\"
    CONFIG_TABLE_NAME = \"config\"
    STAGING_TABLE_PREFIX = \"staging_pipedrive_\"
    STAGE_HISTORY_COLUMN_PREFIX = \"moved_to_stage_\"
    SCHEMA_LOCK_ID = 47835

    def __init__(
        self,
        db_pool: DBConnectionPool,
        custom_field_api_mapping: Dict[str, str],
        all_stages_details: List[Dict]
    ):
        self.db_pool = db_pool
        self.log = log.bind(repository=\"PipedriveRepository\")
        self._raw_custom_field_mapping = custom_field_api_mapping
        self._all_stages_details = all_stages_details

        # Preparar colunas din\u00e2micas
        self._custom_columns_dict = self._prepare_custom_columns(custom_field_api_mapping)
        self._stage_history_columns_dict = self._prepare_stage_history_columns(
            all_stages_details,
            existing_custom_cols=set(self._custom_columns_dict.keys())
        )

        self.ensure_schema_exists()

    @property
    def custom_field_mapping(self) -> Dict[str, str]:
        \"\"\"Retorna o mapeamento de custom fields.\"\"\"
        return self._raw_custom_field_mapping

    # --- M\u00e9todos Auxiliares de Prepara\u00e7\u00e3o de Colunas ---

    def _prepare_custom_columns(self, api_mapping: Dict[str, str]) -> Dict[str, str]:
        \"\"\"Prepara os nomes e tipos de colunas customizadas, ignorando duplicatas ou nomes inv\u00e1lidos.\"\"\"
        custom_cols = {}
        base_col_set = set(BASE_COLUMNS)
        reserved_prefixes = (self.STAGE_HISTORY_COLUMN_PREFIX,)

        for api_key, normalized_name in api_mapping.items():
            if not normalized_name or normalized_name == \"_invalid_normalized_name\":
                self.log.warning(\"Campo customizado com nome normalizado inv\u00e1lido; ignorando.\",
                                 api_key=api_key, normalized_name=normalized_name)
                continue

            if normalized_name in base_col_set:
                self.log.warning(\"Nome normalizado do custom field conflita com coluna base; ignorando.\",
                                 api_key=api_key, normalized_name=normalized_name)
                continue

            if any(normalized_name.startswith(prefix) for prefix in reserved_prefixes):
                self.log.warning(\"Nome normalizado do custom field usa prefixo reservado; ignorando.\",
                                 api_key=api_key, normalized_name=normalized_name)
                continue

            custom_cols[normalized_name] = \"TEXT\"
        self.log.info(\"Custom columns preparadas\", count=len(custom_cols))
        return custom_cols

    def _prepare_stage_history_columns(self, all_stages: List[Dict], existing_custom_cols: Set[str]) -> Dict[str, str]:
        \"\"\"
        Prepara colunas de hist\u00f3rico de stage, garantindo 1 coluna por stage_id.
        Nome da coluna = moved_to_stage_<normalized_name>_<stage_id> (para garantir unicidade).
        \"\"\"
        stage_history_cols = {}
        base_col_set = set(BASE_COLUMNS)
        forbidden_names = base_col_set.union(existing_custom_cols)

        for stage in all_stages:
            stage_id = stage.get(\"id\")
            stage_name = stage.get(\"name\")

            if not stage_id or not stage_name:
                self.log.warning(\"Stage inv\u00e1lido (sem id ou nome)\", stage_data=stage)
                continue

            normalized = normalize_column_name(stage_name)
            if not normalized or normalized == \"_invalid_normalized_name\":
                self.log.warning(\"Stage com nome inv\u00e1lido ao normalizar\", stage_id=stage_id, stage_name=stage_name)
                continue

            final_col_name = f\"{self.STAGE_HISTORY_COLUMN_PREFIX}{normalized}_{stage_id}\"

            if final_col_name in forbidden_names:
                self.log.error(\"Nome de coluna de hist\u00f3rico colide com base/custom\", final_col_name=final_col_name)
                continue

            stage_history_cols[final_col_name] = \"TIMESTAMPTZ\"

        self.log.info(\"Colunas de hist\u00f3rico geradas (sem sufixos duplicados)\", count=len(stage_history_cols))
        return stage_history_cols

    def _get_all_columns(self) -> List[str]:
        \"\"\"Retorna a lista completa de colunas (base + custom + hist\u00f3rico).\"\"\"
        return BASE_COLUMNS + sorted(list(self._custom_columns_dict.keys())) + sorted(list(self._stage_history_columns_dict.keys()))

    # --- M\u00e9todos de Defini\u00e7\u00e3o de Schema ---
    # Agora cada m\u00e9todo de defini\u00e7\u00e3o retorna uma tupla: (lista de SQL, dicion\u00e1rio esperado)

    def _get_main_table_column_definitions(self) -> Tuple[List[sql.SQL], Dict[str, str]]:
        \"\"\"Gera defini\u00e7\u00f5es SQL para a tabela principal.\"\"\"
        defs = []
        expected = {}
        all_column_types = {**COLUMN_TYPES, **self._custom_columns_dict, **self._stage_history_columns_dict}
        ordered_cols = self._get_all_columns()

        for col in ordered_cols:
            col_type = all_column_types.get(col, \"TEXT\")
            defs.append(sql.SQL(\"{} {}\").format(sql.Identifier(col), sql.SQL(col_type)))
            expected[col] = col_type

        return defs, expected

    def _get_config_column_definitions(self) -> Tuple[List[sql.SQL], Dict[str, str]]:
        \"\"\"Gera defini\u00e7\u00f5es para a tabela de configura\u00e7\u00e3o.\"\"\"
        defs = [
            sql.SQL(\"key TEXT PRIMARY KEY\"),
            sql.SQL(\"value JSONB\"),
            sql.SQL(\"updated_at TIMESTAMPTZ DEFAULT NOW()\")
        ]
        expected = {
            \"key\": \"TEXT PRIMARY KEY\",
            \"value\": \"JSONB\",
            \"updated_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
        }
        return defs, expected

    def _get_lookup_table_definitions(self, table_name: str) -> Tuple[List[sql.SQL], Dict[str, str]]:
        \"\"\"Gera defini\u00e7\u00f5es SQL para tabelas de lookup.\"\"\"
        if table_name == LOOKUP_TABLE_USERS:
            defs = [
                sql.SQL(\"user_id INTEGER PRIMARY KEY\"),
                sql.SQL(\"user_name TEXT\"),
                sql.SQL(\"is_active BOOLEAN\"),
                sql.SQL(\"last_synced_at TIMESTAMPTZ DEFAULT NOW()\")
            ]
            expected = {
                \"user_id\": \"INTEGER PRIMARY KEY\",
                \"user_name\": \"TEXT\",
                \"is_active\": \"BOOLEAN\",
                \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
            }
            return defs, expected
        elif table_name == LOOKUP_TABLE_PERSONS:
            defs = [
                sql.SQL(\"person_id INTEGER PRIMARY KEY\"),
                sql.SQL(\"person_name TEXT\"),
                sql.SQL(\"org_id INTEGER\"),
                sql.SQL(\"last_synced_at TIMESTAMPTZ DEFAULT NOW()\")
            ]
            expected = {
                \"person_id\": \"INTEGER PRIMARY KEY\",
                \"person_name\": \"TEXT\",
                \"org_id\": \"INTEGER\",
                \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
            }
            return defs, expected
        elif table_name == LOOKUP_TABLE_STAGES:
            defs = [
                sql.SQL(\"stage_id INTEGER PRIMARY KEY\"),
                sql.SQL(\"stage_name TEXT\"),
                sql.SQL(\"normalized_name TEXT\"),
                sql.SQL(\"pipeline_id INTEGER\"), 
                sql.SQL(\"order_nr INTEGER\"),
                sql.SQL(\"is_active BOOLEAN\"),
                sql.SQL(\"last_synced_at TIMESTAMPTZ DEFAULT NOW()\")
            ]
            expected = {
                \"stage_id\": \"INTEGER PRIMARY KEY\",
                \"stage_name\": \"TEXT\",
                \"normalized_name\": \"TEXT\",
                \"pipeline_id\": \"INTEGER\",
                \"order_nr\": \"INTEGER\",
                \"is_active\": \"BOOLEAN\",
                \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
            }
            return defs, expected
        elif table_name == LOOKUP_TABLE_PIPELINES:
            defs = [
                sql.SQL(\"pipeline_id INTEGER PRIMARY KEY\"),
                sql.SQL(\"pipeline_name TEXT\"),
                sql.SQL(\"is_active BOOLEAN\"),
                sql.SQL(\"last_synced_at TIMESTAMPTZ DEFAULT NOW()\")
            ]
            expected = {
                \"pipeline_id\": \"INTEGER PRIMARY KEY\",
                \"pipeline_name\": \"TEXT\",
                \"is_active\": \"BOOLEAN\",
                \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
            }
            return defs, expected
        elif table_name == LOOKUP_TABLE_ORGANIZATIONS:
            defs = [
                sql.SQL(\"org_id INTEGER PRIMARY KEY\"),
                sql.SQL(\"org_name TEXT\"),
                sql.SQL(\"last_synced_at TIMESTAMPTZ DEFAULT NOW()\")
            ]
            expected = {
                \"org_id\": \"INTEGER PRIMARY KEY\",
                \"org_name\": \"TEXT\",
                \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
            }
            return defs, expected
        else:
            raise ValueError(f\"Unknown lookup table name: {table_name}\")

    def ensure_schema_exists(self):
        \"\"\"Garante que as tabelas (principal, config e lookups) e \u00edndices existam.\"\"\"
        conn = None
        locked = False
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"ensure_schema_exists\").time():
                with conn.cursor() as cur:
                    # 1. Adquire lock para modifica\u00e7\u00e3o do schema
                    cur.execute(\"SELECT pg_advisory_lock(%s)\", (self.SCHEMA_LOCK_ID,))
                    locked = True
                    self.log.info(\"Acquired schema modification lock.\", lock_id=self.SCHEMA_LOCK_ID)

                    # 2. Criar/Alterar tabela principal
                    self._create_or_alter_table(cur, self.TABLE_NAME, self._get_main_table_column_definitions)

                    # 3. Criar/Alterar tabela de configura\u00e7\u00e3o
                    self._create_or_alter_table(cur, self.CONFIG_TABLE_NAME, self._get_config_column_definitions)

                    # 4. Criar/Alterar tabelas de lookup
                    lookup_tables = [
                        LOOKUP_TABLE_USERS, LOOKUP_TABLE_PERSONS, LOOKUP_TABLE_STAGES,
                        LOOKUP_TABLE_PIPELINES, LOOKUP_TABLE_ORGANIZATIONS
                    ]
                    for table in lookup_tables:
                        self._create_or_alter_table(cur, table, lambda t=table: self._get_lookup_table_definitions(t))

                    # 5. Criar \u00edndices
                    self._create_indexes(cur)

                    conn.commit()
                    self.log.info(\"Schema check/modification committed for all tables.\")

        except Exception as e:
            if conn:
                conn.rollback()
            self.log.critical(\"Failed to ensure database schema\", exc_info=True)
            raise
        finally:
            if conn:
                if locked:
                    try:
                        with conn.cursor() as unlock_cur:
                            unlock_cur.execute(\"SELECT pg_advisory_unlock(%s)\", (self.SCHEMA_LOCK_ID,))
                        conn.commit()
                        self.log.info(\"Released schema modification lock.\", lock_id=self.SCHEMA_LOCK_ID)
                    except Exception as unlock_err:
                        self.log.error(\"Failed to release schema lock\", error=str(unlock_err))
                self.db_pool.release_connection(conn)

    def _create_or_alter_table(self, cur: DbCursor, table_name: str, get_col_defs_func: callable):
        \"\"\"
        Fun\u00e7\u00e3o gen\u00e9rica para criar uma tabela ou adicionar colunas faltantes.
        Utiliza o dicion\u00e1rio 'expected_columns_map' retornado pela fun\u00e7\u00e3o de defini\u00e7\u00e3o de colunas.
        \"\"\"
        table_id = sql.Identifier(table_name)
        log_ctx = self.log.bind(table_name=table_name)
        log_ctx.debug(\"Starting schema check/update for table.\")

        # Verifica se a tabela existe
        cur.execute(\"\"\"
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = 'public' AND table_name = %s
            );
        \"\"\", (table_name,))
        table_exists = cur.fetchone()[0]

        # Obt\u00e9m as defini\u00e7\u00f5es de colunas e o mapeamento esperado
        col_defs_result = get_col_defs_func()
        if isinstance(col_defs_result, tuple) and len(col_defs_result) == 2:
            column_defs_sql, expected_columns_map = col_defs_result
        else:
            column_defs_sql = col_defs_result
            expected_columns_map = {}
            for col_def in column_defs_sql:
                col_def_str = col_def.as_string(cur)
                tokens = col_def_str.split()
                if tokens:
                    col_name = tokens[0].strip('\"')
                    expected_columns_map[col_name] = \" \".join(tokens[1:])

        if not table_exists:
            log_ctx.info(\"Table does not exist, creating.\")
            if not column_defs_sql:
                raise RuntimeError(f\"Cannot create table '{table_name}' with no column definitions.\")
            create_sql = sql.SQL(\"CREATE TABLE {table} ({columns})\").format(
                table=table_id,
                columns=sql.SQL(',\n    ').join(column_defs_sql)
            )
            log_ctx.debug(\"Executing CREATE TABLE\", sql_query=create_sql.as_string(cur))
            cur.execute(create_sql)
            log_ctx.info(\"Table created successfully.\")
        else:
            log_ctx.debug(\"Table exists, checking for missing columns.\")
            cur.execute(\"\"\"
                SELECT column_name FROM information_schema.columns
                WHERE table_schema = 'public' AND table_name = %s;
            \"\"\", (table_name,))
            existing_columns = {row[0] for row in cur.fetchall()}
            missing_columns = set(expected_columns_map.keys()) - existing_columns

            if missing_columns:
                log_ctx.info(\"Adding missing columns to table.\", missing=sorted(list(missing_columns)))
                alter_statements = []
                for col in sorted(missing_columns):
                    col_type = expected_columns_map[col]
                    alter_statements.append(
                        sql.SQL(\"ADD COLUMN IF NOT EXISTS {} {}\").format(
                            sql.Identifier(col),
                            sql.SQL(col_type)
                        )
                    )
                if alter_statements:
                    alter_sql = sql.SQL(\"ALTER TABLE {table} \").format(table=table_id) + sql.SQL(', ').join(alter_statements)
                    try:
                        log_ctx.debug(\"Executing ALTER TABLE ADD COLUMN(s)\", columns=sorted(list(missing_columns)))
                        cur.execute(alter_sql)
                    except Exception as alter_err:
                        log_ctx.warning(\"Bulk ALTER TABLE failed, attempting one by one.\", error=str(alter_err))
                        conn = cur.connection
                        if conn:
                            conn.rollback()
                        for stmt in alter_statements:
                            single_alter_sql = sql.SQL(\"ALTER TABLE {table} \").format(table=table_id) + stmt
                            try:
                                log_ctx.debug(\"Executing ALTER TABLE ADD COLUMN (single)\", statement=stmt.as_string(cur))
                                cur.execute(single_alter_sql)
                            except Exception as single_alter_err:
                                log_ctx.error(\"Failed to add column individually\", column=col, error=str(single_alter_err))
                        if conn:
                            conn.commit()
                log_ctx.info(\"Missing columns check/addition process completed.\", count=len(missing_columns))
            else:
                log_ctx.debug(\"No missing columns found.\")

    def _create_indexes(self, cur: DbCursor):
        \"\"\"Cria os \u00edndices padr\u00e3o para a tabela principal e as tabelas de lookup.\"\"\"
        self.log.debug(\"Starting index creation process.\")

        main_table_indexes = {
            f\"idx_{self.TABLE_NAME}_update_time\": sql.SQL(\"(update_time DESC)\"),
            f\"idx_{self.TABLE_NAME}_stage_id\": sql.SQL(\"(stage_id)\"),
            f\"idx_{self.TABLE_NAME}_pipeline_id\": sql.SQL(\"(pipeline_id)\"),
            f\"idx_{self.TABLE_NAME}_person_id\": sql.SQL(\"(person_id)\"),
            f\"idx_{self.TABLE_NAME}_org_id\": sql.SQL(\"(org_id)\"),
            f\"idx_{self.TABLE_NAME}_owner_id\": sql.SQL(\"(owner_id)\"),
            f\"idx_{self.TABLE_NAME}_creator_user_id\": sql.SQL(\"(creator_user_id)\"),
            f\"idx_{self.TABLE_NAME}_status\": sql.SQL(\"(status)\"),
            f\"idx_{self.TABLE_NAME}_add_time\": sql.SQL(\"(add_time DESC)\"),
            f\"idx_{self.TABLE_NAME}_active_deals_update\": sql.SQL(\"(update_time DESC) WHERE status NOT IN ('Ganho', 'Perdido', 'Deletado')\")
        }
        self._apply_indexes(cur, self.TABLE_NAME, main_table_indexes)

        lookup_tables_indexes = {
            LOOKUP_TABLE_USERS: { f\"idx_{LOOKUP_TABLE_USERS}_name\": sql.SQL(\"(user_name text_pattern_ops)\") },
            LOOKUP_TABLE_PERSONS: { f\"idx_{LOOKUP_TABLE_PERSONS}_name\": sql.SQL(\"(person_name text_pattern_ops)\") },
            LOOKUP_TABLE_STAGES: { f\"idx_{LOOKUP_TABLE_STAGES}_norm_name\": sql.SQL(\"(normalized_name)\") },
            LOOKUP_TABLE_PIPELINES: {},
            LOOKUP_TABLE_ORGANIZATIONS: { f\"idx_{LOOKUP_TABLE_ORGANIZATIONS}_name\": sql.SQL(\"(org_name text_pattern_ops)\") },
        }
        for table_name, indexes in lookup_tables_indexes.items():
            self._apply_indexes(cur, table_name, indexes)

        self.log.debug(\"Index creation process completed.\")

    def _apply_indexes(self, cur: DbCursor, table_name: str, indexes_to_create: Dict[str, sql.SQL]):
        \"\"\"Helper para cria\u00e7\u00e3o de \u00edndices em uma tabela espec\u00edfica.\"\"\"
        table_id = sql.Identifier(table_name)
        log_ctx = self.log.bind(table_name=table_name)
        for idx_name, idx_definition in indexes_to_create.items():
            cur.execute(\"\"\"
                SELECT EXISTS (
                    SELECT FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace
                    WHERE c.relname = %s AND c.relkind = 'i' AND n.nspname = 'public'
                );
            \"\"\", (idx_name,))
            index_exists = cur.fetchone()[0]

            if not index_exists:
                create_idx_sql = sql.SQL(\"CREATE INDEX IF NOT EXISTS {} ON {} {}\").format(
                    sql.Identifier(idx_name),
                    table_id,
                    idx_definition
                )
                try:
                    log_ctx.debug(\"Creating index.\", index_name=idx_name)
                    cur.execute(create_idx_sql)
                except Exception as idx_err:
                    log_ctx.warning(\"Failed to create index\", index_name=idx_name, error=str(idx_err))
            else:
                log_ctx.debug(\"Index already exists.\", index_name=idx_name)

    # --- M\u00e9todos de Upsert para Tabelas de Lookup ---
    def _upsert_lookup_data(self, table_name: str, data: List[Dict], id_column: str, columns: List[str]):
        \"\"\"Fun\u00e7\u00e3o gen\u00e9rica para fazer upsert em tabelas de lookup.\"\"\"
        if not data:
            self.log.debug(\"No data provided for lookup upsert\", table_name=table_name)
            return 0

        conn = None
        start_time = py_time.monotonic()
        rows_affected = 0
        table_id = sql.Identifier(table_name)
        col_ids = sql.SQL(', ').join(map(sql.Identifier, columns))
        update_cols = sql.SQL(', ').join(
            sql.SQL(\"{col} = EXCLUDED.{col}\").format(col=sql.Identifier(col))
            for col in columns if col != id_column # N\u00e3o atualiza o ID
        )
        # Adicionar atualiza\u00e7\u00e3o de last_synced_at
        update_cols += sql.SQL(\", last_synced_at = NOW()\")

        upsert_sql_template = sql.SQL(\"\"\"
            INSERT INTO {table} ({insert_cols})
            VALUES %s
            ON CONFLICT ({pk_col}) DO UPDATE SET {update_assignments}
        \"\"\").format(
            table=table_id,
            insert_cols=col_ids,
            pk_col=sql.Identifier(id_column),
            update_assignments=update_cols
        )

        # Preparar dados como tuplas na ordem das colunas
        values_tuples = []
        for record in data:
            row = []
            for col in columns:
                val = record.get(col)
                # Tratamento b\u00e1sico de tipos para SQL
                if isinstance(val, datetime):
                    val = val.isoformat()
                elif val is None:
                    val = None # psycopg2 lida com None -> NULL
                # TODO: Adicionar mais tratamentos se necess\u00e1rio (booleanos, etc.)
                row.append(val)
            values_tuples.append(tuple(row))

        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"_upsert_lookup_data\").time():
                with conn.cursor() as cur:
                    # Usar execute_values para efici\u00eancia
                    extras.execute_values(cur, upsert_sql_template.as_string(cur), values_tuples, page_size=500)
                    rows_affected = cur.rowcount
                    conn.commit()
                    duration = py_time.monotonic() - start_time
                    self.log.info(\"Upsert successful for lookup table\", table_name=table_name,
                    records_processed=len(data), rows_affected=rows_affected, duration_sec=f\"{duration:.3f}s\")
                    return rows_affected
        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Upsert failed for lookup table\", table_name=table_name, error=str(e), record_count=len(data), exc_info=True)
            raise
        finally:
            if conn:
                self.db_pool.release_connection(conn)

    # M\u00e9todos espec\u00edficos por tabela de lookup
    def upsert_users(self, data: List[Dict]):
        \"\"\"Faz upsert na tabela pipedrive_users.\"\"\"
        cols = ['user_id', 'user_name', 'is_active'] # Colunas esperadas no dict `data`
        # Mapear 'id' da API para 'user_id', 'name' para 'user_name', 'active_flag' para 'is_active'
        mapped_data = [
            {'user_id': r['id'], 'user_name': r.get('name', UNKNOWN_NAME), 'is_active': r.get('active_flag', True)}
            for r in data if 'id' in r
        ]
        return self._upsert_lookup_data(LOOKUP_TABLE_USERS, mapped_data, 'user_id', cols)

    def upsert_persons(self, data: List[Dict]):
        \"\"\"Faz upsert na tabela pipedrive_persons.\"\"\"
        cols = ['person_id', 'person_name', 'org_id'] # Ajustar se sincronizar mais campos
        mapped_data = [
            {
                'person_id': r['id'],
                'person_name': r.get('name', UNKNOWN_NAME),
                # Pega o ID da org se for um dict, sen\u00e3o None
                'org_id': r.get('org_id', {}).get('value') if isinstance(r.get('org_id'), dict) else r.get('org_id')
            }
            for r in data if 'id' in r
        ]
        return self._upsert_lookup_data(LOOKUP_TABLE_PERSONS, mapped_data, 'person_id', cols)

    def upsert_stages(self, data: List[Dict]):
        \"\"\"Faz upsert na tabela pipedrive_stages.\"\"\"
        cols = ['stage_id', 'stage_name', 'normalized_name', 'pipeline_id', 'order_nr', 'is_active']
        mapped_data = []
        for r in data:
            if 'id' in r:
                original_name = r.get('name', UNKNOWN_NAME) or UNKNOWN_NAME
                prefix = self.STAGE_HISTORY_COLUMN_PREFIX
                if original_name.startswith(prefix):
                    original_name = original_name[len(prefix):]

                normalized = normalize_column_name(original_name) if original_name != UNKNOWN_NAME else None

                mapped_data.append({
                    'stage_id': r['id'],
                    'stage_name': original_name,     
                    'normalized_name': normalized,    
                    'pipeline_id': r.get('pipeline_id'),
                    'order_nr': r.get('order_nr'),
                    'is_active': r.get('active_flag', True)
                })
        return self._upsert_lookup_data(LOOKUP_TABLE_STAGES, mapped_data, 'stage_id', cols)

    def upsert_pipelines(self, data: List[Dict]):
        \"\"\"Faz upsert na tabela pipedrive_pipelines.\"\"\"
        cols = ['pipeline_id', 'pipeline_name', 'is_active']
        mapped_data = [
            {
                'pipeline_id': r['id'],
                'pipeline_name': r.get('name', UNKNOWN_NAME),
                'is_active': r.get('active_flag', True) # Verificar nome correto do campo na API
            }
             for r in data if 'id' in r
        ]
        return self._upsert_lookup_data(LOOKUP_TABLE_PIPELINES, mapped_data, 'pipeline_id', cols)

    def upsert_organizations(self, data: List[Dict]):
        \"\"\"Faz upsert na tabela pipedrive_organizations.\"\"\"
        cols = ['org_id', 'org_name'] # Ajustar se sincronizar mais campos
        mapped_data = [
            {'org_id': r['id'], 'org_name': r.get('name', UNKNOWN_NAME)}
            for r in data if 'id' in r
        ]
        return self._upsert_lookup_data(LOOKUP_TABLE_ORGANIZATIONS, mapped_data, 'org_id', cols)
    
    # --- M\u00e9todo de Consulta para o ETL Principal ---
    def get_lookup_maps_for_batch(
        self,
        user_ids: Optional[Set[int]] = None,
        person_ids: Optional[Set[int]] = None,
        stage_ids: Optional[Set[int]] = None,
        pipeline_ids: Optional[Set[int]] = None,
        org_ids: Optional[Set[int]] = None
        ) -> Dict[str, Dict[int, str]]:
        \"\"\"
        Busca nomes das tabelas de lookup persistentes para os IDs fornecidos.
        Retorna um dicion\u00e1rio onde as chaves s\u00e3o 'users', 'persons', etc.,
        e os valores s\u00e3o dicion\u00e1rios {id: name}.
        \"\"\"
        results = {
            'users': {}, 'persons': {}, 'stages': {}, 'pipelines': {}, 'orgs': {}
        }
        if not any([user_ids, person_ids, stage_ids, pipeline_ids, org_ids]):
            return results 

        conn = None
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"get_lookup_maps_for_batch\").time():
                with conn.cursor() as cur:
                    if user_ids:
                        cur.execute(sql.SQL(\"SELECT user_id, user_name FROM {} WHERE user_id = ANY(%s)\").format(sql.Identifier(LOOKUP_TABLE_USERS)), (list(user_ids),))
                        results['users'] = {row[0]: row[1] for row in cur.fetchall()}
                    if person_ids:
                        cur.execute(sql.SQL(\"SELECT person_id, person_name FROM {} WHERE person_id = ANY(%s)\").format(sql.Identifier(LOOKUP_TABLE_PERSONS)), (list(person_ids),))
                        results['persons'] = {row[0]: row[1] for row in cur.fetchall()}
                    if stage_ids:
                        cur.execute(sql.SQL(\"SELECT stage_id, normalized_name FROM {} WHERE stage_id = ANY(%s)\").format(sql.Identifier(LOOKUP_TABLE_STAGES)), (list(stage_ids),))
                        results['stages'] = {row[0]: row[1] for row in cur.fetchall() if row[1]}
                    if pipeline_ids:
                        cur.execute(sql.SQL(\"SELECT pipeline_id, pipeline_name FROM {} WHERE pipeline_id = ANY(%s)\").format(sql.Identifier(LOOKUP_TABLE_PIPELINES)), (list(pipeline_ids),))
                        results['pipelines'] = {row[0]: row[1] for row in cur.fetchall()}
                    if org_ids:
                        cur.execute(sql.SQL(\"SELECT org_id, org_name FROM {} WHERE org_id = ANY(%s)\").format(sql.Identifier(LOOKUP_TABLE_ORGANIZATIONS)), (list(org_ids),))
                        results['orgs'] = {row[0]: row[1] for row in cur.fetchall()}

            self.log.debug(\"Fetched lookup maps for batch from DB\",
                           user_count=len(results['users']), person_count=len(results['persons']),
                           stage_count=len(results['stages']), pipeline_count=len(results['pipelines']),
                           org_count=len(results['orgs']))
            return results

        except Exception as e:
            self.log.error(\"Failed to get lookup maps for batch\", exc_info=True)
            return {'users': {}, 'persons': {}, 'stages': {}, 'pipelines': {}, 'orgs': {}}
        finally:
            if conn:
                self.db_pool.release_connection(conn)


    def _record_to_csv_line(self, record: Dict, columns: List[str]) -> str:
        \"\"\"Converts a dictionary record to a CSV string line for COPY.\"\"\"
        output = StringIO()
        writer = csv.writer(output, delimiter='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        row = []
        for field in columns:
            value = record.get(field)
            if value is None:
                row.append('\\N')
            elif isinstance(value, datetime):
                 row.append(value.isoformat())
            elif isinstance(value, bool):
                 row.append('t' if value else 'f')
            else:
                str_value = str(value)
                escaped_value = str_value.replace('|', '\\|').replace('\n', '\\n').replace('\r', '\\r').replace('\\', '\\\\')
                row.append(escaped_value)
                
        writer.writerow(row)
        csv_line = output.getvalue().strip('\n')
        output.close()
        return csv_line

    # --- M\u00e9todos da Tabela Principal (pipedrive_data) ---

    def save_data_upsert(self, data: List[Dict]):
        \"\"\"Upsert eficiente para a tabela principal `pipedrive_data` (c\u00f3digo original mantido).\"\"\"
        if not data:
            self.log.debug(\"No data provided to save_data_upsert, skipping.\")
            return

        conn = None
        start_time = py_time.monotonic()
        columns = self._get_all_columns() 
        unique_suffix = f\"{int(py_time.time())}_{random.randint(1000, 9999)}\"
        staging_table_name = f\"{self.STAGING_TABLE_PREFIX}{unique_suffix}\"
        staging_table_id = sql.Identifier(staging_table_name)
        target_table_id = sql.Identifier(self.TABLE_NAME)
        record_count = len(data)
        self.log.debug(\"Starting upsert process for main table\", record_count=record_count, column_count=len(columns), target_table=self.TABLE_NAME)

        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"save_data_upsert\").time():
                with conn.cursor() as cur:
                    # 1. Criar tabela de staging UNLOGGED (mais r\u00e1pida)
                    staging_col_defs = [sql.SQL(\"{} TEXT\").format(sql.Identifier(col)) for col in columns]
                    create_staging_sql = sql.SQL(\"\"\"
                        CREATE UNLOGGED TABLE {staging_table} ( {columns} )
                    \"\"\").format(
                        staging_table=staging_table_id,
                        columns=sql.SQL(',\n    ').join(staging_col_defs)
                    )
                    self.log.debug(\"Creating unlogged staging table.\", table_name=staging_table_name)
                    cur.execute(create_staging_sql)

                    # 2. Preparar dados e usar COPY
                    buffer = StringIO()
                    copy_failed_records = 0
                    try:
                        writer = csv.writer(buffer, delimiter='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
                        for i, record in enumerate(data):
                            row = []
                            try:
                                for field in columns:
                                    value = record.get(field)
                                    if value is None:
                                        row.append('\\N') 
                                    elif isinstance(value, datetime):
                                        if value.tzinfo is None:
                                            value = value.replace(tzinfo=timezone.utc)
                                        else:
                                            value = value.astimezone(timezone.utc)
                                        row.append(value.isoformat(timespec='microseconds'))
                                    elif isinstance(value, bool):
                                        row.append('t' if value else 'f')
                                    elif isinstance(value, (dict, list)):
                                        row.append(json.dumps(value).replace('\\', '\\\\').replace('|', '\\|').replace('\n', '\\n').replace('\r', '\\r'))
                                    else:
                                        str_value = str(value)
                                        escaped_value = str_value.replace('\\', '\\\\').replace('|', '\\|').replace('\n', '\\n').replace('\r', '\\r')
                                        row.append(escaped_value)
                                writer.writerow(row)
                            except Exception as row_err:
                                copy_failed_records += 1
                                self.log.error(\"Error preparing record for COPY\", record_index=i, error=str(row_err), record_preview=str(record)[:200])
                        buffer.seek(0)

                        if copy_failed_records > 0:
                            self.log.warning(\"Some records failed preparation for COPY\", failed_count=copy_failed_records, total_records=record_count)

                        # 3. Executar COPY
                        copy_sql = sql.SQL(\"COPY {staging_table} ({fields}) FROM STDIN WITH (FORMAT CSV, DELIMITER '|', NULL '\\N', ENCODING 'UTF8')\").format(
                            staging_table=staging_table_id,
                            fields=sql.SQL(', ').join(map(sql.Identifier, columns))
                        )
                        self.log.debug(\"Executing COPY command.\", table_name=staging_table_name)
                        cur.copy_expert(copy_sql, buffer)
                        copy_row_count = cur.rowcount
                        self.log.debug(\"Copied data to staging table.\", copied_row_count=copy_row_count, expected_count=record_count - copy_failed_records, table_name=staging_table_name)

                    finally:
                        buffer.close()

                    # 4. Executar UPSERT com CASTING
                    insert_fields = sql.SQL(', ').join(map(sql.Identifier, columns))
                    update_assignments_list = []
                    for col in columns:
                        if col == 'id': continue
                        if col == 'add_time':
                            update_assignments_list.append(
                                sql.SQL(\"{col} = COALESCE({target}.{col}, EXCLUDED.{col})\").format(
                                    col=sql.Identifier(col), target=target_table_id
                                )
                            )
                            continue
                        update_assignments_list.append(sql.SQL(\"{col} = EXCLUDED.{col}\").format(col=sql.Identifier(col)))

                    update_assignments = sql.SQL(', ').join(update_assignments_list)

                    # Mapear tipos da tabela principal para casting
                    target_types_main = {**COLUMN_TYPES, **self._custom_columns_dict, **self._stage_history_columns_dict}
                    select_expressions = []
                    for col in columns:
                        full_type_definition = target_types_main.get(col, \"TEXT\")
                        base_pg_type = full_type_definition.split()[0].split('(')[0].upper()

                        if base_pg_type in ('INTEGER', 'BIGINT', 'NUMERIC', 'DECIMAL', 'REAL', 'DOUBLE PRECISION', 'DATE', 'TIMESTAMP', 'TIMESTAMPTZ', 'BOOLEAN'):
                            select_expressions.append(
                                sql.SQL(\"NULLIF(TRIM({col}), '')::{type}\").format(
                                    col=sql.Identifier(col),
                                    type=sql.SQL(base_pg_type) 
                                )
                            )
                        elif base_pg_type == 'JSONB': 
                            select_expressions.append(
                                sql.SQL(\"NULLIF(TRIM({col}), '')::JSONB\").format(col=sql.Identifier(col))
                            )
                        else: 
                            select_expressions.append(sql.Identifier(col))

                    select_clause = sql.SQL(', ').join(select_expressions)

                    upsert_sql = sql.SQL(\"\"\"
                        INSERT INTO {target_table} ({insert_fields})
                        SELECT {select_clause} FROM {staging_table}
                        ON CONFLICT (id) DO UPDATE SET {update_assignments}
                        WHERE {target_table}.update_time IS NULL OR EXCLUDED.update_time >= {target_table}.update_time
                    \"\"\").format(
                        target_table=target_table_id,
                        insert_fields=insert_fields,
                        select_clause=select_clause,
                        staging_table=staging_table_id,
                        update_assignments=update_assignments
                    )

                    self.log.debug(\"Executing UPSERT command.\", target_table=self.TABLE_NAME)
                    cur.execute(upsert_sql)
                    upserted_count = cur.rowcount
                    conn.commit()
                    self.log.debug(\"Commit successful after UPSERT.\")

                    duration = py_time.monotonic() - start_time
                    self.log.info(
                        \"Upsert completed successfully for main table.\",
                        record_count=record_count,
                        initial_copy_failures=copy_failed_records,
                        affected_rows_upsert=upserted_count,
                        duration_sec=f\"{duration:.3f}\"
                    )

        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Upsert failed for main table\", error=str(e), record_count=record_count, exc_info=True)
            raise 
        finally:
            # 5. Dropar tabela de staging sempre
            if conn:
                try:
                    with conn.cursor() as final_cur:
                         drop_sql = sql.SQL(\"DROP TABLE IF EXISTS {staging_table}\").format(staging_table=staging_table_id)
                         self.log.debug(\"Dropping staging table.\", table_name=staging_table_name)
                         final_cur.execute(drop_sql)
                         conn.commit() # Commit do drop
                except Exception as drop_err:
                     self.log.error(\"Failed to drop staging table\", table_name=staging_table_name, error=str(drop_err))
                finally:
                    self.db_pool.release_connection(conn) # Liberar conex\u00e3o

    # --- Fun\u00e7\u00f5es de Leitura e Valida\u00e7\u00e3o ---
    def get_deals_needing_history_backfill(self, limit: int = 10000) -> List[str]:
        \"\"\"
        Busca IDs de deals que podem precisar de backfill hist\u00f3rico.
        Simplifica\u00e7\u00e3o: Busca deals antigos onde *qualquer* coluna de stage history seja NULL.
        Uma l\u00f3gica mais robusta poderia verificar quais colunas espec\u00edficas est\u00e3o faltando.
        \"\"\"
        conn = None
        if not self._stage_history_columns_dict:
            self.log.warning(\"No stage history columns defined, cannot find deals for backfill.\")
            return []

        where_conditions = [sql.SQL(\"{} IS NULL\").format(sql.Identifier(col)) for col in self._stage_history_columns_dict.keys()]
        if not where_conditions:
             self.log.warning(\"Could not build WHERE clause for backfill query.\")
             return []
        where_clause = sql.SQL(\" OR \").join(where_conditions)

        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"get_deals_needing_history_backfill\").time():
                with conn.cursor() as cur:
                    query = sql.SQL(\"\"\"
                        SELECT id FROM {table}
                        WHERE {conditions}
                        ORDER BY add_time ASC
                        LIMIT %s
                    \"\"\").format(
                        table=sql.Identifier(self.TABLE_NAME),
                        conditions=where_clause
                    )
                    cur.execute(query, (limit,))
                    deal_ids = [row[0] for row in cur.fetchall()]
                    self.log.info(\"Fetched deal IDs needing history backfill\", count=len(deal_ids), limit=limit)
                    return deal_ids
        except Exception as e:
            self.log.error(\"Failed to get deals for history backfill\", exc_info=True)
            return []
        finally:
            if conn:
                self.db_pool.release_connection(conn)

    def update_stage_history(self, updates: List[Dict[str, Any]]):
        \"\"\"
        Atualiza as colunas de hist\u00f3rico de stage para m\u00faltiplos deals.
        'updates' \u00e9 uma lista de dicts: [{'deal_id': str, 'stage_column': str, 'timestamp': datetime}, ...]
        Usa UPDATE FROM VALUES para efici\u00eancia. S\u00f3 atualiza se o valor atual for NULL.
        \"\"\"
        if not updates:
            self.log.debug(\"No stage history updates to apply.\")
            return

        conn = None
        start_time = py_time.monotonic()
        updated_count = 0

        updates_by_column: Dict[str, List[Tuple[str, datetime]]] = {}
        valid_stage_columns = set(self._stage_history_columns_dict.keys())

        for update in updates:
            deal_id = str(update.get('deal_id')) 
            stage_column = update.get('stage_column')
            timestamp = update.get('timestamp')

            if not deal_id or not stage_column or not isinstance(timestamp, datetime):
                 self.log.warning(\"Invalid data in stage history update\", update_data=update)
                 continue

            if stage_column not in valid_stage_columns:
                 self.log.warning(\"Attempted to update non-existent/invalid stage history column\", column_name=stage_column, deal_id=deal_id)
                 continue

            if stage_column not in updates_by_column:
                updates_by_column[stage_column] = []
            updates_by_column[stage_column].append((deal_id, timestamp))

        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"update_stage_history\").time():
                with conn.cursor() as cur:
                    for stage_column, column_updates in updates_by_column.items():
                        if not column_updates: continue

                        column_id = sql.Identifier(stage_column)
                        table_id = sql.Identifier(self.TABLE_NAME)

                        values_tuples = [(upd[0], upd[1]) for upd in column_updates]

                        update_sql = sql.SQL(\"\"\"
                            UPDATE {table} AS t
                            SET {column_to_update} = v.ts
                            FROM (VALUES %s) AS v(id, ts)
                            WHERE t.id = v.id
                            AND t.{column_to_update} IS NULL
                        \"\"\").format(
                            table=table_id,
                            column_to_update=column_id
                        )

                        try:
                            extras.execute_values(cur, update_sql.as_string(cur), values_tuples)
                            updated_count += cur.rowcount
                            self.log.debug(f\"Executed batch update for column '{stage_column}'\", records_in_batch=len(values_tuples), affected_rows=cur.rowcount)
                        except Exception as exec_err:
                            self.log.error(f\"Failed to execute batch update for column '{stage_column}'\", error=str(exec_err), records_count=len(values_tuples), exc_info=True)
                            conn.rollback() 
                            raise exec_err 

                    conn.commit()
                    duration = py_time.monotonic() - start_time
                    self.log.info(
                        \"Stage history update batch completed.\",
                        total_updates_processed=len(updates),
                        total_rows_affected=updated_count,
                        columns_updated=list(updates_by_column.keys()),
                        duration_sec=f\"{duration:.3f}s\"
                    )

        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Failed to update stage history\", error=str(e), total_updates=len(updates), exc_info=True)
        finally:
            if conn:
                self.db_pool.release_connection(conn)

    def count_deals_needing_backfill(self) -> int:
        \"\"\"
        Conta o n\u00famero total de deals que precisam de backfill hist\u00f3rico
        (onde pelo menos uma coluna de hist\u00f3rico de stage \u00e9 NULL).
        Retorna -1 em caso de erro.
        \"\"\"
        conn = None
        if not self._stage_history_columns_dict:
            self.log.warning(\"No stage history columns defined, cannot count deals for backfill.\")
            return -1

        where_conditions = [sql.SQL(\"{} IS NULL\").format(sql.Identifier(col)) for col in self._stage_history_columns_dict.keys()]
        if not where_conditions:
             self.log.warning(\"Could not build WHERE clause for backfill count query.\")
             return -1
        where_clause = sql.SQL(\" OR \").join(where_conditions)

        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"count_deals_needing_backfill\").time():
                with conn.cursor() as cur:
                    query = sql.SQL(\"\"\"
                        SELECT COUNT(*) FROM {table}
                        WHERE {conditions}
                    \"\"\").format(
                        table=sql.Identifier(self.TABLE_NAME),
                        conditions=where_clause
                    )
                    cur.execute(query)
                    count = cur.fetchone()[0]
                    self.log.info(\"Counted deals needing history backfill\", count=count)
                    return count if count is not None else 0
        except Exception as e:
            self.log.error(\"Failed to count deals for history backfill\", exc_info=True)
            return -1 
        finally:
            if conn:
                self.db_pool.release_connection(conn)
                
    def filter_data_by_ids(self, data: List[Dict], id_key: str = \"id\") -> List[Dict]:
        \"\"\"Filters data, returning records whose IDs are NOT in the database.\"\"\"
        if not data:
            return []

        ids_to_check = {str(rec.get(id_key)) for rec in data if rec.get(id_key) is not None}
        if not ids_to_check:
             self.log.warning(\"No valid IDs found in data for filtering.\")
             return data 

        conn = None
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"filter_data_by_ids\").time():
                with conn.cursor() as cur:
                    query = sql.SQL(\"SELECT id FROM {} WHERE id IN %s\").format(sql.Identifier(self.TABLE_NAME))
                    cur.execute(query, (tuple(ids_to_check),))
                    existing_ids = {row[0] for row in cur.fetchall()}

            new_records = [rec for rec in data if str(rec.get(id_key)) not in existing_ids]
            self.log.debug(\"Filtered existing records.\", initial_count=len(data), existing_count=len(existing_ids), new_count=len(new_records))
            return new_records

        except Exception as e:
            self.log.error(\"Failed to filter existing records by ID\", exc_info=True)
            return data
        finally:
            if conn:
                self.db_pool.release_connection(conn)

    def get_record_by_id(self, record_id: Any) -> Optional[Dict]:
        \"\"\"Busca um registro completo pelo ID, tratando ID como TEXT.\"\"\"
        conn = None
        record_id_str = str(record_id)
        self.log.debug(\"Fetching record by ID\", record_id=record_id_str)
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with conn.cursor(cursor_factory=extras.DictCursor) as cur:
                query = sql.SQL(\"SELECT * FROM {table} WHERE id = %s\").format(
                    table=sql.Identifier(self.TABLE_NAME)
                )
                cur.execute(query, (record_id_str,))
                row = cur.fetchone()
                return dict(row) if row else None
        except Exception as e:
             self.log.error(\"Failed to get record by ID\", record_id=record_id_str, exc_info=True)
             return None
        finally:
            if conn:
                self.db_pool.release_connection(conn)
                
    def add_columns_to_main_table(self, new_columns: List[str], inferred_from_df: pd.DataFrame) -> None:
        \"\"\"Tenta adicionar dinamicamente colunas novas no schema da tabela principal, inferindo o tipo via DataFrame.\"\"\"
        conn = None
        added_columns = []
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"add_columns_to_main_table\").time():
                with conn.cursor() as cur:
                    for col in new_columns:
                        sample_value = inferred_from_df[col].dropna().iloc[0] if not inferred_from_df[col].dropna().empty else None

                        inferred_type = \"TEXT\"
                        if isinstance(sample_value, (int, float, np.integer, np.floating)):
                            inferred_type = \"NUMERIC(18, 4)\"
                        elif isinstance(sample_value, (datetime, pd.Timestamp)):
                            inferred_type = \"TIMESTAMPTZ\"
                        elif isinstance(sample_value, bool):
                            inferred_type = \"BOOLEAN\"

                        alter_sql = sql.SQL(\"ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {col} {ctype}\").format(
                            table=sql.Identifier(self.TABLE_NAME),
                            col=sql.Identifier(col),
                            ctype=sql.SQL(inferred_type)
                        )
                        cur.execute(alter_sql)
                        added_columns.append((col, inferred_type))
                    conn.commit()
                    self.log.info(\"Added new columns to main table dynamically.\", columns=added_columns)
        except Exception as e:
            if conn:
                conn.rollback()
            self.log.error(\"Failed to add columns dynamically\", error=str(e), columns=new_columns, exc_info=True)
            raise
        finally:
            if conn:
                self.db_pool.release_connection(conn)

    def get_configuration(self, config_key: str) -> Any:
        query = sql.SQL(\"SELECT value FROM configuration WHERE key = %s LIMIT 1;\")
        with self._connection.cursor() as cursor:
            cursor.execute(query, (config_key,))
            result = cursor.fetchone()
            if result:
                return result[0]
            raise KeyError(f\"Configuration key '{config_key}' not found\")
        
    def fetch_all_stages_from_db_table(self) -> List[Dict]:
        \"\"\"Busca todos os stages da tabela de lookup persistente.\"\"\"
        conn = None
        log_ctx = self.log.bind(lookup_table=self.LOOKUP_TABLE_STAGES)
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with conn.cursor(cursor_factory=extras.DictCursor) as cur:
                query = sql.SQL(\"SELECT stage_id, stage_name, normalized_name FROM {} ORDER BY stage_id\").format(
                    sql.Identifier(self.LOOKUP_TABLE_STAGES)
                )
                cur.execute(query)
                stages_from_db = [dict(row) for row in cur.fetchall()]
                log_ctx.info(\"Fetched stages from DB table\", count=len(stages_from_db))

                formatted_stages = []
                for s in stages_from_db:
                    formatted_stages.append({
                        'id': s['stage_id'],        # Mapeia stage_id para 'id'
                        'name': s['stage_name'],    # Mapeia stage_name para 'name'
                        'normalized_name': s.get('normalized_name') # Mant\u00e9m normalized_name se existir
                    })
                return formatted_stages
        except Exception as e:
            log_ctx.error(\"Failed to fetch stages from DB table\", exc_info=True)
            return []
        finally:
            if conn:
                self.db_pool.release_connection(conn)
                
    def get_stage_id_to_column_map(self) -> Dict[int, str]:
        \"\"\"
        Mapeia stage_id \u2192 nome final da coluna, com base na nova regra com stage_id no nome.
        \"\"\"
        column_map = {}
        for stage in self._all_stages_details:
            stage_id = stage.get(\"id\")
            stage_name = stage.get(\"name\")
            if not stage_id or not stage_name:
                continue
            normalized = normalize_column_name(stage_name)
            col_name = f\"{self.STAGE_HISTORY_COLUMN_PREFIX}{normalized}_{stage_id}\"
            if col_name in self._stage_history_columns_dict:
                column_map[stage_id] = col_name
        return column_map

    def save_configuration(self, key: str, value: Dict):
        \"\"\"Salva configura\u00e7\u00f5es din\u00e2micas (formato JSONB) no banco.\"\"\"
        conn = None
        config_table_id = sql.Identifier(self.CONFIG_TABLE_NAME)
        self.log.debug(\"Saving configuration\", config_key=key)
        try:
            conn = self.db_pool.get_connection()
            db_active_connections.set(self.db_pool.num_active())
            db_idle_connections.set(self.db_pool.num_idle())
            with db_operation_duration_hist.labels(operation=\"save_configuration\").time():
                with conn.cursor() as cur:
                    upsert_sql = sql.SQL(\"\"\"
                        INSERT INTO {config_table} (key, value, updated_at)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (key) DO UPDATE SET
                            value = EXCLUDED.value,
                            updated_at = EXCLUDED.updated_at;
                    \"\"\").format(config_table=config_table_id)

                    timestamp_str = value.get('updated_at', datetime.now(timezone.utc).isoformat())
                    try:
                        ts_obj = datetime.fromisoformat(timestamp_str).astimezone(timezone.utc)
                    except Exception:
                        self.log.warning(\"Could not parse timestamp from config value, using current time.\", config_key=key, value_ts=timestamp_str)
                        ts_obj = datetime.now(timezone.utc)

                    cur.execute(upsert_sql, (key, json.dumps(value), ts_obj)) 
                    conn.commit()
                    self.log.info(\"Configuration saved successfully\", config_key=key)
        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Failed to save configuration\", config_key=key, exc_info=True)
        finally:
            if conn:
                self.db_pool.release_connection(conn)
            
    def get_all_ids(self) -> Set[str]:
        \"\"\"Retorna todos os IDs presentes no banco.\"\"\"
        conn = self.db_pool.get_connection()
        db_active_connections.set(self.db_pool.num_active())
        db_idle_connections.set(self.db_pool.num_idle())
        try:
            with db_operation_duration_hist.labels(operation=\"get_all_ids\").time():
                with conn.cursor() as cur:
                    cur.execute(f\"SELECT id FROM {self.TABLE_NAME}\")
                    return {row[0] for row in cur.fetchall()}
        finally:
            self.db_pool.release_connection(conn)
            
    def validate_date_consistency(self) -> int:
        \"\"\"Verifica consist\u00eancia b\u00e1sica de datas, retorna n\u00famero de problemas.\"\"\"
        conn = self.db_pool.get_connection()
        db_active_connections.set(self.db_pool.num_active())
        db_idle_connections.set(self.db_pool.num_idle())
        try:
            with db_operation_duration_hist.labels(operation=\"validate_date_consistency\").time():
                with conn.cursor() as cur:
                    cur.execute(f\"\"\"
                        SELECT COUNT(*) FROM {self.TABLE_NAME}
                        WHERE add_time > CURRENT_DATE 
                            OR update_time < add_time
                            OR (close_time IS NOT NULL AND close_time < add_time)
                    \"\"\")
                    return cur.fetchone()[0]
        finally:
            self.db_pool.release_connection(conn)

    def count_records(self) -> int:
        \"\"\"Conta o total de registros na tabela.\"\"\"
        conn = self.db_pool.get_connection()
        db_active_connections.set(self.db_pool.num_active())
        db_idle_connections.set(self.db_pool.num_idle())
        try:
            with db_operation_duration_hist.labels(operation=\"count_records\").time():
                with conn.cursor() as cur:
                    cur.execute(f\"SELECT COUNT(*) FROM {self.TABLE_NAME}\")
                    return cur.fetchone()[0]
        finally:
            self.db_pool.release_connection(conn)
            
    def save_data(self, data: List[Dict]) -> None:
        \"\"\"Default save implementation, uses upsert.\"\"\"
        self.log.debug(\"Calling save_data, delegating to save_data_upsert.\")
        self.save_data_upsert(data)"}
,
{"path": "infrastructure/cache.py", "encoding": "utf-8", "content": "import json
import redis
from datetime import timedelta

class RedisCache:
    def __init__(self, connection_string: str):
        self.client = redis.Redis.from_url(
            connection_string,
            decode_responses=True
        )

    def get(self, key):
        value = self.client.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return value
        return None

    def set(self, key, value, ex_seconds=3600):
        if isinstance(value, (dict, list)):
            value = json.dumps(value)
        self.client.setex(
            name=key,
            time=timedelta(seconds=ex_seconds),
            value=value
        )

    def delete(self, key):
        self.client.delete(key)"}
,
{"path": "infrastructure/prefect/orion/Dockerfile", "encoding": "utf-8", "content": "FROM prefecthq/prefect:3.2.14-python3.12
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

CMD [\"prefect\", \"server\", \"start\", \"--host\", \"0.0.0.0\"]
"}
,
{"path": "infrastructure/k8s/pipedrive_metabase_integration.yaml", "encoding": "utf-8", "content": "apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-orion
  labels:
    app: prefect-orion
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prefect-orion
  template:
    metadata:
      labels:
        app: prefect-orion
    spec:
      containers:
      - name: prefect-orion
        image: pipedrive_metabase_integration-prefect-orion:latest 
        imagePullPolicy: Never 
        ports:
        - containerPort: 4200
          name: http 
        readinessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5
        env:
        - name: APP_ROLE
          value: \"orion\"
        envFrom:
        - configMapRef:
            name: observability-config
        resources:
          requests:
            memory: \"1Gi\"
            cpu: \"500m\"
          limits:
            memory: \"1.5Gi\"
            cpu: \"1.5\"
---
apiVersion: v1
kind: Service
metadata:
  name: prefect-orion
spec:
  selector:
    app: prefect-orion
  ports:
  - protocol: TCP
    port: 4200
    targetPort: http 
  type: ClusterIP 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics
  labels:
    app: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics
  template:
    metadata:
      labels:
        app: metrics
      annotations: 
         prometheus.io/scrape: 'true'
         prometheus.io/port: '8082' 
         prometheus.io/path: '/metrics' 
    spec:
      containers:
      - name: metrics
        envFrom:
        - secretRef:
            name: app-secrets
        - secretRef:
            name: db-secrets
        image: pipedrive_metabase_integration-etl:latest 
        imagePullPolicy: Never
        ports:
        - containerPort: 8082
          name: metrics-port
        readinessProbe:
          httpGet:
            path: /metrics
            port: metrics-port
          initialDelaySeconds: 5
          periodSeconds: 15
        livenessProbe:
          tcpSocket: 
            port: metrics-port
          initialDelaySeconds: 30
          periodSeconds: 30
        resources:
          requests:
            memory: \"64Mi\"
            cpu: \"50m\"
          limits:
            memory: \"128Mi\"
            cpu: \"100m\"
        env:
          - name: APP_ROLE
            value: \"metrics\"
          - name: APP_METRICS_PORT 
            value: \"8082\"
---
apiVersion: v1
kind: Service
metadata:
  name: metrics
spec:
  selector:
    app: metrics
  ports:
  - protocol: TCP
    port: 8082
    targetPort: metrics-port 
  type: ClusterIP 
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine 
        ports:
        - containerPort: 6379
          name: redis-port 
        readinessProbe:
          tcpSocket:
            port: redis-port
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          exec:
            command: [\"redis-cli\", \"ping\"] 
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
        resources:
          requests:
            memory: \"128Mi\"
            cpu: \"100m\"
          limits:
            memory: \"512Mi\"
            cpu: \"500m\"
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: redis-port
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db
  labels:
    app: db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
      - name: db
        image: postgres:14-alpine 
        ports:
        - containerPort: 5432
          name: pgsql 
        envFrom:
          - secretRef:
              name: db-secrets
        volumeMounts:
        - name: pgdata
          mountPath: /var/lib/postgresql/data
        readinessProbe:
          tcpSocket:
              port: pgsql 
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        livenessProbe:
          tcpSocket:
            port: pgsql 
          initialDelaySeconds: 60
          periodSeconds: 30
        resources:
          requests:
            memory: \"1Gi\"
            cpu: \"500m\"
          limits:
            memory: \"1.5Gi\"
            cpu: \"1.5\"
      volumes:
      - name: pgdata
        persistentVolumeClaim:
          claimName: pgdata-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  selector:
    app: db
  ports:
  - protocol: TCP
    port: 5432
    targetPort: pgsql 
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana-oss:latest 
        ports:
        - containerPort: 3015
          name: http
        env:
        - name: GF_SERVER_HTTP_PORT
          value: \"3015\"
        readinessProbe:
          httpGet:
            path: /api/health
            port: http 
          initialDelaySeconds: 10
        livenessProbe:
          httpGet:
            path: /api/health
            port: http 
          initialDelaySeconds: 60 
          periodSeconds: 30
        resources:
          requests:
            memory: \"256Mi\"
            cpu: \"100m\"
          limits:
            memory: \"1Gi\"
            cpu: \"500m\"

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  selector:
    app: grafana
  ports:
  - protocol: TCP
    port: 3015
    targetPort: http 
  type: ClusterIP 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-agent
  labels:
    app: prefect-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prefect-agent
  template:
    metadata:
      labels:
        app: prefect-agent
    spec:
      containers:
      - name: agent
        image: pipedrive_metabase_integration-etl:latest
        imagePullPolicy: Never 
        command: [\"prefect\", \"worker\", \"start\", \"--pool\", \"kubernetes-pool\"]
        envFrom: 
        - secretRef:
            name: app-secrets
        - secretRef:
            name: db-secrets
        env:
          - name: PREFECT_API_URL
            value: \"http://prefect-orion:4200/api\"
        resources:
          requests:
            memory: \"512Mi\"
            cpu: \"200m\"
          limits:
            memory: \"1Gi\"
            cpu: \"500m\"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metabase
  labels:
    app: metabase
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metabase
  template:
    metadata:
      labels:
        app: metabase
    spec:
      containers:
      - name: metabase
        image: metabase/metabase:latest 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 3000
          name: http 
        readinessProbe:
          httpGet:
            path: /api/health # Endpoint de health do Metabase
            port: http
          initialDelaySeconds: 45 # Metabase pode demorar um pouco mais para carregar JVM/etc
          periodSeconds: 20
          timeoutSeconds: 10
          failureThreshold: 6 
        livenessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 180
          periodSeconds: 30
          failureThreshold: 5
        resources:
          requests:
            memory: \"1.5Gi\"
            cpu: \"500m\"
          limits:
            memory: \"2Gi\"
            cpu: \"1.5\" 
---
apiVersion: v1
kind: Service
metadata:
  name: metabase
spec:
  selector:
    app: metabase
  ports:
  - protocol: TCP
    port: 3000       
    targetPort: http 
  type: ClusterIP"}
,
{"path": "infrastructure/k8s/observability-config.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: ConfigMap
metadata:
  name: observability-config
data:
  LOG_LEVEL: \"INFO\"
  METRICS_ENDPOINT: \"/metrics\" "}
,
{"path": "infrastructure/k8s/prometheus.yml", "encoding": "utf-8", "content": "---
# --- 1. ConfigMap com prometheus.yml ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s # Intervalo padr\u00e3o de coleta
      evaluation_interval: 15s

    scrape_configs:
      # Coleta m\u00e9tricas do pr\u00f3prio Prometheus
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      - job_name: 'pushgateway'
        honor_labels: true # Importante para manter labels do job que fez o push
        static_configs:
          - targets: ['pushgateway:9091']

      # Coleta m\u00e9tricas de servi\u00e7os Kubernetes que possuem anota\u00e7\u00f5es espec\u00edficas
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
          - role: service # Descobre servi\u00e7os
        relabel_configs:
          # Seleciona apenas servi\u00e7os com a anota\u00e7\u00e3o prometheus.io/scrape=true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          # (Opcional) Usa a anota\u00e7\u00e3o prometheus.io/path se definida, sen\u00e3o usa /metrics
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Usa a anota\u00e7\u00e3o prometheus.io/port se definida, sen\u00e3o usa a porta do servi\u00e7o
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Define o label 'job' como o nome do servi\u00e7o
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: job
          # Define o label 'instance' como o IP:Porta do pod/endpoint
          # (Pode ser mais \u00fatil mudar role para 'pod' ou 'endpoints' para ter labels de pod)
          # Para simplificar, vamos manter o servi\u00e7o por enquanto.
          # - source_labels: [__address__]
          #   action: replace
          #   target_label: instance

---
# --- 2. RBAC: Service Account, ClusterRole, ClusterRoleBinding ---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-sa
  labels:
    app: prometheus

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-cr
rules:
- apiGroups: [\"\"]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: [\"get\", \"list\", \"watch\"]
- apiGroups: [\"extensions\", \"networking.k8s.io\"]
  resources:
  - ingresses
  verbs: [\"get\", \"list\", \"watch\"]
- nonResourceURLs: [\"/metrics\"]
  verbs: [\"get\"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-crb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-cr
subjects:
- kind: ServiceAccount
  name: prometheus-sa
  namespace: default

---
# --- 3. PersistentVolumeClaim ---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  labels:
    app: prometheus
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi 
      memory: \"1Gi\"  
      cpu: \"500m\"   
    limits:
      memory: \"2Gi\"  
      cpu: \"1\"     

---
# --- 4. Deployment do Prometheus Server ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus-sa
      containers:
        - name: prometheus
          image: prom/prometheus:latest 
          args:
            - \"--config.file=/etc/prometheus/prometheus.yml\"
            - \"--storage.tsdb.path=/prometheus/\"
            - \"--web.console.libraries=/usr/share/prometheus/console_libraries\"
            - \"--web.console.templates=/usr/share/prometheus/consoles\"
            - \"--web.enable-lifecycle\"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: config-volume
              mountPath: /etc/prometheus/
            - name: storage-volume
              mountPath: /prometheus/
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-pvc 

---
# --- 5. Service para expor o Prometheus internamente ---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service 
  labels:
    app: prometheus
spec:
  selector:
    app: prometheus 
  ports:
    - protocol: TCP
      port: 9090      
      targetPort: 9090  
  type: ClusterIP "}
,
{"path": "infrastructure/k8s/pushgateway.yaml", "encoding": "utf-8", "content": "apiVersion: apps/v1
kind: Deployment
metadata:
  name: pushgateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pushgateway
  template:
    metadata:
      labels:
        app: pushgateway
    spec:
      containers:
      - name: pushgateway
        image: prom/pushgateway:latest 
        ports:
        - containerPort: 9091
---
apiVersion: v1
kind: Service
metadata:
  name: pushgateway
  labels:
    app: pushgateway
  annotations:
     prometheus.io/scrape: 'true'
     prometheus.io/port: '9091'
spec:
  selector:
    app: pushgateway
  ports:
  - protocol: TCP
    port: 9091
    targetPort: 9091"}
,
{"path": "infrastructure/k8s/persistent-volume-claim.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pgdata-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
"}
,
{"path": "infrastructure/k8s/prefect.yaml", "encoding": "utf-8", "content": "# yaml-language-server: $schema=https://raw.githubusercontent.com/PrefectHQ/prefect/main/src/prefect/server/schemas/core/deployment.json
prefect-version: 3.3.2

name: pipedrive-etl

deployments:
# --- Main ETL Flow ---
- name: Pipedrive Sync
  version: '2.0'
  tags:
  - pipedrive
  - sync
  - etl
  - main
  description: Sincroniza deals recentes do Pipedrive com o banco de dados, usando
    lookups no DB.
  entrypoint: flows/pipedrive_metabase_etl.py:main_etl_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  concurrency_limit: 1
  schedules:
  - interval: 1800.0
    anchor_date: '2025-04-10T12:00:00+00:00'
    timezone: America/Sao_Paulo
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
- name: Pipedrive Backfill Stage History
  version: '1.0'
  tags:
  - pipedrive
  - backfill
  - history
  description: Preenche o hist\u00f3rico de stages para deals antigos.
  entrypoint: flows/pipedrive_metabase_etl.py:backfill_stage_history_flow
  parameters:
    daily_deal_limit: 10000
    db_batch_size: 1000
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  schedules: []
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Batch Size Experiment
  version: '1.0'
  tags:
  - experiment
  - batch-size
  - optimization
  description: Testa diferentes tamanhos de batch, calcula e salva o \u00f3timo na config.
  entrypoint: flows/pipedrive_metabase_etl.py:batch_size_experiment_flow
  parameters:
    batch_sizes:
    - 300
    - 500
    - 750
    - 1000
    - 1500
    test_data_size: 10000
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/experiment-k8s-job
  schedules: []
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Users
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - users
  description: Sincroniza a tabela pipedrive_users com a API.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_users_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/light-sync-k8s-job
  schedules:
  - cron: 0 3 * * *
    timezone: America/Sao_Paulo
    day_or: true
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Persons and Orgs
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - persons
  - orgs
  description: Sincroniza as tabelas pipedrive_persons e pipedrive_organizations.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_persons_orgs_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  schedules:
  - interval: 14400.0
    anchor_date: '2025-04-10T12:00:00+00:00'
    timezone: America/Sao_Paulo
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Stages and Pipelines
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - stages
  - pipelines
  description: Sincroniza as tabelas pipedrive_stages e pipedrive_pipelines.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_stages_pipelines_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/light-sync-k8s-job
  schedules:
  - cron: 0 4 * * *
    timezone: America/Sao_Paulo
    day_or: true
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: main
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
"}
,
{"path": "infrastructure/k8s/wait-for-it.sh", "encoding": "utf-8", "content": "#!/usr/bin/env bash
host=\"$1\"
port=\"$2\"
shift 2
cmd=\"$@\"

echo \"Aguardando Orion em http://$host:$port/api/health...\"
until curl -s \"http://$host:$port/api/health\" >/dev/null 2>&1; do
  echo \"Ainda aguardando...\"
  sleep 1
done

echo \"Orion est\u00e1 dispon\u00edvel. Executando comando: $cmd\"
exec $cmd
"}
,
{"path": "infrastructure/k8s/db-secrets.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: Secret
metadata:
  name: db-secrets
type: Opaque
data:
  POSTGRES_USER: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
  POSTGRES_PASSWORD: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
  POSTGRES_DB: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
"}
,
{"path": "infrastructure/k8s/entrypoint.sh", "encoding": "utf-8", "content": "#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

##############################
# Configura\u00e7\u00f5es
##############################
declare -A APP_PORTS=(
    [\"orion\"]=\"4200\"
    [\"metrics\"]=\"8082\"
)

##############################
# Fun\u00e7\u00f5es Auxiliares
##############################
log() {
    local LEVEL=\"$1\"
    local MESSAGE=\"$2\"
    printf \"[%s] [%s] %s\n\" \"$(date '+%Y-%m-%d %H:%M:%S')\" \"${LEVEL^^}\" \"${MESSAGE}\"
}

validate_env() {
    local REQUIRED_ENV=(\"POSTGRES_USER\" \"POSTGRES_PASSWORD\" \"PIPEDRIVE_API_KEY\")
    
    for var in \"${REQUIRED_ENV[@]}\"; do
        if [[ -z \"${!var:-}\" ]]; then
            log \"error\" \"Vari\u00e1vel de ambiente obrigat\u00f3ria n\u00e3o definida: $var\"
            exit 1
        fi
    done
}

start_server() {
    local APP=\"$1\"
    shift
    
    log \"info\" \"Iniciando $APP...\"
    exec \"$@\"
}

##############################
# Fluxo Principal
##############################
cd /app

case \"${APP_ROLE:-}\" in
    etl)
        validate_env

        # Executar fluxo ETL
        log \"info\" \"Iniciando fluxo ETL (depend\u00eancias esperadas via Init Containers)...\"
        poetry run python -u flows/pipedrive_metabase_etl.py
        ;;
    metrics)
        start_server \"metrics server\" \
            python -m infrastructure.monitoring.metrics_server
        ;;
    orion)
        start_server \"Prefect Orion\" \
            prefect orion start \
                --host 0.0.0.0 \
                --port \"${APP_PORTS[orion]}\" \
                --log-level WARNING
        ;;
    *)
        log \"error\" \"APP_ROLE inv\u00e1lido ou n\u00e3o definido. Valores permitidos: etl, metrics, orion\"
        exit 1
        ;;
esac"}
,
{"path": "infrastructure/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/api_clients/pipedrive_api_client.py", "encoding": "utf-8", "content": "import time
import requests
import structlog
from tenacity import RetryError, retry, retry_if_exception, stop_after_attempt, wait_exponential, retry_if_exception_type
from pybreaker import CircuitBreaker
from typing import Dict, List, Optional, Generator, Any, Set, Tuple

from infrastructure.config.settings import settings
from infrastructure.cache import RedisCache
from application.ports.pipedrive_client_port import PipedriveClientPort
from application.utils.column_utils import normalize_column_name 
from infrastructure.monitoring.metrics import (
    api_request_duration_hist,
    api_errors_counter,
    pipedrive_api_token_cost_total,
    pipedrive_api_call_total,
    pipedrive_api_cache_hit_total,
    pipedrive_api_cache_miss_total,
    pipedrive_api_rate_limit_remaining,
    pipedrive_api_rate_limit_reset_seconds,
    pipedrive_api_retries_total,
    pipedrive_api_retry_failures_total
)


log = structlog.get_logger(__name__)

# Circuit Breaker
api_breaker = CircuitBreaker(fail_max=3, reset_timeout=60)

class PipedriveAPIClient(PipedriveClientPort):
    BASE_URL_V1 = \"https://api.pipedrive.com/v1\"
    BASE_URL_V2 = \"https://api.pipedrive.com/api/v2\"

    DEFAULT_TIMEOUT = 45
    DEFAULT_V2_LIMIT = 500
    MAX_V1_PAGINATION_LIMIT = 500
    CHANGELOG_PAGE_LIMIT = 500

    # TTL para mapas e lookups individuais
    DEFAULT_MAP_CACHE_TTL_SECONDS = 300 * 12  # 12 hours
    STAGE_DETAILS_CACHE_TTL_SECONDS = 300 * 6  # 6 hours
    
    ENDPOINT_COSTS = {
        # V1 endpoints
        '/deals/detail/changelog': 20,  # GET /v1/deals/{id}/changelog
        '/dealFields': 20,              # GET /v1/dealFields
        '/users': 20,                   # GET /v1/users
        '/users/detail': 5,             # GET /v1/users/{id} se existir

        # V2 endpoints
        '/deals': 10,                   # GET /api/v2/deals
        '/stages': 5,                   # GET /api/v2/stages
        '/pipelines': 5,                # GET /api/v2/pipelines
        '/persons/detail': 1,           # GET /api/v2/persons/{id}
        '/persons': 10,                 # GET /api/v2/persons?ids=...
    }
    
    DEFAULT_ENDPOINT_COST = 10
    UNKNOWN_NAME = \"Desconhecido\"
    
    def __init__(self, cache: RedisCache):
        self.api_key = settings.PIPEDRIVE_API_KEY
        if not self.api_key:
            log.error(\"PIPEDRIVE_API_KEY is not set!\")
            raise ValueError(\"Pipedrive API Key is required.\")
        
        self.session = requests.Session()
        self.session.headers.update({\"Accept\": \"application/json\"})
        self.cache = cache
        self.log = log.bind(client=\"PipedriveAPIClient\")
    
    def _normalize_endpoint_for_metrics(self, url_path: str) -> str:
        \"\"\"Normaliza o path da URL para usar como label na m\u00e9trica, tratando IDs etc.\"\"\"
        base_url_v1_len = len(self.BASE_URL_V1)
        base_url_v2_len = len(self.BASE_URL_V2)
        path = url_path

        if url_path.startswith(self.BASE_URL_V1):
            path = url_path[base_url_v1_len:]
        elif url_path.startswith(self.BASE_URL_V2):
            path = url_path[base_url_v2_len:]

        path = path.split('?')[0].strip('/')
        parts = path.split('/')

        if not parts or not parts[0]:
            return \"/\"

        resource = parts[0]
        normalized_path = f\"/{resource}\"

        # /resource/{id} -> /resource/detail
        if len(parts) > 1 and parts[1].isdigit():
            normalized_path += \"/detail\"
            # /resource/{id}/subresource -> /resource/detail/subresource
            if len(parts) > 2:
                # (exemplo: /deals/detail/changelog)
                known_subresources = [
                    'changelog', 'followers', 'activities', 'files',
                    'mailMessages', 'participants', 'products'
                ]
                if parts[2] in known_subresources:
                    normalized_path += f\"/{parts[2]}\"
                # /resource/{id}/products/{product_id} => /resource/detail/products/detail
                elif len(parts) > 3 and parts[3].isdigit():
                    normalized_path += f\"/{parts[2]}/detail\"

        # /resource/subresource
        elif len(parts) > 1:
            known_actions = ['search', 'summary', 'timeline', 'collection', 'products', 'installments']
            if parts[1] in known_actions:
                normalized_path += f\"/{parts[1]}\"

        self.log.debug(\"Normalized endpoint\", original=url_path, normalized=normalized_path)
        return normalized_path
    
    @api_breaker
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=(
            retry_if_exception_type(requests.exceptions.Timeout) |
            retry_if_exception_type(requests.exceptions.ConnectionError) |
            retry_if_exception_type(requests.exceptions.ChunkedEncodingError) |
            retry_if_exception(lambda e: isinstance(e, requests.exceptions.HTTPError) 
                               and getattr(e.response, 'status_code', None) >= 500) |
            retry_if_exception(lambda e: isinstance(e, requests.exceptions.HTTPError) 
                               and getattr(e.response, 'status_code', None) == 429)
        ),
        reraise=True
    )
    def _get(self, url: str, params: Optional[Dict[str, Any]] = None) -> requests.Response:
        \"\"\"
        \u00danico m\u00e9todo que realmente faz uma requisi\u00e7\u00e3o GET ao Pipedrive.
        - S\u00f3 aqui incrementamos o custo de tokens.
        - Se algo vier do cache, n\u00e3o passar\u00e1 por aqui.
        \"\"\"
        effective_params = params.copy() if params else {}
        if \"api_token\" not in effective_params:
            effective_params[\"api_token\"] = self.api_key

        start_time = time.monotonic()
        error_type = \"success\"
        status_code = None
        response = None
        log_params = {k: v for k, v in effective_params.items() if k != 'api_token'}

        raw_endpoint_path = url
        normalized_endpoint_label = self._normalize_endpoint_for_metrics(raw_endpoint_path)
        request_log = self.log.bind(endpoint=normalized_endpoint_label, method='GET', params=log_params)

        try:
            request_log.debug(\"Making API GET request\", url=url)
            response = self.session.get(url, params=effective_params, timeout=self.DEFAULT_TIMEOUT)
            status_code = response.status_code
            pipedrive_api_call_total.labels(
                endpoint=normalized_endpoint_label,
                method='GET',
                status_code=str(status_code)
            ).inc()

            # Rate limit handling
            if status_code == 429:
                retry_after = response.headers.get(\"Retry-After\")
                request_log.warning(\"Rate limit hit (429)\", retry_after=retry_after)

            if not response.ok:
                error_type = f\"http_{status_code}\"
                try:
                    response.raise_for_status()
                except requests.exceptions.HTTPError as e:
                    snippet = e.response.text[:200] if e.response else \"N/A\"
                    log_method = request_log.error if status_code >= 500 else request_log.warning
                    log_method(\"API request failed with HTTP error\", 
                               status_code=status_code, 
                               response_text=snippet, 
                               error=str(e))
                    raise e

            # --- Se chegou aqui, \u00e9 2xx (ok) ou 3xx sem raise_for_status ---
            cost = self.ENDPOINT_COSTS.get(normalized_endpoint_label, self.DEFAULT_ENDPOINT_COST)
            request_log.debug(\"Incrementing API token cost\", cost=cost, endpoint=normalized_endpoint_label)
            pipedrive_api_token_cost_total.labels(endpoint=normalized_endpoint_label).inc(cost)
            remaining = response.headers.get('X-RateLimit-Remaining')
            if remaining and remaining.isdigit():
                pipedrive_api_rate_limit_remaining.labels(endpoint=normalized_endpoint_label).set(int(remaining))
            reset = response.headers.get('X-RateLimit-Reset')
            if reset and reset.isdigit():
                pipedrive_api_rate_limit_reset_seconds.labels(endpoint=normalized_endpoint_label).set(int(reset))

            duration = time.monotonic() - start_time
            api_request_duration_hist.labels(
                endpoint=normalized_endpoint_label, 
                method='GET', 
                status_code=status_code
            ).observe(duration)

            request_log.debug(\"API GET request successful\", status_code=status_code, duration_sec=f\"{duration:.3f}s\")
            return response

        except requests.exceptions.Timeout as e:
            pipedrive_api_retries_total.labels(endpoint=normalized_endpoint_label).inc()
            error_type = \"timeout\"
            request_log.warning(\"API request timed out\", error=str(e))
            raise
        except requests.exceptions.ConnectionError as e:
            error_type = \"connection_error\"
            request_log.warning(\"API connection error\", error=str(e))
            raise
        except requests.exceptions.RequestException as e:
            pipedrive_api_retry_failures_total.labels(endpoint=normalized_endpoint_label).inc()
            error_type = \"request_exception\"
            request_log.error(\"API request failed (RequestException)\", exc_info=True)

        # Caso de exce\u00e7\u00e3o
        current_status_code = status_code
        if hasattr(e, 'response') and e.response is not None:
            current_status_code = e.response.status_code
        elif isinstance(e, RetryError) and hasattr(e.cause, 'response') and e.cause.response is not None:
            current_status_code = e.cause.response.status_code

        final_status_code_label = str(current_status_code) if current_status_code else 'N/A'
        api_errors_counter.labels(endpoint=normalized_endpoint_label, 
                                  error_type=error_type, 
                                  status_code=final_status_code_label).inc()
        request_log.debug(\"API Error counter incremented\", 
                          error_type=error_type, 
                          status_code=final_status_code_label)
        raise

    def _fetch_paginated_v1(self, url: str, params: Optional[Dict[str, Any]] = None) -> List[Dict]:
        \"\"\"Helper para buscar todos os itens de um endpoint V1 paginado (start/limit).\"\"\"
        all_data = []; start = 0
        base_params = params or {}; base_params[\"limit\"] = self.MAX_V1_PAGINATION_LIMIT
        endpoint_name = url.split(self.BASE_URL_V1)[-1] if self.BASE_URL_V1 in url else url
        page_num = 0

        while True:
            page_num += 1
            current_params = base_params.copy(); current_params[\"start\"] = start
            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, start=start, limit=current_params[\"limit\"])
            page_log.debug(\"Fetching V1 page\")
            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"API response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.debug(\"No more V1 data found on this page.\"); break
                all_data.extend(current_data); page_log.debug(f\"Fetched {len(current_data)} V1 items for this page.\")
                pagination_info = json_response.get(\"additional_data\", {}).get(\"pagination\", {}); more_items = pagination_info.get(\"more_items_in_collection\", False)
                if more_items:
                    next_start = pagination_info.get(\"next_start\")
                    if next_start is not None: start = next_start
                    else: page_log.warning(\"API indicates more items but no 'next_start'. Stopping.\", pagination=pagination_info); break
                else: page_log.debug(\"API indicates no more items in collection.\"); break
            except Exception as e: page_log.error(\"Error during V1 fetching page\", exc_info=True); break 
        self.log.info(f\"V1 Paginated fetch complete.\", endpoint=endpoint_name, total_items=len(all_data), total_pages=page_num)
        return all_data

    def _fetch_paginated_v2(self, url: str, params: Optional[Dict[str, Any]] = None) -> List[Dict]:
        \"\"\"Helper para buscar todos os itens de um endpoint V2 paginado (cursor).\"\"\"
        all_data = []; next_cursor: Optional[str] = None
        base_params = params or {}; base_params[\"limit\"] = self.DEFAULT_V2_LIMIT
        normalized_endpoint_label = self._normalize_endpoint_for_metrics(url)
        page_num = 0

        while True:
            page_num += 1
            current_params = base_params.copy()
            if next_cursor: current_params[\"cursor\"] = next_cursor
            elif \"cursor\" in current_params: del current_params[\"cursor\"]
            page_log = self.log.bind(endpoint=normalized_endpoint_label, page=page_num, limit=current_params[\"limit\"], cursor=next_cursor)
            page_log.debug(\"Fetching V2 page\")
            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"API response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.debug(\"No more V2 data found on this page.\"); break
                all_data.extend(current_data); page_log.debug(f\"Fetched {len(current_data)} V2 items for this page.\")
                additional_data = json_response.get(\"additional_data\", {}); next_cursor = additional_data.get(\"next_cursor\") 
                if not next_cursor: page_log.debug(\"No 'next_cursor' found. Ending pagination.\"); break
            except Exception as e: page_log.error(\"Error during V2 fetching page\", exc_info=True); break 
        self.log.info(f\"V2 Paginated fetch complete.\", endpoint=normalized_endpoint_label, total_items=len(all_data), total_pages=page_num)
        return all_data

    # --- M\u00e9todos de busca de mapas ---
    def fetch_all_users_map(self) -> Dict[int, str]:
        cache_key = \"pipedrive:all_users_map\"
        cached = self.cache.get(cache_key)
        if cached:
            pipedrive_api_cache_hit_total.labels(entity=\"users\", source=\"redis\").inc()
            return cached
        else:
            pipedrive_api_cache_miss_total.labels(entity=\"users\", source=\"redis\").inc()
            
        if cached and isinstance(cached, dict): self.log.info(\"Users map retrieved from cache.\", cache_hit=True, map_size=len(cached)); return cached
        self.log.info(\"Fetching users map from API (V1).\", cache_hit=False)
        url = f\"{self.BASE_URL_V1}/users\"
        try:
            all_users = self._fetch_paginated_v1(url)
            user_map = {user['id']: user.get('name', self.UNKNOWN_NAME)
                        for user in all_users if user and 'id' in user}
            if user_map: self.cache.set(cache_key, user_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS); self.log.info(\"Users map fetched and cached.\", map_size=len(user_map))
            else: self.log.warning(\"Fetched user list was empty or malformed.\")
            return user_map
        except Exception as e: self.log.error(\"Failed to fetch/process users map\", exc_info=True); return {}

    def fetch_all_pipelines_map(self) -> Dict[int, str]:
        cache_key = \"pipedrive:all_pipelines_map\"
        cached = self.cache.get(cache_key)
        if cached:
            pipedrive_api_cache_hit_total.labels(entity=\"pipelines\", source=\"redis\").inc()
            return cached
        else:
            pipedrive_api_cache_miss_total.labels(entity=\"pipelines\", source=\"redis\").inc()
        if cached and isinstance(cached, dict): self.log.info(\"Pipelines map retrieved from cache.\", cache_hit=True, map_size=len(cached)); return cached
        self.log.info(\"Fetching pipelines map from API (V2).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/pipelines\"
        try:
            all_pipelines = self._fetch_paginated_v2(url)
            pipeline_map = {p['id']: p.get('name', self.UNKNOWN_NAME)
                            for p in all_pipelines if p and 'id' in p}
            if pipeline_map: self.cache.set(cache_key, pipeline_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS); self.log.info(\"Pipelines map fetched and cached.\", map_size=len(pipeline_map))
            else: self.log.warning(\"Fetched pipeline list was empty or malformed.\")
            return pipeline_map
        except Exception as e: self.log.error(\"Failed to fetch/process pipelines map\", exc_info=True); return {}

    def fetch_all_persons_map(self) -> Dict[int, str]:
        \"\"\"
        Busca todos os persons usando pagina\u00e7\u00e3o V2 via stream
        e armazena em cache de forma eficiente em mem\u00f3ria.
        \"\"\"
        cache_key = \"pipedrive:all_persons_map\"
        cached = self.cache.get(cache_key)
        if cached:
            pipedrive_api_cache_hit_total.labels(entity=\"persons\", source=\"redis\").inc()
            return cached
        else:
            pipedrive_api_cache_miss_total.labels(entity=\"persons\", source=\"redis\").inc()
        if cached and isinstance(cached, dict):
            self.log.info(\"Persons map retrieved from cache.\", cache_hit=True, map_size=len(cached))
            return cached

        self.log.info(\"Fetching persons map from API (V2 - streamed).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/persons\"
        person_map: Dict[int, str] = {}
        total_persons_processed = 0

        try:
            person_stream = self._fetch_paginated_v2_stream(url)

            for person in person_stream:
                total_persons_processed += 1
                person_id = person.get('id')
                person_name = person.get('name', '').strip() 
                if person_id and person_name:
                    person_map[person_id] = person_name
                elif person_id:
                    self.log.debug(\"Person found with empty name, skipping map entry.\", person_id=person_id)

            if person_map:
                self.cache.set(cache_key, person_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS)
                self.log.info(\"Persons map fetched via stream and cached.\", map_size=len(person_map), total_persons_processed=total_persons_processed)
            else:
                self.log.warning(\"Fetched person stream resulted in an empty map (no valid names found?).\", total_persons_processed=total_persons_processed)

            return person_map

        except Exception as e:
            self.log.error(\"Failed to fetch/process persons map via stream\", exc_info=True)
            return {}
        
    def fetch_all_stages_details(self) -> List[Dict]:
        \"\"\"Busca detalhes de todos os stages (necess\u00e1rio para nomes normalizados).\"\"\"
        cache_key = \"pipedrive:all_stages_details\"
        cached = self.cache.get(cache_key)
        if cached:
            pipedrive_api_cache_hit_total.labels(entity=\"stages\", source=\"redis\").inc()
            return cached
        else:
            pipedrive_api_cache_miss_total.labels(entity=\"stages\", source=\"redis\").inc()
        if cached and isinstance(cached, list):
            self.log.info(\"Stage details retrieved from cache.\", cache_hit=True, count=len(cached))
            return cached

        self.log.info(\"Fetching stage details from API (V2).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/stages\"
        try:
            all_stages = self._fetch_paginated_v2(url)
            if all_stages:
                self.cache.set(cache_key, all_stages, ex_seconds=self.STAGE_DETAILS_CACHE_TTL_SECONDS)
                self.log.info(\"Stage details fetched and cached.\", count=len(all_stages))
            else:
                self.log.warning(\"Fetched stage list was empty.\")
            return all_stages
        except Exception as e:
            self.log.error(\"Failed to fetch/process stage details\", exc_info=True)
            return []
        
    def fetch_deal_fields_mapping(self) -> Dict[str, str]:
        cache_key = \"pipedrive:deal_fields_mapping\"; cache_ttl_seconds = 86400 # 24h
        cached = self.cache.get(cache_key)
        if cached:
            pipedrive_api_cache_hit_total.labels(entity=\"deal_fields\", source=\"redis\").inc()
            return cached
        else:
            pipedrive_api_cache_miss_total.labels(entity=\"deal_fields\", source=\"redis\").inc()
        if cached and isinstance(cached, dict): self.log.info(\"Deal fields mapping retrieved from cache.\", cache_hit=True, map_size=len(cached)); return cached
        self.log.info(\"Fetching deal fields mapping from API (V1).\", cache_hit=False)
        url = f\"{self.BASE_URL_V1}/dealFields\"
        try:
            all_fields_data = self._fetch_paginated_v1(url)
            if not all_fields_data: self.log.warning(\"Received no data for deal fields from API.\"); return {}
            non_custom_keys = {
                \"id\", \"creator_user_id\", \"person_id\", \"org_id\",
                \"stage_id\", \"pipeline_id\", \"title\", \"value\", \"currency\", \"add_time\",
                \"update_time\", \"status\", \"lost_reason\", \"visible_to\", \"close_time\",
                \"won_time\", \"lost_time\", \"first_won_time\", \"products_count\",
                \"files_count\", \"notes_count\", \"followers_count\", \"email_messages_count\",
                \"activities_count\", \"done_activities_count\", \"undone_activities_count\",
                \"participants_count\", \"expected_close_date\", \"probability\",
                \"next_activity_date\", \"next_activity_time\", \"next_activity_id\",
                \"last_activity_id\", \"last_activity_date\", \"stage_change_time\",
                \"last_incoming_mail_time\", \"last_outgoing_mail_time\",
                \"label\", \"stage_order_nr\", \"person_name\", \"org_name\", \"next_activity_subject\",
                \"next_activity_type\", \"next_activity_duration\", \"next_activity_note\",
                \"formatted_value\", \"weighted_value\", \"formatted_weighted_value\",
                \"weighted_value_currency\", \"rotten_time\", \"owner_name\", \"cc_email\"
            }

            mapping = {}
            for field in all_fields_data:
                 api_key = field.get(\"key\")
                 name = field.get(\"name\")
                 if api_key and name and api_key not in non_custom_keys:
                     normalized = normalize_column_name(name) 
                     if normalized:
                         # Adiciona log se houver colis\u00e3o de nome normalizado
                         if normalized in [m for m in mapping.values()]:
                             self.log.warning(\"Normalized custom field name collision detected.\",
                                               conflicting_api_key=api_key,
                                               conflicting_name=name,
                                               normalized_name=normalized)
                         mapping[api_key] = normalized
                     else:
                         self.log.warning(\"Failed to normalize custom field name.\", api_key=api_key, original_name=name)

            self.cache.set(cache_key, mapping, ex_seconds=cache_ttl_seconds)
            self.log.info(\"Deal fields mapping fetched and cached.\", total_fields_api=len(all_fields_data), custom_mapping_count=len(mapping))
            return mapping
        except Exception as e: self.log.error(\"Failed to fetch and process deal fields mapping\", exc_info=True); return {}

    def get_last_timestamp(self) -> str | None:
        cache_key = \"pipedrive:last_update_timestamp\"
        timestamp = self.cache.get(cache_key)
        if timestamp and isinstance(timestamp, str):
             self.log.debug(\"Last timestamp retrieved from cache\", timestamp=timestamp)
             return timestamp
        self.log.info(\"No last update timestamp found in cache.\"); return None 

    def _fetch_paginated_v2_stream(self, url: str, params: Optional[Dict[str, Any]] = None) -> Generator[Dict, None, None]:
        \"\"\"Helper generator para buscar itens V2 um por um via cursor.\"\"\"
        next_cursor: Optional[str] = None
        base_params = params or {}; base_params[\"limit\"] = self.DEFAULT_V2_LIMIT
        endpoint_name = url.split(self.BASE_URL_V2)[-1] if self.BASE_URL_V2 in url else url
        endpoint_name = endpoint_name.split('?')[0].strip('/') 
        items_yielded = 0; page_num = 0
        log_every_n_pages = 50

        while True:
            page_num += 1
            current_params = base_params.copy()
            if next_cursor: current_params[\"cursor\"] = next_cursor
            elif \"cursor\" in current_params: del current_params[\"cursor\"]

            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, limit=current_params[\"limit\"], cursor=next_cursor)

            if page_num == 1 or page_num % log_every_n_pages == 0:
                 page_log.info(\"Fetching V2 page for stream\", items_yielded_so_far=items_yielded)
            else:
                 page_log.debug(\"Fetching V2 page for stream\", items_yielded_so_far=items_yielded)


            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"V2 API stream response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.info(\"No more V2 stream data found on this page.\"); break

                for item in current_data: items_yielded += 1; yield item

                additional_data = json_response.get(\"additional_data\", {}); next_cursor = additional_data.get(\"next_cursor\")
                if not next_cursor: page_log.info(\"No 'next_cursor' found. Ending pagination stream.\"); break
            except Exception as e: page_log.error(\"Error during V2 stream fetching page, stopping stream.\", exc_info=True); break

        self.log.info(f\"V2 Stream fetch complete.\", endpoint=endpoint_name, total_items_yielded=items_yielded, total_pages=page_num)
        
    def _fetch_paginated_v1_stream_adapter(self, url: str, params: Optional[Dict[str, Any]] = None) -> Generator[Dict, None, None]:
        \"\"\"
        Gera itens de endpoints V1 paginados usando start/limit.
        \u00datil para consumir dados sem carregar tudo na mem\u00f3ria.
        \"\"\"
        start = 0
        base_params = params.copy() if params else {}
        base_params[\"limit\"] = self.MAX_V1_PAGINATION_LIMIT
        endpoint_name = url.split(self.BASE_URL_V1)[-1] if self.BASE_URL_V1 in url else url
        page_num = 0
        items_yielded = 0

        while True:
            page_num += 1
            current_params = base_params.copy()
            current_params[\"start\"] = start

            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, start=start, limit=current_params[\"limit\"])

            try:
                response = self._get(url, params=current_params)
                json_response = response.json()

                if not json_response or not json_response.get(\"success\"):
                    page_log.warning(\"V1 stream response indicates failure or empty data\", response_preview=str(json_response)[:200])
                    break

                current_data = json_response.get(\"data\", [])
                if not current_data:
                    page_log.info(\"No more V1 stream data found.\")
                    break

                for item in current_data:
                    items_yielded += 1
                    yield item

                pagination_info = json_response.get(\"additional_data\", {}).get(\"pagination\", {})
                more_items = pagination_info.get(\"more_items_in_collection\", False)
                if more_items:
                    next_start = pagination_info.get(\"next_start\")
                    if next_start is not None:
                        start = next_start
                    else:
                        page_log.warning(\"Missing 'next_start' despite 'more_items_in_collection' being true.\")
                        break
                else:
                    page_log.info(\"Pagination completed via stream.\")
                    break

            except Exception as e:
                page_log.error(\"Error during V1 stream pagination\", exc_info=True)
                break

        self.log.info(\"V1 stream fetch completed.\", endpoint=endpoint_name, items_yielded=items_yielded, total_pages=page_num)

    def fetch_all_deals_stream(self, updated_since: str = None, items_limit: int = None) -> Generator[Dict, None, None]:
        \"\"\"Busca deals (V2) com limite opcional usando pagina\u00e7\u00e3o por cursor.\"\"\"
        url = f\"{self.BASE_URL_V2}/deals\"
        params = {\"sort_by\": \"update_time\", \"sort_direction\": \"asc\"}
        if updated_since:
            params[\"updated_since\"] = updated_since
        if items_limit:
            params[\"limit\"] = min(items_limit, self.DEFAULT_V2_LIMIT)
        
        count = 0
        for deal in self._fetch_paginated_v2_stream(url, params):
            if items_limit and count >= items_limit:
                break
            count += 1
            yield deal


    def update_last_timestamp(self, new_timestamp: str):
        \"\"\"Armazena o \u00faltimo timestamp processado no cache.\"\"\"
        cache_key = \"pipedrive:last_update_timestamp\"; cache_ttl_seconds = 2592000 # 30 dias
        try: self.cache.set(cache_key, new_timestamp, ex_seconds=cache_ttl_seconds); self.log.info(\"Updated last update timestamp in cache\", timestamp=new_timestamp)
        except Exception as e: self.log.error(\"Failed to store last update timestamp in cache\", timestamp=new_timestamp, exc_info=True)
        
    def fetch_deal_changelog(self, deal_id: int) -> List[Dict]:
            \"\"\"Busca o changelog para um deal espec\u00edfico (V1).\"\"\"
            if not deal_id or deal_id <= 0:
                self.log.warning(\"Invalid deal_id for changelog fetch\", deal_id=deal_id)
                return []

            url = f\"{self.BASE_URL_V1}/deals/{deal_id}/changelog\"
            self.log.debug(\"Fetching deal changelog from API (V1)\", deal_id=deal_id)
            try:
                changelog_data = self._fetch_paginated_v1(url)
                return changelog_data
            except Exception as e:
                self.log.error(\"Failed to fetch deal changelog\", deal_id=deal_id, error=str(e))
                raise e"}
,
{"path": "infrastructure/api_clients/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/logging_config.py", "encoding": "utf-8", "content": "import logging
import sys
import structlog
import os 


def setup_logging(level=logging.INFO, force_json=False):
    \"\"\"
    Configura\u00e7\u00e3o robusta e testada para structlog + logging padr\u00e3o
    \"\"\"
    # 1. Processadores comuns para todos os loggers
    common_processors = [
        structlog.contextvars.merge_contextvars,
        structlog.threadlocal.merge_threadlocal,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt=\"iso\", utc=True),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
    ]

    # 2. Escolher renderizador final
    if not force_json and sys.stdout.isatty():
        renderer = structlog.dev.ConsoleRenderer(colors=True)
    else:
        renderer = structlog.processors.JSONRenderer()

    # 3. Configura\u00e7\u00e3o completa do structlog
    structlog.configure(
        processors=common_processors + [renderer],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # 4. Configurar logging padr\u00e3o para usar structlog
    formatter = structlog.stdlib.ProcessorFormatter(
        processor=renderer,
        foreign_pre_chain=common_processors,
    )

    handler = logging.StreamHandler()
    handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.handlers = [handler]
    root_logger.setLevel(level)

    # 5. Configurar n\u00edveis para bibliotecas ruidosas
    for lib in [\"httpx\", \"httpcore\", \"urllib3\"]:
        logging.getLogger(lib).setLevel(logging.WARNING)

    # Log inicial
    logger = structlog.get_logger(__name__)
    logger.info(\"Logging configurado com sucesso\", level=level)"}
,
{"path": "create_or_update_core_blocks.py", "encoding": "utf-8", "content": "import os
from prefect.blocks.system import Secret, JSON
from prefect_kubernetes.jobs import KubernetesJob
from dotenv import load_dotenv
import structlog
from typing import Optional, Dict

try:
    structlog.configure(
        processors=[
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt=\"iso\"),
            structlog.dev.ConsoleRenderer(),
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
except structlog.exceptions.AlreadyConfiguredError:
    pass 
log = structlog.get_logger(__name__)

load_dotenv()

print(\"--- Iniciando Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o de Blocos Prefect ---\")

# --- Bloco Secret ---
secret_name = \"github-access-token\"
github_pat = os.getenv(\"GITHUB_PAT\")
print(f\"Processando Bloco Secret '{secret_name}'...\")
if github_pat:
    try:
        secret_block = Secret(value=github_pat)
        secret_block.save(name=secret_name, overwrite=True)
        print(f\"-> Bloco Secret '{secret_name}' salvo com sucesso.\")
    except Exception as e:
        print(f\"Erro ao salvar Bloco Secret '{secret_name}': {e}\")
else:
    print(f\"AVISO: Vari\u00e1vel de ambiente GITHUB_PAT n\u00e3o definida. Bloco '{secret_name}' n\u00e3o criado/atualizado.\")

# --- Bloco JSON para DB Pool ---
db_block_name = \"postgres-pool\"
db_config = {
    \"dsn\": os.getenv(\"DATABASE_URL\", \"postgresql://user:password@db:5432/pipedrive\"),
    \"minconn\": int(os.getenv(\"DB_MIN_CONN\", 1)),
    \"maxconn\": int(os.getenv(\"DB_MAX_CONN\", 10)) 
}
print(f\"Processando Bloco JSON '{db_block_name}'...\")
try:
    json_block_db = JSON(value=db_config)
    json_block_db.save(name=db_block_name, overwrite=True)
    print(f\"-> Bloco JSON '{db_block_name}' salvo com sucesso.\")
except Exception as e:
    print(f\"Erro ao salvar Bloco JSON '{db_block_name}': {e}\")


# --- Bloco JSON para Redis Cache ---
redis_block_name = \"redis-cache\"
redis_config = {
    \"connection_string\": os.getenv(\"REDIS_URL\", \"redis://redis:6379/0\")
}
print(f\"Processando Bloco JSON '{redis_block_name}'...\")
try:
    json_block_redis = JSON(value=redis_config)
    json_block_redis.save(name=redis_block_name, overwrite=True)
    print(f\"-> Bloco JSON '{redis_block_name}' salvo com sucesso.\")
except Exception as e:
    print(f\"Erro ao salvar Bloco JSON '{redis_block_name}': {e}\")


# --- Blocos KubernetesJob ---
# Configura\u00e7\u00f5es comuns que podem ser reutilizadas
default_image = \"pipedrive_metabase_integration-etl:latest\"
default_namespace = \"default\"
default_env_from = [
    {\"secretRef\": {\"name\": \"app-secrets\"}},
    {\"secretRef\": {\"name\": \"db-secrets\"}},
]
default_env = {
    \"PUSHGATEWAY_ADDRESS\": os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\"),
    \"PREFECT_API_URL\": \"http://prefect-orion:4200/api\"
}
default_init_containers = [
    { 
        \"name\": \"wait-for-db\", 
        \"image\": \"busybox:1.36\", 
        \"command\": ['sh', '-c', 'echo Waiting for db...; while ! nc -z -w 1 db 5432; do sleep 2; done; echo DB ready.'] 
    },
    { 
        \"name\": \"wait-for-redis\", 
        \"image\": \"busybox:1.36\", 
        \"command\": ['sh', '-c', 'echo Waiting for redis...; while ! nc -z -w 1 redis 6379; do sleep 2; done; echo Redis ready.'] 
    },
    { 
        \"name\": \"wait-for-orion\", 
        \"image\": \"curlimages/curl:latest\", 
        \"command\": ['sh', '-c', 'echo Waiting for orion...; until curl -sf http://prefect-orion:4200/api/health > /dev/null; do echo -n \".\"; sleep 3; done; echo Orion ready.'] 
    }
]
default_job_watch_timeout = 120

# Fun\u00e7\u00e3o helper para criar o dicion\u00e1rio do Job Template
def create_job_spec_dict(image: str, resources: dict, pod_labels: Optional[dict] = None) -> dict:
    \"\"\"Cria manualmente a estrutura do Job Kubernetes.\"\"\"
    labels = {\"app.kubernetes.io/created-by\": \"prefect\"}
    if pod_labels:
        labels.update(pod_labels)

    job_spec = {
        \"metadata\": {\"labels\": labels},
        \"spec\": {
            \"template\": {
                \"spec\": {
                    \"initContainers\": default_init_containers,
                    \"containers\": [
                        {
                            \"name\": \"prefect-job\",
                            \"image\": image,
                            \"resources\": resources,
                            \"envFrom\": default_env_from,
                            \"env\": [{\"name\": k, \"value\": v} for k, v in default_env.items()],
                        }
                    ],
                }
            }
        }
    }
    return job_spec

# 1. Bloco para Infraestrutura K8s Padr\u00e3o
default_k8s_job_block_name = \"default-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{default_k8s_job_block_name}'...\")
try:
    default_resources = {
        \"requests\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"},
        \"limits\": {\"memory\": \"4Gi\", \"cpu\": \"2\"}
    }
    # Gerar o dicion\u00e1rio completo do Job
    default_job_dict = create_job_spec_dict(image=default_image, resources=default_resources)

    # Instanciar o Bloco usando o par\u00e2metro v1_job
    default_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=default_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    default_k8s_job_block.save(name=default_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{default_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{default_k8s_job_block_name}'\", exc_info=True)


# 2. Bloco para Infraestrutura K8s do Experimento
experiment_k8s_job_block_name = \"experiment-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{experiment_k8s_job_block_name}'...\")
try:
    experiment_resources = {
        \"requests\": {\"memory\": \"2Gi\", \"cpu\": \"1\"},
        \"limits\": {\"memory\": \"8Gi\", \"cpu\": \"2\"}
    }
    experiment_job_dict = create_job_spec_dict(image=default_image, resources=experiment_resources, pod_labels={\"flow\": \"experiment\"})

    experiment_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=experiment_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    experiment_k8s_job_block.save(name=experiment_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{experiment_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{experiment_k8s_job_block_name}'\", exc_info=True)


# 3. Bloco para Syncs Leves
light_sync_k8s_job_block_name = \"light-sync-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{light_sync_k8s_job_block_name}'...\")
try:
    light_sync_resources = {
         \"requests\": {\"memory\": \"512Mi\", \"cpu\": \"250m\"},
         \"limits\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"}
    }
    light_sync_job_dict = create_job_spec_dict(image=default_image, resources=light_sync_resources, pod_labels={\"flow\": \"light-sync\"})

    light_sync_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=light_sync_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    light_sync_k8s_job_block.save(name=light_sync_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{light_sync_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{light_sync_k8s_job_block_name}'\", exc_info=True)


print(\"--- Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o de Blocos Prefect Conclu\u00edda ---\")
"}
,
{"path": ".gitignore", "encoding": "utf-8", "content": "# Ignorar todos os diret\u00f3rios __pycache__
**/__pycache__/
/__pycache/

# Ignorar arquivos espec\u00edficos
.qodo
.env

db-secrets.yaml
kubectl*

"}
,
{"path": "flows/pipedrive_deployments.py", "encoding": "utf-8", "content": "import os
from datetime import timedelta
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import IntervalSchedule, CronSchedule
from prefect_kubernetes.jobs import KubernetesJob

# Importar TODOS os fluxos
from flows.pipedrive_metabase_etl import (
    main_etl_flow,
    backfill_stage_history_flow,
    batch_size_experiment_flow
)
# Importar novos fluxos de sync
from flows.pipedrive_sync_aux import (
    sync_pipedrive_users_flow,
    sync_pipedrive_persons_orgs_flow,
    sync_pipedrive_stages_pipelines_flow
)

# --- Configura\u00e7\u00e3o Comum da Infraestrutura K8s Job ---
K8S_IMAGE_NAME = \"pipedrive_metabase_integration-etl:latest\"
K8S_NAMESPACE = \"default\" 
WORK_QUEUE_NAME = \"kubernetes\" 

# Definir recursos padr\u00e3o para os jobs de fluxo
DEFAULT_JOB_RESOURCES = {
    \"requests\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"},
    \"limits\": {\"memory\": \"4Gi\", \"cpu\": \"2\"}
}

# Definir Init Containers padr\u00e3o para aguardar depend\u00eancias
DEFAULT_INIT_CONTAINERS = [
    {
        \"name\": \"wait-for-db\",
        \"image\": \"busybox:1.36\",
        \"command\": ['sh', '-c', 'echo Waiting for db service...; while ! nc -z -w 1 db 5432; do sleep 2; done; echo DB ready.'],
    },
    {
        \"name\": \"wait-for-redis\",
        \"image\": \"busybox:1.36\",
        \"command\": ['sh', '-c', 'echo Waiting for redis service...; while ! nc -z -w 1 redis 6379; do sleep 2; done; echo Redis ready.'],
    },
     {
        \"name\": \"wait-for-orion\",
        \"image\": \"curlimages/curl:latest\", 
        \"command\": ['sh', '-c', 'echo Waiting for orion service...; until curl -sf http://prefect-orion:4200/api/health > /dev/null; do echo -n \".\"; sleep 3; done; echo Orion ready.'],
    }
]

# Definir vari\u00e1veis de ambiente e secrets comuns
DEFAULT_ENV_FROM = [
    {\"secretRef\": {\"name\": \"app-secrets\"}},
    {\"secretRef\": {\"name\": \"db-secrets\"}},
]
PUSHGATEWAY_SVC_ADDRESS = os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\")

DEFAULT_ENV = {
    \"PUSHGATEWAY_ADDRESS\": PUSHGATEWAY_SVC_ADDRESS,
    \"PREFECT_API_URL\": \"http://prefect-orion:4200/api\"
}

k8s_job_infra = KubernetesJob(
    image=K8S_IMAGE_NAME,
    namespace=K8S_NAMESPACE,
    job=KubernetesJob.job_template(
        metadata={
            \"labels\": {\"app.kubernetes.io/created-by\": \"prefect\"}
        },
        spec={
            \"template\": {
                \"spec\": {
                    \"initContainers\": DEFAULT_INIT_CONTAINERS,
                    \"containers\": [
                        {
                            \"name\": \"prefect-job\", 
                            \"resources\": DEFAULT_JOB_RESOURCES,
                            \"envFrom\": DEFAULT_ENV_FROM,       
                            \"env\": [ {\"name\": k, \"value\": v} for k, v in DEFAULT_ENV.items() ], 
                        }
                    ],
                }
            }
        }
    ),
    # Timeout para esperar o Job K8s INICIAR (n\u00e3o completar)
    job_watch_timeout_seconds=120,
)

# --- Deployment para Main Sync ---
main_sync_deployment = Deployment.build_from_flow(
    flow=main_etl_flow, 
    name=\"Pipedrive Sync\",
    description=\"Sincroniza deals recentes do Pipedrive com o banco de dados, usando lookups no DB.\",
    version=\"2.0\", 
    tags=[\"pipedrive\", \"sync\", \"etl\", \"main\"],
    # schedule=IntervalSchedule(interval=timedelta(minutes=30)), 
    parameters={}, 
    infrastructure=k8s_job_infra,
    work_queue_name=WORK_QUEUE_NAME,
    concurrency_limit=1
)

# --- Deployment para Backfill ---
backfill_deployment = Deployment.build_from_flow(
    flow=backfill_stage_history_flow,
    name=\"Pipedrive Backfill Stage History\",
    description=\"Preenche o hist\u00f3rico de stages para deals antigos.\",
    version=\"1.0\",
    tags=[\"pipedrive\", \"backfill\", \"history\"],
    # SEM schedule inicial, ser\u00e1 disparado por automa\u00e7\u00e3o
    schedule=None,
    # Par\u00e2metros padr\u00e3o para este deployment
    parameters={\"daily_deal_limit\": 10000, \"db_batch_size\": 1000},
    infrastructure=k8s_job_infra,
    work_queue_name=WORK_QUEUE_NAME,
)

# Sync Users
users_sync_deployment = Deployment.build_from_flow(
    flow=sync_pipedrive_users_flow,
    name=\"Sync Pipedrive Users\",
    description=\"Sincroniza a tabela pipedrive_users com a API.\",
    version=\"1.0\",
    tags=[\"pipedrive\", \"sync\", \"aux\", \"users\"],
    schedule=CronSchedule(cron=\"0 3 * * *\", timezone=\"America/Sao_Paulo\"), 
    parameters={},
    infrastructure=k8s_job_infra.duplicate(update={ 
         \"job\": KubernetesJob.job_template(
             spec={ \"template\": { \"spec\": {
                 \"initContainers\": DEFAULT_INIT_CONTAINERS,
                 \"containers\": [{
                     \"name\": \"prefect-job\",
                     \"resources\": {
                         \"requests\": {\"memory\": \"512Mi\", \"cpu\": \"250m\"},
                         \"limits\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"}
                     },
                     \"envFrom\": DEFAULT_ENV_FROM, \"env\": [ {\"name\": k, \"value\": v} for k, v in DEFAULT_ENV.items() ]
                 }]
             }}}
         )
    }),
    work_queue_name=WORK_QUEUE_NAME,
    concurrency_limit=1
)

# Sync Persons & Orgs
persons_orgs_sync_deployment = Deployment.build_from_flow(
    flow=sync_pipedrive_persons_orgs_flow,
    name=\"Sync Pipedrive Persons and Orgs\",
    description=\"Sincroniza as tabelas pipedrive_persons e pipedrive_organizations.\",
    version=\"1.0\",
    tags=[\"pipedrive\", \"sync\", \"aux\", \"persons\", \"orgs\"],
    schedule=IntervalSchedule(interval=timedelta(hours=4)),
    parameters={},
    infrastructure=k8s_job_infra, 
    work_queue_name=WORK_QUEUE_NAME,
    concurrency_limit=1
)

# Sync Stages & Pipelines
stages_pipelines_sync_deployment = Deployment.build_from_flow(
    flow=sync_pipedrive_stages_pipelines_flow,
    name=\"Sync Pipedrive Stages and Pipelines\",
    description=\"Sincroniza as tabelas pipedrive_stages e pipedrive_pipelines.\",
    version=\"1.0\",
    tags=[\"pipedrive\", \"sync\", \"aux\", \"stages\", \"pipelines\"],
     schedule=CronSchedule(cron=\"0 4 * * *\", timezone=\"America/Sao_Paulo\"),
    parameters={},
    infrastructure=users_sync_deployment.infrastructure,
    work_queue_name=WORK_QUEUE_NAME,
    concurrency_limit=1
)


batch_experiment_deployment = Deployment.build_from_flow(
    flow=batch_size_experiment_flow,
    name=\"Batch Size Experiment\",
    description=\"Testa diferentes tamanhos de batch para otimiza\u00e7\u00e3o de performance.\",
    version=\"1.0\",
    tags=[\"experiment\", \"batch-size\", \"optimization\"],
    schedule=None,
    parameters={
        \"batch_sizes\": [300, 500, 750, 1000, 1500],
        \"test_data_size\": 10000
    },
    infrastructure=k8s_job_infra.duplicate(
        update={
            \"job\": KubernetesJob.job_template(
                metadata={
                    \"labels\": {\"app.kubernetes.io/created-by\": \"prefect\"}
                },
                spec={
                    \"template\": {
                        \"spec\": {
                            \"initContainers\": DEFAULT_INIT_CONTAINERS,
                            \"containers\": [
                                {
                                    \"name\": \"prefect-job\",
                                    \"resources\": {
                                        \"requests\": {\"memory\": \"2Gi\", \"cpu\": \"1\"},
                                        \"limits\": {\"memory\": \"8Gi\", \"cpu\": \"2\"}
                                    },
                                    \"envFrom\": DEFAULT_ENV_FROM,
                                    \"env\": [{\"name\": k, \"value\": v} for k, v in DEFAULT_ENV.items()],
                                }
                            ],
                        }
                    }
                }
            )
        }
    ),
    work_queue_name=WORK_QUEUE_NAME,
)

# Bloco para aplicar os deployments
if __name__ == \"__main__\":
    print(\"Applying Pipedrive Sync deployment...\")
    main_sync_deployment.apply()

    print(\"Applying Backfill deployment...\")
    backfill_deployment.apply()

    print(\"Applying Batch Experiment deployment...\")
    batch_experiment_deployment.apply()

    print(\"Applying Users Sync deployment...\")
    users_sync_deployment.apply()

    print(\"Applying Persons & Orgs Sync deployment...\")
    persons_orgs_sync_deployment.apply()

    print(\"Applying Stages & Pipelines Sync deployment...\")
    stages_pipelines_sync_deployment.apply()

    print(\"\nDeployments aplicados com sucesso!\")
    print(f\"Garanta que um agente Prefect esteja rodando e escutando a fila '{WORK_QUEUE_NAME}'.\")
    print(f\"Exemplo: prefect agent start -q {WORK_QUEUE_NAME}\")
    print(\"Configure as Automa\u00e7\u00f5es na UI do Prefect para orquestrar os fluxos.\")"}
,
{"path": "flows/pipedrive_sync_aux.py", "encoding": "utf-8", "content": "from typing import Generator, List, Dict, Optional
import structlog
from prefect import flow, task, get_run_logger

from application.ports.pipedrive_client_port import PipedriveClientPort
from infrastructure.repository_impl.pipedrive_repository import PipedriveRepository
from flows.pipedrive_metabase_etl import initialize_components_no_maps
from infrastructure.monitoring.metrics import (
  push_metrics_to_gateway, records_synced_counter, sync_counter, sync_failure_counter,db_operation_duration_hist
)

log = structlog.get_logger(__name__)

# --- Tasks Gen\u00e9ricas de Sincroniza\u00e7\u00e3o ---
@task(name=\"Sync Pipedrive Entity Task\", retries=2, retry_delay_seconds=60, cache_policy=None)
def sync_entity_task(
    entity_type: str,
    client: PipedriveClientPort,
    repository: PipedriveRepository
    ) -> int:
    \"\"\"
    Task gen\u00e9rica para buscar dados de uma entidade Pipedrive via stream
    e fazer upsert no reposit\u00f3rio.
    \"\"\"
    logger = get_run_logger()
    logger.info(f\"Starting sync task for entity: {entity_type}\")
    sync_counter.labels(entity_type=entity_type).inc()
    total_synced = 0
    processed_count = 0
    batch_size = 500
    data_batch: List[Dict] = []
    stream: Optional[Generator[Dict, None, None]] = None
    upsert_method: Optional[callable] = None

    # Determinar o m\u00e9todo de stream e upsert baseado no entity_type
    if entity_type == \"users\":
        stream = client._fetch_paginated_v1_stream_adapter(f\"{client.BASE_URL_V1}/users\") 
        upsert_method = repository.upsert_users
    elif entity_type == \"persons\":
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/persons\")
        upsert_method = repository.upsert_persons
    elif entity_type == \"stages\":
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/stages\")
        upsert_method = repository.upsert_stages
    elif entity_type == \"pipelines\":
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/pipelines\") 
        upsert_method = repository.upsert_pipelines
    elif entity_type == \"organizations\":
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/organizations\") 
        upsert_method = repository.upsert_organizations
    else:
        logger.error(f\"Unknown entity type for sync task: {entity_type}\")
        raise ValueError(f\"Unknown entity type: {entity_type}\")

    if not stream or not upsert_method:
         logger.error(f\"Stream or upsert method not defined for entity: {entity_type}\")
         raise NotImplementedError(f\"Sync not implemented for {entity_type}\")

    try:
        for item in stream:
            data_batch.append(item)
            processed_count += 1
            if len(data_batch) >= batch_size:
                logger.debug(f\"Upserting batch of {len(data_batch)} {entity_type}...\")
                with db_operation_duration_hist.labels(operation=f\"sync_{entity_type}\").time():
                    upserted = upsert_method(data_batch)
                total_synced += upserted
                records_synced_counter.labels(entity_type=entity_type).inc(upserted)
                data_batch = []

        if data_batch:
            logger.debug(f\"Upserting final batch of {len(data_batch)} {entity_type}...\")
            with db_operation_duration_hist.labels(operation=f\"sync_{entity_type}\").time():
                upserted = upsert_method(data_batch)
            total_synced += upserted
            records_synced_counter.labels(entity_type=entity_type).inc(upserted)

        logger.info(f\"Sync task completed for {entity_type}\")
        return total_synced

    except Exception as e:
        logger.error(f\"Sync task failed for {entity_type}\", exc_info=True)
        sync_failure_counter.labels(entity_type=entity_type).inc()
        raise e

# --- Fluxos Espec\u00edficos de Sincroniza\u00e7\u00e3o ---
@flow(name=\"Sync Pipedrive Users\", log_prints=True)
def sync_pipedrive_users_flow():
    logger = get_run_logger()
    logger.info(\"Starting Pipedrive Users sync flow...\")
    flow_run_id = getattr(logger, \"extra\", {}).get(\"flow_run_id\", \"local_sync_users\")
    try:
        client, repository, _ = initialize_components_no_maps()
        sync_entity_task(entity_type=\"users\", client=client, repository=repository)
        logger.info(\"Pipedrive Users sync flow finished successfully.\")
    except Exception as e:
         logger.critical(f\"Pipedrive Users sync flow failed. Error: {e}\", exc_info=True)
    finally:
        push_metrics_to_gateway(job_name=\"pipedrive_sync_users\", grouping_key={'flow_run_id': str(flow_run_id)})


@flow(name=\"Sync Pipedrive Persons and Orgs\", log_prints=True)
def sync_pipedrive_persons_orgs_flow():
    logger = get_run_logger()
    logger.info(\"Starting Pipedrive Persons & Orgs sync flow...\")
    flow_run_id = get_run_logger().extra.get(\"flow_run_id\", \"local_sync_persons_orgs\")
    try:
        client, repository, _ = initialize_components_no_maps()
        sync_entity_task(entity_type=\"persons\", client=client, repository=repository)
        sync_entity_task(entity_type=\"organizations\", client=client, repository=repository)
        logger.info(\"Pipedrive Persons & Orgs sync flow finished successfully.\")
    except Exception as e:
         logger.critical(\"Pipedrive Persons & Orgs sync flow failed.\", exc_info=True)
    finally:
        push_metrics_to_gateway(job_name=\"pipedrive_sync_persons_orgs\", grouping_key={'flow_run_id': str(flow_run_id)})


@flow(name=\"Sync Pipedrive Stages and Pipelines\", log_prints=True)
def sync_pipedrive_stages_pipelines_flow():
    logger = get_run_logger()
    logger.info(\"Starting Pipedrive Stages & Pipelines sync flow...\")
    flow_run_id = get_run_logger().extra.get(\"flow_run_id\", \"local_sync_stages_pipelines\")
    try:
        client, repository, _ = initialize_components_no_maps()
        sync_entity_task(entity_type=\"stages\", client=client, repository=repository)
        sync_entity_task(entity_type=\"pipelines\", client=client, repository=repository)
        logger.info(\"Pipedrive Stages & Pipelines sync flow finished successfully.\")
    except Exception as e:
         logger.critical(\"Pipedrive Stages & Pipelines sync flow failed.\", exc_info=True)
    finally:
        push_metrics_to_gateway(job_name=\"pipedrive_sync_stages_pipelines\", grouping_key={'flow_run_id': str(flow_run_id)})
"}
,
{"path": "flows/pipedrive_metabase_etl.py", "encoding": "utf-8", "content": "import time
from typing import Any, Dict, List, Tuple
import pandas as pd
import structlog
from prefect import flow, get_run_logger, task, context
from prefect.blocks.system import JSON
import logging

from application.services.etl_service import ETLService
from flows.utils.flows_utils import calculate_optimal_batch_size, get_optimal_batch_size, validate_loaded_data, update_optimal_batch_config
from infrastructure.api_clients.pipedrive_api_client import PipedriveAPIClient
from infrastructure.cache import RedisCache
from infrastructure.db_pool import DBConnectionPool
from infrastructure.logging_config import setup_logging
from infrastructure.repository_impl.pipedrive_repository import PipedriveRepository
from infrastructure.monitoring.metrics import (
    push_metrics_to_gateway,
    etl_counter,
    etl_failure_counter,
    memory_usage_gauge,
    batch_size_gauge,
    backfill_deals_remaining_gauge,
    batch_experiment_counter,
    etl_heartbeat,
    batch_experiment_success_rate,
    batch_experiment_best_score,
    etl_duration_hist
)

log = structlog.get_logger() 

# Constantes e Configura\u00e7\u00f5es
DEFAULT_OPTIMAL_BATCH_SIZE = 1000 
DEFAULT_MAIN_FLOW_TIMEOUT = 9000
DEFAULT_BACKFILL_FLOW_TIMEOUT = 10800
BACKFILL_BATCH_SIZE = 1000
BACKFILL_DAILY_LIMIT = 2000
DEFAULT_TASK_RETRIES = 3
DEFAULT_TASK_RETRY_DELAY = 60

@task(name=\"Initialize ETL Components (No Maps)\", retries=2, retry_delay_seconds=30)
def initialize_components_no_maps() -> Tuple[PipedriveAPIClient, PipedriveRepository, ETLService]:
    \"\"\"Inicializa componentes principais SEM buscar mapas na mem\u00f3ria.\"\"\"
    task_log = get_run_logger()
    task_log.info(\"Initializing ETL components (Lookups via DB)...\")

    # Carregar configura\u00e7\u00f5es de conex\u00e3o 
    postgres_config = JSON.load(\"postgres-pool\").value
    redis_config = JSON.load(\"redis-cache\").value

    db_pool = DBConnectionPool(
        minconn=postgres_config.get(\"minconn\", 1),
        maxconn=postgres_config.get(\"maxconn\", 10), 
        dsn=postgres_config[\"dsn\"]
    )
    redis_cache = RedisCache(connection_string=redis_config[\"connection_string\"])
    pipedrive_client = PipedriveAPIClient(cache=redis_cache)

    # Repository ainda precisa de stages/fields para schema e backfill
    try:
        all_stages = pipedrive_client.fetch_all_stages_details()
        if not all_stages:
             task_log.warning(\"Fetched stage details list is empty!\")
    except Exception as stage_err:
        task_log.error(\"Failed to fetch stage details during initialization.\", error=str(stage_err))
        raise stage_err

    try:
        custom_mapping = pipedrive_client.fetch_deal_fields_mapping()
    except Exception as mapping_err:
        task_log.error(\"Failed to fetch custom field mapping.\", error=str(mapping_err))
        raise mapping_err

    # Repository garante o schema
    repository = PipedriveRepository(
        db_pool=db_pool,
        custom_field_api_mapping=custom_mapping,
        all_stages_details=all_stages
    )

    # ETLService \u00e9 inicializado com batch_size default, ser\u00e1 sobrescrito no flow
    etl_service = ETLService(
        client=pipedrive_client,
        repository=repository,
        batch_size=DEFAULT_OPTIMAL_BATCH_SIZE 
    )
    task_log.info(\"ETL components initialized (Lookups via DB).\")
    return pipedrive_client, repository, etl_service


@flow(
    name=\"Pipedrive to Database ETL Flow (Main Sync)\",
    log_prints=True,
    timeout_seconds=DEFAULT_MAIN_FLOW_TIMEOUT
)
def main_etl_flow():
    \"\"\"Fluxo principal ETL: busca deals recentes e usa lookups no DB.\"\"\"
    setup_logging(level=logging.INFO)
    flow_log = get_run_logger()
    ctx = context.get_run_context()
    if hasattr(ctx, \"flow_run\"):
        flow_run_id = ctx.flow_run.id
        flow_run_name = ctx.flow_run.name
    else:
        flow_run_id = \"local_main_sync\"
        flow_run_name = \"MainSyncRun\"
    flow_log.info(\"Main ETL flow started\", extra={\"flow_run_id\": flow_run_id})

    flow_log.info(\"Main ETL flow started\", extra={\"flow_run_id\": str(flow_run_id)})
    flow_type = \"sync\" 
    etl_counter.labels(flow_type=flow_type).inc()
    flow_start = time.monotonic()

    flow_log.info(f\"Starting flow run '{flow_run_name}'...\")
    result = {}

    try:
        # 1. Inicializa componentes (incluindo busca de stages/fields)
        _, repository, etl_service = initialize_components_no_maps()

        # 2. Executa o ETL principal (run_etl agora foca na sincroniza\u00e7\u00e3o atual)
        optimal_batch_size = get_optimal_batch_size(repository, default_size=DEFAULT_OPTIMAL_BATCH_SIZE)
        flow_log.info(f\"Using optimal batch size from config: {optimal_batch_size}\")
        etl_service.process_batch_size = optimal_batch_size 
        
        # 3. Executa o ETL principal
        result = etl_service.run_etl(flow_type=flow_type)

        # --- Valida\u00e7\u00e3o e M\u00e9tricas ---
        flow_log.info(\"Performing post-load validation.\")
        status = result.get(\"status\", \"error\")
        processed = result.get(\"total_loaded\", 0) 
        duration = result.get(\"duration_seconds\", -1)
        peak_mem = result.get(\"peak_memory_mb\", -1)

        if status != \"success\":
            etl_failure_counter.labels(flow_type=flow_type).inc() 
            message = result.get(\"message\", \"Unknown error\")
            flow_log.critical(f\"ETL task reported failure. Message: {message}\")
            raise RuntimeError(f\"ETL task failed: {result.get('message', 'Unknown error')}\")


        assert processed >= 0, \"Processed count cannot be negative.\"
        if result.get(\"total_fetched\", 0) > 0 and processed == 0:
            flow_log.warning(\"ETL fetched data but loaded zero records.\", fetched=result.get(\"total_fetched\"))
            raise ValueError(\"ETL fetched data but loaded zero records.\")

        sla_duration = 3600
        if duration < 0 or duration >= sla_duration:
             flow_log.warning(f\"ETL duration ({duration:.2f}s) outside expected range (0-{sla_duration}s)\")

        sla_memory_mb = 4 * 1024
        if peak_mem > 0 and peak_mem >= sla_memory_mb:
             flow_log.warning(f\"ETL peak memory ({peak_mem:.2f}MB) approached or exceeded limit ({sla_memory_mb}MB)\")

        peak_mem_str = f\"{peak_mem:.2f}\" if peak_mem > 0 else \"N/A\"
        flow_log.info(
            f\"ETL flow completed successfully. \"
            f\"Processed Records: {processed}, \"
            f\"Duration: {duration:.2f}s, \"
            f\"Peak Memory: {peak_mem_str}MB\"
        )
        
        if peak_mem > 0:
             memory_usage_gauge.labels(flow_type=flow_type).set(peak_mem)
        return result 

    except Exception as e:
        if result.get(\"status\") != \"error\": 
            etl_failure_counter.labels(flow_type=flow_type).inc()
        flow_log.critical(f\"Main ETL flow failed critically: {str(e)}\", exc_info=True)
        raise 

    finally:
        etl_heartbeat.labels(flow_type=flow_type).set_to_current_time()
        flow_log.info(\"Pushing metrics to Pushgateway for main sync flow.\")
        duration = time.monotonic() - flow_start
        etl_duration_hist.labels(flow_type=flow_type).observe(duration)
        push_metrics_to_gateway(job_name=\"pipedrive_sync_job\", grouping_key={'flow_run_id': str(flow_run_id)})

    
@task(name=\"Get Deals for Backfill Task\", retries=1)
def get_deals_for_backfill_task(limit: int) -> List[str]:
    \"\"\"Busca IDs de deals que precisam de backfill.\"\"\"
    logger = get_run_logger()
    logger.info(f\"Fetching up to {limit} deal IDs for history backfill.\")
    _, repository, _ = initialize_components_no_maps()
    ids = repository.get_deals_needing_history_backfill(limit=limit)
    logger.info(f\"Found {len(ids)} deals for this backfill batch.\")
    return ids

@task(name=\"Get Backfill Remaining Count Task\", retries=1)
def get_initial_backfill_count_task() -> int:
    \"\"\"Busca a contagem inicial de deals que precisam de backfill.\"\"\"
    logger = get_run_logger()
    logger.info(\"Counting total deals needing history backfill.\")
    _, repository, _ = initialize_components_no_maps()
    count = repository.count_deals_needing_backfill()
    if count >= 0:
         logger.info(f\"Estimated {count} deals remaining for backfill.\")
         backfill_deals_remaining_gauge.set(count) 
    else:
         logger.warning(\"Failed to get backfill remaining count.\")
         backfill_deals_remaining_gauge.set(-1) 
    return count

@task(
    name=\"Run Backfill Batch Task\", 
    retries=DEFAULT_TASK_RETRIES, 
    retry_delay_seconds=DEFAULT_TASK_RETRY_DELAY, 
    log_prints=True,
)
def run_backfill_batch_task(deal_ids: List[str]) -> Dict[str, Any]: 
    \"\"\"Executa o backfill para um lote de IDs.\"\"\"
    logger = get_run_logger()
    flow_type=\"backfill\"
    if not deal_ids:
        logger.info(\"No deals in this batch to backfill.\")
        return {\"status\": \"skipped\", \"processed_deals\": 0}
    batch_size_gauge.labels(flow_type=flow_type).set(len(deal_ids))
    logger.info(f\"Running backfill for {len(deal_ids)} deals.\")
    _, _, etl_service = initialize_components_no_maps()
    result = etl_service.run_retroactive_backfill(deal_ids)
    logger.info(\"Backfill batch finished.\", extra=result)
    return result

@flow(
    name=\"Batch Size Experiment Flow - Enhanced\",
    log_prints=True,
    timeout_seconds=10800  # 3 horas para experimentos longos
)
def batch_size_experiment_flow(
    batch_sizes: List[int] = [300, 500, 750, 1000, 1500, 2000],
    test_data_size: int = 10000  # Quantidade de dados reais a serem usados
):
    \"\"\"Fluxo aprimorado para experimentos de tamanho de batch com an\u00e1lise autom\u00e1tica e valida\u00e7\u00e3o.\"\"\"
    setup_logging(level=logging.INFO)
    flow_log = get_run_logger()
    ctx = context.get_run_context()
    if hasattr(ctx, \"flow_run\"):
        flow_run_id = ctx.flow_run.id
    else:
        flow_run_id = \"local_experiment\"

    try:
        # 1. Buscar dados reais para o teste
        client, repository, etl_service = initialize_components_no_maps()
        flow_log.info(f\"Fetching {test_data_size} recent deals for testing...\")
        test_data = list(client.fetch_all_deals_stream(items_limit=test_data_size))
        
        if not test_data:
            raise ValueError(\"No test data available for experiment\")

        # 2. Executar experimentos
        results = []
        for size in batch_sizes:
            flow_log.info(f\"Starting experiment with batch size: {size}\")
            
            # Configurar m\u00e9tricas
            metrics_labels = {
                \"experiment\": \"batch_size\", 
                \"batch_size\": str(size),
                \"flow_run_id\": str(flow_run_id)
            }
            
            # Executar ETL e coletar m\u00e9tricas
            with batch_experiment_counter.labels(**metrics_labels).count_exceptions():
                start_time = time.monotonic()
                
                # Processar dados de teste
                result = etl_service.run_etl_with_data(test_data, batch_size=size, flow_type=\"experiment\")
                
                duration = time.monotonic() - start_time
                records_processed = result.get(\"total_loaded\", 0)
                
                # Coletar m\u00e9tricas
                batch_metrics = {
                    'batch_size': size,
                    'duration': duration,
                    'throughput': records_processed / duration if duration > 0 else 0,
                    'memory_peak': result.get(\"peak_memory_mb\", 0),
                    'success_rate': result.get(\"success_rate\", 0),
                    'data_quality_issues': result.get(\"data_quality_issues\", 0)
                }
                
                # Publicar m\u00e9tricas em tempo real
                push_metrics_to_gateway(
                    job_name=\"batch_experiment\",
                    grouping_key=metrics_labels
                )
                
                # Valida\u00e7\u00e3o dos dados
                validation_result = validate_loaded_data(
                    repository=repository,
                    source_data=test_data,
                    batch_size=size
                )
                
                batch_metrics.update(validation_result)
                results.append(batch_metrics)

                flow_log.info(
                    \"Batch experiment completed\",
                    extra={
                        \"batch_size\": size,
                        \"duration\": duration,
                        \"throughput\": records_processed / duration if duration > 0 else 0,
                        \"memory_peak\": result.get(\"peak_memory_mb\", 0),
                        \"success_rate\": result.get(\"success_rate\", 0),
                        \"data_quality_issues\": result.get(\"data_quality_issues\", 0),
                        \"validation_result\": validation_result
                    }
                )
                
        # 3. An\u00e1lise Autom\u00e1tica
        optimal_size = calculate_optimal_batch_size(results)
        flow_log.info(
            \"Optimal batch size determined\",
            extra={
                \"optimal_batch_size\": optimal_size,
                \"analysis_metrics\": results
            }
        )

        # 4. Persistir resultados
        df = pd.DataFrame(results)
        df.to_csv('batch_metrics.csv', index=False)
        
        # 5. Atualizar configura\u00e7\u00e3o din\u00e2mica
        update_optimal_batch_config(repository, optimal_size)
        
        return {
            \"status\": \"completed\",
            \"optimal_batch_size\": optimal_size,
            \"detailed_results\": results
        }

    except Exception as e:
        flow_log.error(\"Batch experiment failed\", exc_info=str(e))
        raise


@flow(
    name=\"Pipedrive Stage History Backfill Flow\",
    log_prints=True,
    timeout_seconds=DEFAULT_BACKFILL_FLOW_TIMEOUT
)
def backfill_stage_history_flow(daily_deal_limit: int = BACKFILL_DAILY_LIMIT, db_batch_size: int = BACKFILL_BATCH_SIZE):
    \"\"\"Orquestra o backfill do hist\u00f3rico de stages em lotes.\"\"\"
    setup_logging(level=logging.INFO)
    base_flow_log = get_run_logger()
    ctx = context.get_run_context()
    if hasattr(ctx, \"flow_run\"):
        flow_run_id = ctx.flow_run.id
    else:
        flow_run_id = \"local_backfill\"

    flow_log = base_flow_log
    flow_log.info(f\"Starting backfill flow\", extra={\"flow_run_id\": flow_run_id})

    flow_log.info(f\"Starting stage history backfill flow. Daily limit: {daily_deal_limit}, DB batch size: {db_batch_size}\")
    flow_type = \"backfill\"
    etl_counter.labels(flow_type=flow_type).inc()
    flow_start = time.monotonic()

    total_processed_today = 0
    total_api_errors = 0
    total_processing_errors = 0
    all_batch_results = []
    final_status = \"completed\"
    initial_count = -1
    backfill_completed_successfully = False 
    result_payload = {}

    try:
        _, repository, _ = initialize_components_no_maps()
        initial_count = get_initial_backfill_count_task()

        while total_processed_today < daily_deal_limit:
            remaining_limit = daily_deal_limit - total_processed_today
            current_batch_limit = min(db_batch_size, remaining_limit)
            if current_batch_limit <= 0:
                 flow_log.info(\"Daily limit reached.\")
                 break

            flow_log.info(f\"Attempting to fetch next batch of deals (limit: {current_batch_limit}).\")
            deal_ids_batch = get_deals_for_backfill_task(limit=current_batch_limit)

            if not deal_ids_batch:
                flow_log.info(\"No more deals found needing backfill.\")
                backfill_deals_remaining_gauge.set(0)
                backfill_completed_successfully = (final_status == \"completed\")
                break 

            batch_result = run_backfill_batch_task(deal_ids_batch)
            all_batch_results.append(batch_result)

            processed_in_batch = batch_result.get(\"processed_deals\", 0)
            api_errors_in_batch = batch_result.get(\"api_errors\", 0)
            proc_errors_in_batch = batch_result.get(\"processing_errors\", 0)

            total_processed_today += processed_in_batch
            total_api_errors += api_errors_in_batch
            total_processing_errors += proc_errors_in_batch

            if batch_result.get(\"status\") != \"skipped\" and batch_result.get(\"status\") != \"success\":
                 final_status = \"completed_with_errors\"

            if initial_count >= 0:
                 current_remaining = max(0, initial_count - total_processed_today)
                 backfill_deals_remaining_gauge.set(current_remaining)

            flow_log.info(f\"Backfill batch completed. Processed so far today: {total_processed_today}/{daily_deal_limit}\")

            time.sleep(5)

        flow_log.info(\"Backfill flow finished for today.\",
                       total_processed=total_processed_today,
                       total_api_errors=total_api_errors,
                       total_processing_errors=total_processing_errors,
                       final_status=final_status)

        final_remaining_count = -1
        try:
            final_remaining_count = repository.count_deals_needing_backfill()
            if final_remaining_count >= 0:
                 backfill_deals_remaining_gauge.set(final_remaining_count)
                 if final_remaining_count == 0 and final_status == \"completed\":
                     backfill_completed_successfully = True
            else:
                 backfill_deals_remaining_gauge.set(-1)
                 final_status = \"completed_with_errors\" 
        except Exception as count_err:
            flow_log.error(\"Failed to get final remaining count\", error=str(count_err))
            backfill_deals_remaining_gauge.set(-1)
            final_status = \"completed_with_errors\"
            
        if final_status != \"completed\":
             etl_failure_counter.labels(flow_type=flow_type).inc()

        try:
             final_remaining = repository.count_deals_needing_backfill()
             if final_remaining >= 0:
                  backfill_deals_remaining_gauge.set(final_remaining) 
             else:
                  backfill_deals_remaining_gauge.set(-1)
        except Exception:
             flow_log.warning(\"Could not get final remaining count for backfill gauge.\")
             backfill_deals_remaining_gauge.set(-1)
             
        result_payload = {
            \"status\": final_status,
            \"total_processed_deals\": total_processed_today,
            \"total_api_errors\": total_api_errors,
            \"total_processing_errors\": total_processing_errors,
            \"batch_results\": all_batch_results,
            \"backfill_complete\": backfill_completed_successfully, 
            \"estimated_remaining\": final_remaining_count 
        }
        flow_log.info(\"Final backfill run result\", **result_payload)
        return result_payload

    except Exception as e:
        etl_failure_counter.labels(flow_type=flow_type).inc()
        flow_log.critical(\"Backfill flow failed critically.\", exc_info=True)
        final_status = \"failed\"
        raise

    finally:
        etl_heartbeat.labels(flow_type=flow_type).set_to_current_time()
        flow_log.info(\"Pushing metrics to Pushgateway for backfill flow.\")
        duration = time.monotonic() - flow_start
        etl_duration_hist.labels(flow_type=flow_type).observe(duration)
        push_metrics_to_gateway(job_name=\"pipedrive_backfill_job\", grouping_key={'flow_run_id': str(flow_run_id)})
        
@task(name=\"Calculate and Save Optimal Batch Size\")
def calculate_and_save_optimal_batch(
    results: List[Dict],
) -> int:
    \"\"\"
    Calcula o tamanho \u00f3timo de batch com base nas m\u00e9tricas e salva na config do DB.
    Retorna o tamanho \u00f3timo calculado.
    \"\"\"
    _, repository, _ = initialize_components_no_maps()
    flow_log = get_run_logger()
    # dentro de tasks, o contexto \u00e9 TaskRunContext, sem .flow_run, ent\u00e3o extra\u00edmos via task_run
    ctx = context.get_run_context()
    if hasattr(ctx, \"flow_run\"):
        flow_run_id = ctx.flow_run.id
    elif hasattr(ctx, \"task_run\") and hasattr(ctx.task_run, \"flow_run\"):
        flow_run_id = ctx.task_run.flow_run.id
    else:
        flow_run_id = \"calculate_and_save_optimal_batch\"
    flow_log.info(\"Starting batch size experiment flow.\", extra={\"flow_run_id\": str(flow_run_id)})
    logger = get_run_logger()
    if not results:
        logger.warning(\"No results provided for batch size calculation.\")
        return DEFAULT_OPTIMAL_BATCH_SIZE 

    df = pd.DataFrame(results)
    logger.info(\"Batch experiment results:\n\" + df.to_string())

    valid_df = df[
        (df['status'] == 'success') & 
        df['duration'].notna() & (df['duration'] > 0) &
        df['memory_peak'].notna() & (df['memory_peak'] >= 0) &
        df['data_quality_issues'].notna() & (df['data_quality_issues'] == 0)
    ].copy()

    if valid_df.empty:
        logger.error(f\"No valid results found after filtering for batch size calculation. Using default. Count: {len(df)}\")
        optimal_size = get_optimal_batch_size(repository, default_size=DEFAULT_OPTIMAL_BATCH_SIZE)
        logger.warning(f\"Could not calculate optimal size, will keep/use: {optimal_size}\")
        return optimal_size

    # Normaliza\u00e7\u00e3o e Score 
    # Priorizar throughput (loaded/duration), penalizar mem\u00f3ria e dura\u00e7\u00e3o alta
    valid_df['throughput'] = valid_df['total_loaded'] / valid_df['duration']
    max_throughput = valid_df['throughput'].max()
    max_memory = valid_df['memory_peak'].max()

    # Score: Maior throughput \u00e9 melhor, menor mem\u00f3ria \u00e9 melhor
    # Normalizar: throughput/max_throughput ; memory/max_memory
    valid_df['norm_throughput'] = valid_df['throughput'] / max_throughput if max_throughput > 0 else 0
    valid_df['norm_memory'] = valid_df['memory_peak'] / max_memory if max_memory > 0 else 0

    # Pesos (ajustar conforme prioridade)
    weight_throughput = 0.7
    weight_memory = 0.3

    valid_df['score'] = (weight_throughput * valid_df['norm_throughput']) + \
                        (weight_memory * (1 - valid_df['norm_memory'])) # 1 - norm_memory pq menor \u00e9 melhor

    best_idx = valid_df['score'].idxmax()
    best_run = valid_df.loc[best_idx]
    optimal_size = int(best_run['batch_size'])

    logger.info(
        f\"Optimal batch size calculated: {optimal_size} \"
        f\"(score={best_run['score']:.2f}, \"
        f\"throughput={best_run['throughput']:.2f}, \"
        f\"memory={best_run['memory_peak']}, \"
        f\"duration={best_run['duration']})\"
    )
    logger.info(\"Scores per batch size (valid runs):\n\" + \
                valid_df[['batch_size', 'score', 'throughput', 'memory_peak', 'duration']].round(3).to_string())
    
    batch_experiment_best_score.labels(flow_run_id=flow_run_id, metric=\"score\").set(best_run[\"score\"])
    batch_experiment_success_rate.labels(batch_size=str(optimal_size), flow_run_id=flow_run_id).set(best_run[\"success_rate\"])

    try:
        update_optimal_batch_config(repository, optimal_size) 
    except Exception as save_err:
        logger.error(\"Failed to save optimal batch size configuration\", error=str(save_err), exc_info=True)

    return optimal_size


@flow(
    name=\"Batch Size Experiment Flow - Enhanced\",
    log_prints=True,
    timeout_seconds=10800
)
def batch_size_experiment_flow(
    batch_sizes: List[int] = [300, 500, 750, 1000, 1500, 2000],
    test_data_size: int = 10000
):
    \"\"\"Testa tamanhos de batch, calcula e salva o \u00f3timo na config.\"\"\"
    setup_logging(level=logging.INFO)
    flow_log = get_run_logger()
    ctx = context.get_run_context()
    if hasattr(ctx, \"flow_run\"):
        flow_run_id = ctx.flow_run.id
    else:
        flow_run_id = \"local_experiment\"
    flow_log.info(\"Main ETL flow started\", extra={\"flow_run_id\": str(flow_run_id)})
    etl_counter.labels(flow_type=\"experiment\").inc()
    flow_start = time.monotonic()

    results = []
    optimal_size = DEFAULT_OPTIMAL_BATCH_SIZE 

    try:
        # 1. Inicializar componentes 
        client, repository, etl_service = initialize_components_no_maps()

        # 2. Buscar dados reais para o teste
        flow_log.info(f\"Fetching {test_data_size} recent deals for testing...\")
        # Usar list() para materializar o gerador para o teste
        test_data = list(client.fetch_all_deals_stream(items_limit=test_data_size))
        if not test_data:
            raise ValueError(f\"No test data ({test_data_size} deals) could be fetched.\")
        flow_log.info(f\"Fetched {len(test_data)} deals for experiment.\")


        # 3. Executar experimentos em loop
        for size in batch_sizes:
            flow_log.info(f\"Starting experiment with batch size: {size}\")
            run_result = {}
            validation_result = {\"data_quality_issues\": -1} 

            try:
                run_result = etl_service.run_etl_with_data(
                    data=test_data,
                    batch_size=size,
                    flow_type=\"experiment\"
                )

                if run_result.get(\"status\") == \"success\":
                     validation_result = {\"data_quality_issues\": 0} 

            except Exception as exp_err:
                 flow_log.error(f\"Experiment failed for batch size {size}\", error=str(exp_err), exc_info=True)
                 run_result = {\"status\": \"error\", \"message\": str(exp_err), \"batch_size\": size}

            # Coletar m\u00e9tricas b\u00e1sicas do resultado do run_etl
            metrics = {
                'batch_size': size,
                'status': run_result.get(\"status\", \"error\"),
                'duration': run_result.get(\"duration_seconds\"),
                'total_loaded': run_result.get(\"total_loaded\"),
                'total_failed': run_result.get(\"total_failed\"),
                'memory_peak': run_result.get(\"peak_memory_mb\"),
                **validation_result 
            }
            results.append(metrics)
            flow_log.info(\"Experiment completed\", extra={\"batch_size\": size, \"success\": True})

            # Pequena pausa entre testes
            time.sleep(5)

        # 4. An\u00e1lise e Persist\u00eancia do Tamanho \u00d3timo
        optimal_size = calculate_and_save_optimal_batch(results)

        return {
            \"status\": \"completed\",
            \"optimal_batch_size_calculated\": optimal_size,
            \"detailed_results\": results
        }

    except Exception as e:
        flow_log.critical(\"Batch experiment flow failed critically\", exc_info=True)
        raise
    finally:
        duration = time.monotonic() - flow_start
        etl_duration_hist.labels(flow_type=\"experiment\").observe(duration)
        push_metrics_to_gateway(job_name=\"batch_experiment\", grouping_key={'flow_run_id': str(flow_run_id)})


"}
,
{"path": "flows/utils/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "flows/utils/flows_utils.py", "encoding": "utf-8", "content": "from datetime import datetime, timezone
import random
from typing import Any, Dict, List
import pandas as pd
import structlog
from prefect.utilities.hashing import hash_objects

from infrastructure.repository_impl.pipedrive_repository import PipedriveRepository

log = structlog.get_logger(__name__)

def validate_loaded_data(
    repository: PipedriveRepository,
    source_data: List[Dict],
    batch_size: int
) -> Dict[str, Any]:
    \"\"\"Valida\u00e7\u00e3o completa dos dados carregados.\"\"\"
    validation_log = structlog.get_logger().bind(batch_size=batch_size)
    issues = 0
    validated = 0
    
    try:
        # 1. Verificar contagem b\u00e1sica
        expected_count = len(source_data)
        actual_count = repository.count_records()
        count_match = expected_count == actual_count
        
        # 2. Verificar IDs ausentes
        source_ids = {str(item['id']) for item in source_data if 'id' in item}
        db_ids = set(repository.get_all_ids())
        
        missing_ids = source_ids - db_ids
        extra_ids = db_ids - source_ids
        
        # 3. Amostragem de valida\u00e7\u00e3o detalhada
        sample_size = min(100, len(source_data))
        sample_records = random.sample(source_data, sample_size)
        detailed_issues = []
        
        for record in sample_records:
            db_data = repository.get_record_by_id(record['id'])
            if not db_data:
                detailed_issues.append(f\"Missing record {record['id']}\")
                continue
                
            # Verificar campos cr\u00edticos
            critical_fields = ['value', 'currency', 'status', 'stage_name']
            for field in critical_fields:
                source_val = record.get(field)
                db_val = db_data.get(field)
                
                if source_val != db_val:
                    detailed_issues.append(
                        f\"Field mismatch {field}: {source_val} vs {db_val}\"
                    )
                    issues += 1
        
        # 4. Verificar consist\u00eancia de datas
        date_issues = repository.validate_date_consistency()
        
        return {
            \"data_quality_issues\": issues + len(missing_ids) + len(extra_ids) + date_issues,
            \"count_match\": count_match,
            \"missing_ids_count\": len(missing_ids),
            \"extra_ids_count\": len(extra_ids),
            \"detailed_issues_sample\": detailed_issues[:5],
            \"date_issues\": date_issues
        }
        
    except Exception as e:
        validation_log.error(\"Data validation failed\", error=str(e))
        return {\"data_quality_issues\": -1, \"error\": str(e)}

def calculate_optimal_batch_size(results: List[Dict]) -> int:
    \"\"\"Calcula o tamanho ideal de batch com base nas m\u00e9tricas coletadas, ignorando falhas.\"\"\"
    default_batch_size = 1000 

    if not results:
        print(f\"WARN: No results provided, returning default batch size {default_batch_size}\")
        return default_batch_size

    df = pd.DataFrame(results)

    valid_df = df[
        (df['data_quality_issues'] != -1) &
        df['duration'].notna() & (df['duration'] > 0) &
        df['memory_peak'].notna() & (df['memory_peak'] >= 0)
    ].copy() 

    if valid_df.empty:
        print(f\"WARN: No valid results after filtering, returning default batch size {default_batch_size}\")
        print(\"Original results head:\n\", df.head())
        return default_batch_size

    # --- Normaliza\u00e7\u00e3o e C\u00e1lculo do Score (apenas em dados v\u00e1lidos) ---
    max_duration = valid_df['duration'].max()
    max_memory = valid_df['memory_peak'].max()
    max_quality_issues = valid_df['data_quality_issues'].max() 

    valid_df['norm_duration'] = valid_df['duration'] / max_duration if max_duration > 0 else 0
    valid_df['norm_memory'] = valid_df['memory_peak'] / max_memory if max_memory > 0 else 0

    if max_quality_issues > 0:
         valid_df['norm_quality'] = 1 - (valid_df['data_quality_issues'] / max_quality_issues)
    else:
         valid_df['norm_quality'] = 1.0 

    valid_df.fillna(0, inplace=True)

    weights = {
        'duration': 0.4,  # Menor dura\u00e7\u00e3o \u00e9 melhor (1 - norm_duration)
        'memory': 0.4,    # Menor mem\u00f3ria \u00e9 melhor (1 - norm_memory)
        'quality': 0.2    # Maior qualidade \u00e9 melhor (norm_quality)
    }

    valid_df['score'] = (
        weights['duration'] * (1 - valid_df['norm_duration']) +
        weights['memory'] * (1 - valid_df['norm_memory']) +
        weights['quality'] * valid_df['norm_quality']
    )

    best_idx = valid_df['score'].idxmax()
    best = valid_df.loc[best_idx]

    optimal_size = int(best['batch_size'])
    print(f\"INFO: Optimal batch size calculated: {optimal_size} based on score {best['score']:.3f}\")
    print(\"INFO: Scores per batch size (valid runs):\n\", valid_df[['batch_size', 'score', 'duration', 'memory_peak', 'data_quality_issues']])

    return optimal_size

def update_optimal_batch_config(repository: PipedriveRepository, optimal_size: int):
    \"\"\"Atualiza a configura\u00e7\u00e3o do tamanho \u00f3timo de batch no banco de dados.\"\"\"
    config_key = \"optimal_batch_size\"
    logger = log.bind(config_key=config_key, optimal_size=optimal_size)
    try:
        now_iso = datetime.now(timezone.utc).isoformat()
        config_value = {'value': optimal_size, 'updated_at': now_iso }
        repository.save_configuration(key=config_key, value=config_value)
        logger.info(\"Optimal batch size configuration updated in DB.\")
    except Exception as e:
        logger.error(\"Failed to update optimal batch size config in DB\", exc_info=True)
        
def get_optimal_batch_size(repository: PipedriveRepository, default_size: int = 1000) -> int:
    \"\"\"Busca o tamanho \u00f3timo de batch da configura\u00e7\u00e3o do banco de dados.\"\"\"
    config_key = \"optimal_batch_size\"
    logger = log.bind(config_key=config_key)
    try:
        config = repository.get_configuration(config_key)
        if config and isinstance(config.get(\"value\"), int) and config[\"value\"] > 0:
             size = int(config[\"value\"])
             logger.info(f\"Retrieved optimal batch size from config: {size}\")
             return size
        else:
             logger.warning(f\"Optimal batch size not found or invalid in config. Using default: {default_size}\", config_value=config)
             return default_size
    except Exception as e:
        logger.error(\"Failed to get optimal batch size from config. Using default.\", error=str(e), default_size=default_size, exc_info=True)
        return default_size
    
def backfill_cache_key_from_deal_ids(_, arguments: Dict[str, Any]) -> str:
    deal_ids = arguments.get(\"deal_ids\", [])
    return hash_objects(sorted(deal_ids))
"}
,
{"path": "flows/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "scripts/register_k8s_block.py", "encoding": "utf-8", "content": "import os
from prefect_kubernetes import KubernetesJob

K8S_IMAGE_NAME = \"pipedrive_metabase_integration-etl:latest\"
K8S_NAMESPACE = \"default\"
PUSHGATEWAY_SVC_ADDRESS = os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\")
PREFECT_API_URL_FOR_JOB = \"http://prefect-orion:4200/api\" 

# Defini\u00e7\u00e3o dos recursos e env vars 
DEFAULT_JOB_RESOURCES = {
    \"requests\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"},
    \"limits\": {\"memory\": \"4Gi\", \"cpu\": \"2\"}
}
DEFAULT_INIT_CONTAINERS = [
    {
        \"name\": \"wait-for-db\",
        \"image\": \"busybox:1.36\",
        \"command\": ['sh', '-c', 'echo Waiting for db...; while ! nc -z -w 1 db 5432; do sleep 2; done; echo DB ready.'],
    },
    {
        \"name\": \"wait-for-redis\",
        \"image\": \"busybox:1.36\",
        \"command\": ['sh', '-c', 'echo Waiting for redis...; while ! nc -z -w 1 redis 6379; do sleep 2; done; echo Redis ready.'],
    },
     {
        \"name\": \"wait-for-orion\",
        \"image\": \"curlimages/curl:latest\",
        \"command\": ['sh', '-c', 'echo Waiting for orion...; until curl -sf http://prefect-orion:4200/api/health > /dev/null; do echo -n \".\"; sleep 10; done; echo Orion ready.'],
    }
]
DEFAULT_ENV_FROM = [
    {\"secretRef\": {\"name\": \"app-secrets\"}},
    {\"secretRef\": {\"name\": \"db-secrets\"}},
]
DEFAULT_ENV = {
    \"PUSHGATEWAY_ADDRESS\": PUSHGATEWAY_SVC_ADDRESS,
    \"PREFECT_API_URL\": PREFECT_API_URL_FOR_JOB,
}

k8s_job_block = KubernetesJob(
    image=K8S_IMAGE_NAME,
    image_pull_policy='NEVER',
    namespace=K8S_NAMESPACE,
    env=DEFAULT_ENV, 
    job=KubernetesJob.job_template(
         metadata={\"labels\": {\"app.kubernetes.io/created-by\": \"prefect\"}},
         spec={
             \"template\": {
                 \"spec\": {
                     \"initContainers\": DEFAULT_INIT_CONTAINERS,
                     \"containers\": [{
                         \"name\": \"prefect-job\",
                         \"resources\": DEFAULT_JOB_RESOURCES,
                         \"envFrom\": DEFAULT_ENV_FROM,
                     }],
                 }
             }
         }
    ),
    stream_output=True
)

block_name = \"k8s-job-infra-block\"
print(f\"Salvando bloco de infraestrutura Kubernetes como '{block_name}'...\")
k8s_job_block.save(block_name, overwrite=True)
print(f\"Bloco '{block_name}' salvo com sucesso!\")"}
,
{"path": "scripts/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/schemas/deal_schema.py", "encoding": "utf-8", "content": "from typing import Optional, Dict, Any, Union
from pydantic import BaseModel, Field, field_validator, confloat
from datetime import datetime, timezone

class DealSchema(BaseModel, extra='allow'):
    id: int
    title: Optional[str] = None
    creator_user_id: Optional[int] = None 
    person_id: Optional[int] = None
    stage_id: Optional[int] = None
    stage_name: Optional[str] = None
    pipeline_id: Optional[int] = None
    pipeline_name: Optional[str] = None 
    status: Optional[str] = None
    value: Optional[float] = 0.0
    currency: Optional[str] = 'USD'
    add_time: Optional[datetime] = None
    update_time: Optional[datetime] = None
    custom_fields: Dict[str, Any] = Field({}, alias='custom_fields')

    class Config:
        populate_by_name  = True
        alias_generator = lambda x: x 

    @field_validator('creator_user_id', 'person_id', mode='before')
    def extract_id_from_dict(cls, value):
        if isinstance(value, dict):
            return value.get(\"id\")
        return value
    
    @field_validator('add_time', 'update_time')
    def parse_datetime_optional(cls, value):
        if value is None:
            return None
        if isinstance(value, datetime):
            return value.replace(tzinfo=timezone.utc) if value.tzinfo is None else value
        try:
            dt = datetime.fromisoformat(str(value).replace('Z', '+00:00'))
            return dt.astimezone(timezone.utc)
        except (ValueError, TypeError):
            print(f\"Warning: Could not parse date '{value}'. Setting to None.\")
            return None

    @field_validator('value')
    def value_to_float(cls, value):
        if value is None:
            return 0.0
        try:
            return float(value)
        except (ValueError, TypeError):
            print(f\"Warning: Could not convert value '{value}' to float. Setting to 0.0.\")
            return 0.0"}
,
{"path": "application/services/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/services/etl_service.py", "encoding": "utf-8", "content": "import psutil
import time
from datetime import datetime, timedelta, timezone
import tracemalloc
from typing import Any, Dict, List, Tuple, Optional
import pandas as pd
import numpy as np
import requests
import structlog
import json
from pydantic import ValidationError
from tenacity import RetryError

from application.utils.column_utils import flatten_custom_fields, normalize_column_name
from infrastructure.monitoring.metrics import (
    etl_counter, 
    etl_failure_counter, 
    etl_duration_hist,
    records_processed_counter, 
    memory_usage_gauge, 
    batch_size_gauge,
    db_operation_duration_hist,
    transform_duration_summary,
    etl_empty_batches_total,
    etl_batch_validation_errors_total,
    etl_final_column_mismatch_total,
    etl_heartbeat,
    etl_cpu_usage_percent,
    etl_thread_count,
    etl_disk_usage_bytes,
    etl_skipped_batches_total,
    etl_last_successful_run_timestamp,
    etl_transformation_error_rate,
    etl_loaded_records_per_batch,
    hist_load,
    hist_extract,
    hist_transform
)
from application.ports.pipedrive_client_port import PipedriveClientPort
from application.ports.data_repository_port import DataRepositoryPort
from application.schemas.deal_schema import DealSchema
from infrastructure.repository_impl.pipedrive_repository import UNKNOWN_NAME

log = structlog.get_logger()

STAGE_HISTORY_COLUMN_PREFIX = \"moved_to_stage_\"

class ETLService:
    def __init__(
        self,
        client: PipedriveClientPort,
        repository: DataRepositoryPort,
        batch_size: int = 1000 
    ):
        self.client = client
        self.repository = repository
        self.process_batch_size = batch_size 
        self.log = log.bind(service=\"ETLService\")
        self._stage_id_to_column_name_map = self.repository.get_stage_id_to_column_map()

        try:
             self._all_stages_details = self.client.fetch_all_stages_details()
             self._stage_id_to_normalized_name_map = self._build_stage_id_map(self._all_stages_details)
        except Exception as stage_err:
             self.log.error(\"Failed to fetch initial stage details during init.\", error=str(stage_err))
             self._all_stages_details = []
             self._stage_id_to_normalized_name_map = {}
        
    def run_etl_with_data(self, data: List[Dict], batch_size: int, flow_type: str) -> Dict:
        self.log.warning(\"Running ETL with provided data for test/experiment.\", data_size=len(data), batch_size=batch_size)
        original_fetch = self.client.fetch_all_deals_stream
        original_process_batch_size = self.process_batch_size

        def mock_stream(**kwargs):
            self.log.debug(\"Using mocked deal stream for test run.\")
            yield from data

        self.client.fetch_all_deals_stream = mock_stream
        self.process_batch_size = batch_size
        result = self.run_etl(flow_type=flow_type)

        self.client.fetch_all_deals_stream = original_fetch
        self.process_batch_size = original_process_batch_size
        self.log.info(\"Restored original deal stream and batch size after test run.\")
        return result

    def _build_stage_id_map(self, all_stages: List[Dict]) -> Dict[int, str]:
        \"\"\"Cria um mapa de stage_id para nome normalizado.\"\"\"
        id_map = {}
        if not all_stages:
            return {}
        for stage in all_stages:
            stage_id = stage.get('id')
            stage_name = stage.get('name')
            if stage_id and stage_name:
                try:
                    normalized = normalize_column_name(stage_name)
                    if normalized:
                        id_map[stage_id] = normalized
                except Exception as e:
                    self.log.warning(\"Failed to normalize stage name for map\",
                                     stage_id=stage_id, name=stage_name, error=str(e))
        return id_map

    def _parse_changelog_timestamp(self, timestamp_str: Optional[str]) -> Optional[datetime]:
        \"\"\"Converte o timestamp do changelog (ex: '2020-09-25 09:21:55') para datetime com UTC.\"\"\"
        if not timestamp_str:
            return None
        try:
            dt = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            return dt.replace(tzinfo=timezone.utc)
        except (ValueError, TypeError):
            self.log.warning(\"Failed to parse changelog timestamp\", timestamp_str=timestamp_str)
            return None

    def _validate_and_transform_batch_pandas(
        self,
        batch: List[Dict],
        flow_type: str
    ) -> Tuple[List[Dict], int]:
        \"\"\"Valida, transforma e enriquece um batch de deals usando lookups no DB.\"\"\"
        start_time = time.monotonic()
        pydantic_failed_count = 0
        original_count = len(batch)
        transform_log = self.log.bind(batch_original_size=original_count, flow_type=flow_type)

        if not batch:
            return [], 0

        # 1. Valida\u00e7\u00e3o com Pydantic
        valid_input_for_df = []
        for i, record in enumerate(batch):
            record_id = record.get(\"id\", f\"no-id-index-{i}\")
            try:
                DealSchema.model_validate(record)
                valid_input_for_df.append(record)
            except ValidationError as e:
                pydantic_failed_count += 1
                errors_summary = [f\"{err['loc']}: {err['msg']}\" for err in e.errors()]
                transform_log.warning(\"Pydantic validation failed\", record_id=record_id, errors=errors_summary)
            except Exception as e:
                pydantic_failed_count += 1
                transform_log.error(\"Unexpected error during Pydantic validation\", record_id=record_id, exc_info=True)

        pydantic_valid_count = len(valid_input_for_df)
        transform_log = transform_log.bind(pydantic_valid_count=pydantic_valid_count, pydantic_failed_count=pydantic_failed_count)

        if not valid_input_for_df:
            transform_log.warning(\"No records passed Pydantic validation in the batch.\")
            return [], pydantic_failed_count

        # 2. DataFrame e transforma\u00e7\u00e3o
        validated_records: List[Dict] = []
        transform_failed_count = 0
        db_lookup_maps = {}
        
        try:
            df = pd.DataFrame(valid_input_for_df)
            
            # --- Coletar IDs para Lookup no DB ---
            user_ids_needed = set(df['creator_user_id'].dropna().unique()) | set(df['owner_id'].dropna().unique())
            df[\"owner_id_parsed\"] = df[\"owner_id\"].apply(lambda x: x.get(\"id\") if isinstance(x, dict) else x).astype('Int64')
            owner_ids_needed = set(df['owner_id_parsed'].dropna().unique())
            user_ids_needed.update(owner_ids_needed) 

            person_ids_needed = set(df['person_id'].dropna().unique())
            stage_ids_needed = set(df['stage_id'].dropna().unique())
            pipeline_ids_needed = set(df['pipeline_id'].dropna().unique())
            df[\"org_id_parsed\"] = df[\"org_id\"].apply(lambda x: x.get(\"id\") if isinstance(x, dict) else x).astype('Int64')
            org_ids_needed = set(df['org_id_parsed'].dropna().unique())


            transform_log.debug(\"IDs collected for DB lookup\",
                                users=len(user_ids_needed), persons=len(person_ids_needed),
                                stages=len(stage_ids_needed), pipelines=len(pipeline_ids_needed),
                                orgs=len(org_ids_needed))

            # --- Buscar Mapas do DB para o Batch ---
            lookup_start_time = time.monotonic()
            try:
                db_lookup_maps = self.repository.get_lookup_maps_for_batch(
                    user_ids={int(uid) for uid in user_ids_needed if pd.notna(uid)}, 
                    person_ids={int(pid) for pid in person_ids_needed if pd.notna(pid)},
                    stage_ids={int(sid) for sid in stage_ids_needed if pd.notna(sid)},
                    pipeline_ids={int(plid) for plid in pipeline_ids_needed if pd.notna(plid)},
                    org_ids={int(oid) for oid in org_ids_needed if pd.notna(oid)}
                )
                lookup_duration = time.monotonic() - lookup_start_time
                transform_log.info(\"Fetched batch lookups from DB\", duration_sec=f\"{lookup_duration:.3f}s\",
                                   users=len(db_lookup_maps.get('users', {})), persons=len(db_lookup_maps.get('persons', {})),
                                   stages=len(db_lookup_maps.get('stages', {})), pipelines=len(db_lookup_maps.get('pipelines', {})),
                                   orgs=len(db_lookup_maps.get('orgs', {})))
            except Exception as lookup_err:
                 transform_log.error(\"Failed to fetch lookups from DB for batch\", error=str(lookup_err), exc_info=True)
                 db_lookup_maps = {'users': {}, 'persons': {}, 'stages': {}, 'pipelines': {}, 'orgs': {}}


            # --- Transforma\u00e7\u00f5es usando os mapas buscados do DB ---
            transformed_df = pd.DataFrame()
            transformed_df[\"id\"] = df[\"id\"].astype(str)
            transformed_df[\"titulo\"] = df[\"title\"].fillna(\"\").astype(str)
            status_map = { \"won\": \"Ganho\", \"lost\": \"Perdido\", \"open\": \"Em aberto\", \"deleted\": \"Deletado\" }
            transformed_df[\"status\"] = df[\"status\"].fillna(\"\").astype(str).map(status_map).fillna(df[\"status\"])
            transformed_df[\"currency\"] = df[\"currency\"].fillna(\"USD\").astype(str)
            transformed_df[\"value\"] = pd.to_numeric(df[\"value\"], errors='coerce').fillna(0.0)
            transformed_df[\"add_time\"] = pd.to_datetime(df[\"add_time\"], errors='coerce', utc=True)
            transformed_df[\"update_time\"] = pd.to_datetime(df[\"update_time\"], errors='coerce', utc=True)

            # Mapeamentos usando db_lookup_maps
            batch_user_map = db_lookup_maps.get('users', {})
            batch_person_map = db_lookup_maps.get('persons', {})
            batch_stage_map = db_lookup_maps.get('stages', {}) 
            batch_pipeline_map = db_lookup_maps.get('pipelines', {})
            batch_org_map = db_lookup_maps.get('orgs', {})

            transformed_df[\"creator_user_id\"] = pd.to_numeric(df[\"creator_user_id\"], errors='coerce').astype('Int64')
            transformed_df['creator_user_name'] = transformed_df['creator_user_id'].map(batch_user_map).fillna(UNKNOWN_NAME)

            transformed_df[\"stage_id\"] = pd.to_numeric(df[\"stage_id\"], errors='coerce').astype('Int64')
            transformed_df['stage_name'] = transformed_df['stage_id'].map(batch_stage_map).fillna(UNKNOWN_NAME)

            transformed_df[\"pipeline_id\"] = pd.to_numeric(df[\"pipeline_id\"], errors='coerce').astype('Int64')
            transformed_df['pipeline_name'] = transformed_df['pipeline_id'].map(batch_pipeline_map).fillna(UNKNOWN_NAME)

            transformed_df[\"person_id\"] = pd.to_numeric(df[\"person_id\"], errors='coerce').astype('Int64')
            transformed_df['person_name'] = transformed_df['person_id'].map(batch_person_map).fillna(UNKNOWN_NAME)

            # Usar owner_id_parsed que j\u00e1 foi calculado
            transformed_df[\"owner_id\"] = df[\"owner_id_parsed\"] 
            transformed_df[\"owner_name\"] = transformed_df[\"owner_id\"].map(batch_user_map).fillna(UNKNOWN_NAME)

            # Usar org_id_parsed que j\u00e1 foi calculado
            transformed_df[\"org_id\"] = df[\"org_id_parsed\"] 
            transformed_df['org_name'] = transformed_df['org_id'].map(batch_org_map).fillna(UNKNOWN_NAME)

            # --- Outros campos base ---
            transformed_df[\"lost_reason\"] = df[\"lost_reason\"].fillna(\"\").astype(str)
            transformed_df[\"visible_to\"] = df[\"visible_to\"].fillna(\"\").astype(str)
            transformed_df[\"close_time\"] = pd.to_datetime(df[\"close_time\"], errors='coerce', utc=True)
            transformed_df[\"won_time\"] = pd.to_datetime(df[\"won_time\"], errors='coerce', utc=True)
            transformed_df[\"lost_time\"] = pd.to_datetime(df[\"lost_time\"], errors='coerce', utc=True)
            transformed_df[\"first_won_time\"] = pd.to_datetime(
                df.get(\"first_won_time\", pd.NaT), errors='coerce', utc=True
            )
            transformed_df[\"expected_close_date\"] = pd.to_datetime(df[\"expected_close_date\"], errors='coerce').dt.date
            transformed_df[\"probability\"] = pd.to_numeric(df[\"probability\"], errors='coerce')

            # --- Campos Customizados ---
            repo_custom_mapping = self.repository.custom_field_mapping
            if repo_custom_mapping and 'custom_fields' in df.columns:
                transform_log.warning(\"Flatten Custom Fields\", custom_fields_after_flatten=repo_custom_mapping)
                df['custom_fields_parsed'] = df['custom_fields'].apply(
                    lambda x: json.loads(x) if isinstance(x, str) else (x if isinstance(x, dict) else {})
                )
                transform_log.warning(\"Flatten Custom Fields\", custom_fields_before_flatten=repo_custom_mapping)

                custom_fields_flattened_df = pd.json_normalize(
                    df['custom_fields_parsed'].apply(
                        lambda x: flatten_custom_fields(x, repo_custom_mapping)
                    ).tolist()
                )

                # Garantir alinhamento
                custom_fields_flattened_df.index = df.index

                # Concat com df principal
                transformed_df = pd.concat([transformed_df, custom_fields_flattened_df], axis=1)
            
            # --- Selecionar e Ordenar Colunas Finais ---
            final_columns = self.repository._get_all_columns()
            existing_cols_in_df = list(transformed_df.columns)
            ordered_final_columns = [col for col in final_columns if col in existing_cols_in_df]

            # Adicionar colunas que faltam no DataFrame com None
            missing_final_cols = [col for col in final_columns if col not in existing_cols_in_df]
            if missing_final_cols:
                transform_log.debug(\"Columns defined in repository are missing in transformed DataFrame, adding as None.\", missing_columns=missing_final_cols)
                missing_df = pd.DataFrame({col: [None] * len(transformed_df) for col in missing_final_cols})
                transformed_df = pd.concat([transformed_df, missing_df], axis=1)
                transformed_df = transformed_df.copy()
                ordered_final_columns.extend(missing_final_cols)

            # Verificar colunas extras
            extra_cols = [col for col in existing_cols_in_df if col not in ordered_final_columns and col not in missing_final_cols]
            if extra_cols:
                transform_log.warning(\"Columns created during transform but not in final repository schema (will be dropped)\", extra_columns=extra_cols)
            try:
                if extra_cols:
                    transform_log.info(\"Attempting to update schema dynamically for extra columns detected.\", columns=extra_cols)
                    self.repository.add_columns_to_main_table(extra_cols, inferred_from_df=transformed_df)
                    etl_final_column_mismatch_total.labels(flow_type=flow_type).inc()
            except Exception as schema_err:
                transform_log.error(\"Failed to update schema with new columns\", error=str(schema_err), exc_info=True)        

            # Selecionar apenas as colunas finais na ordem definida
            # Remover colunas extras que n\u00e3o est\u00e3o no schema atual
            transformed_df = transformed_df[[col for col in ordered_final_columns if col in transformed_df.columns]]

            # Verifica se h\u00e1 colunas extras ap\u00f3s concatena\u00e7\u00e3o com campos customizados
            current_cols = set(transformed_df.columns)
            defined_cols = set(final_columns)
            extra_cols = current_cols - defined_cols

            if extra_cols:
                transform_log.warning(\"Detected extra columns not present in schema. Will add dynamically.\", extra_columns=list(extra_cols))
                try:
                    self.repository.add_columns_to_main_table(list(extra_cols), inferred_from_df=transformed_df)
                    # Recarregar o schema ap\u00f3s adicionar
                    final_columns = self.repository._get_all_columns()
                    ordered_final_columns = [col for col in final_columns if col in transformed_df.columns]
                except Exception as schema_err:
                    transform_log.error(\"Failed to dynamically update schema with extra columns\", error=str(schema_err), exc_info=True)

            # --- Limpeza Final ---
            transformed_df = transformed_df.replace({pd.NA: None, np.nan: None, pd.NaT: None})
            validated_records = transformed_df.to_dict('records')
            transform_succeeded_count = len(validated_records)
            transform_failed_count = pydantic_valid_count - transform_succeeded_count 
            
            

        except AttributeError as ae: 
            transform_failed_count = pydantic_valid_count
            transform_log.error(\"Pandas transformation failed due to AttributeError\", error=str(ae), exc_info=True)
            raise ae 
        except KeyError as ke:
            transform_failed_count = pydantic_valid_count
            transform_log.error(\"Pandas transformation failed due to KeyError\", error=str(ke), exc_info=True)
            raise ke
        except Exception as e:
            transform_failed_count = pydantic_valid_count
            transform_log.error(\"Pandas transformation/enrichment failed\", exc_info=True)
            raise e 

        duration = time.monotonic() - start_time
        transform_duration_summary.labels(flow_type=flow_type).observe(duration)
        total_failed_in_batch = pydantic_failed_count + transform_failed_count
        if original_count > 0:
            error_rate = total_failed_in_batch / original_count
            etl_transformation_error_rate.labels(flow_type=flow_type).set(error_rate)
        etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"pydantic\").inc(pydantic_failed_count)
        etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"transform\").inc(transform_failed_count)
        transform_log.info(
            \"Batch transformation completed\",
            validated_count=len(validated_records),
            transform_errors=transform_failed_count,
            total_failed_in_batch=total_failed_in_batch,
            duration_sec=f\"{duration:.3f}s\"
        )
        return validated_records, total_failed_in_batch

    def _track_resources(self, flow_type: str) -> float:
        \"\"\"Monitora o uso de mem\u00f3ria e retorna a mem\u00f3ria atual em bytes.\"\"\"
        current_mem = 0.0
        if tracemalloc.is_tracing():
            try:
                current, peak = tracemalloc.get_traced_memory()
                memory_usage_gauge.labels(flow_type=flow_type).set(peak / 1e6)  # em MB
                self.log.debug(f\"Memory Usage: Current={current/1e6:.2f}MB, Peak={peak/1e6:.2f}MB\")
                current_mem = current
            except Exception as mem_err:
                self.log.warning(\"Failed to track memory usage\", error=str(mem_err))
        else:
            self.log.debug(\"Tracemalloc is not running, skipping memory tracking.\")
        return current_mem

    def run_etl(self, flow_type: str) -> Dict[str, object]:
        \"\"\"Executa o processo ETL completo, agora sem buscar mapas na inicializa\u00e7\u00e3o.\"\"\"
        run_start_time = time.monotonic()
        run_start_utc = datetime.now(timezone.utc)
        if not tracemalloc.is_tracing():
            tracemalloc.start()
        etl_counter.labels(flow_type=flow_type).inc()

        result = {
            \"status\": \"error\", \"total_fetched\": 0, \"total_validated\": 0, \"total_loaded\": 0,
            \"total_failed\": 0, \"start_time\": run_start_utc.isoformat(), \"end_time\": None,
            \"duration_seconds\": 0, \"message\": \"ETL process did not complete.\", \"peak_memory_mb\": 0
        }
        latest_update_time_in_run: Optional[datetime] = None
        total_fetched = 0; total_validated = 0; total_loaded = 0; total_failed = 0
        run_log = self.log.bind(run_start_time=run_start_utc.isoformat(), flow_type=flow_type)

        try:
            run_log.info(f\"Starting ETL run ({flow_type})\")

            # --- Extra\u00e7\u00e3o (Streaming) ---
            last_timestamp_str = self.client.get_last_timestamp()
            run_log.info(\"Fetching deals stream from Pipedrive\", updated_since=last_timestamp_str)
            with hist_extract.labels(flow_type=flow_type).time():
                deal_stream_iterator = self.client.fetch_all_deals_stream(updated_since=last_timestamp_str)

            # --- Transforma\u00e7\u00e3o e Carga (Batching) ---
            batch_num = 0
            records_for_processing_batch: List[Dict] = []
            run_log.info(\"Starting ETL batch processing loop\", batch_size=self.process_batch_size)

            for deal_record in deal_stream_iterator:
                total_fetched += 1
                records_for_processing_batch.append(deal_record)
                update_time_str = deal_record.get(\"update_time\")
                
                if update_time_str:
                    try:
                        current_record_time = pd.to_datetime(update_time_str, errors='coerce', utc=True)
                        if pd.notna(current_record_time):
                            if latest_update_time_in_run is None or current_record_time > latest_update_time_in_run:
                                latest_update_time_in_run = current_record_time
                        else:
                             run_log.warning(\"Could not parse update_time (Pandas)\", record_id=deal_record.get(\"id\"), time_str=update_time_str)
                    except Exception:
                        run_log.warning(\"Could not parse update_time from fetched record\", record_id=deal_record.get(\"id\"), time_str=update_time_str)


                if len(records_for_processing_batch) >= self.process_batch_size:
                    batch_num += 1
                    batch_to_process = records_for_processing_batch
                    records_for_processing_batch = []
                    batch_log = run_log.bind(batch_num=batch_num, batch_size=len(batch_to_process))
                    batch_log.info(\"Processing ETL batch\")
                    batch_start_time = time.monotonic()
                    batch_size_gauge.labels(flow_type=flow_type).set(len(batch_to_process))

                    try:
                        with hist_transform.labels(flow_type=flow_type).time():
                            validated_batch, failed_count_in_batch = self._validate_and_transform_batch_pandas(
                                batch=batch_to_process,
                                flow_type=flow_type
                            )
                        total_failed += failed_count_in_batch
                        total_validated += len(validated_batch)
                        
                        if not validated_batch:
                            etl_empty_batches_total.labels(flow_type=flow_type).inc()
                        else:
                            etl_skipped_batches_total.labels(flow_type=flow_type).inc()
                        if validated_batch:
                            load_start = time.monotonic()
                            try:
                                with db_operation_duration_hist.labels(operation='upsert').time():
                                    batch_log.warning(\"Sending batch to repository upsert.\", first_ids=[rec.get(\"id\") for rec in validated_batch[:5]])
                                    with hist_load.labels(flow_type=flow_type).time():
                                        self.repository.save_data_upsert(validated_batch)
                                current_loaded_count = len(validated_batch)
                                total_loaded += current_loaded_count
                                records_processed_counter.labels(flow_type=flow_type).inc(current_loaded_count)
                                etl_loaded_records_per_batch.labels(flow_type=flow_type).observe(current_loaded_count)
                                load_duration = time.monotonic() - load_start
                                batch_log.info(\"ETL Batch loaded/upserted successfully\", loaded_count=current_loaded_count,
                                               load_duration_sec=f\"{load_duration:.3f}s\")
                            except Exception as load_error:
                                etl_failure_counter.labels(flow_type=flow_type).inc(len(validated_batch))
                                failed_on_load = len(validated_batch)
                                total_failed += failed_on_load
                                total_validated -= failed_on_load
                                batch_log.error(\"Failed to load batch to repository\", error=str(load_error),
                                                records_count=failed_on_load, exc_info=True)
                        else:
                            batch_log.warning(\"ETL Batch resulted in no validated records to load\", failed_in_transform=failed_count_in_batch)

                    except Exception as batch_proc_err: 
                        batch_log.error(\"Critical error processing ETL batch, skipping.\", error=str(batch_proc_err), exc_info=True)
                        failed_in_this_batch = len(batch_to_process)
                        etl_failure_counter.labels(flow_type=flow_type).inc(failed_in_this_batch)
                        total_failed += failed_in_this_batch

                    batch_duration = time.monotonic() - batch_start_time
                    batch_log.debug(\"ETL Batch processing complete\", duration_sec=f\"{batch_duration:.3f}s\")

            if records_for_processing_batch:
                batch_num += 1
                batch_to_process = records_for_processing_batch
                batch_log = run_log.bind(batch_num=batch_num, batch_size=len(batch_to_process))
                batch_log.info(\"Processing final ETL batch\")
                batch_start_time = time.monotonic()
                batch_size_gauge.labels(flow_type=flow_type).set(len(batch_to_process))

                try:
                    with hist_transform.labels(flow_type=flow_type).time():
                        validated_batch, failed_count_in_batch = self._validate_and_transform_batch_pandas(
                            batch=batch_to_process,
                            flow_type=flow_type
                        )
                    total_failed += failed_count_in_batch
                    total_validated += len(validated_batch)

                    if validated_batch:
                        load_start = time.monotonic()
                        try:
                            with db_operation_duration_hist.labels(operation='upsert').time():
                                batch_log.warning(\"Sending batch to repository upsert.\", first_ids=[rec.get(\"id\") for rec in validated_batch[:5]])
                                with hist_load.labels(flow_type=flow_type).time():
                                    self.repository.save_data_upsert(validated_batch)
                            current_loaded_count = len(validated_batch)
                            total_loaded += current_loaded_count
                            records_processed_counter.labels(flow_type=flow_type).inc(current_loaded_count)
                            etl_loaded_records_per_batch.labels(flow_type=flow_type).observe(current_loaded_count)
                            load_duration = time.monotonic() - load_start
                            batch_log.info(\"Final ETL Batch loaded/upserted successfully\", loaded_count=current_loaded_count,
                                           load_duration_sec=f\"{load_duration:.3f}s\")
                        except Exception as load_error:
                            etl_failure_counter.labels(flow_type=flow_type).inc(len(validated_batch))
                            failed_on_load = len(validated_batch)
                            total_failed += failed_on_load
                            total_validated -= failed_on_load
                            batch_log.error(\"Failed to load final batch to repository\", error=str(load_error),
                                            records_count=failed_on_load, exc_info=True)
                    else:
                        batch_log.warning(\"Final ETL Batch resulted in no validated records to load\", failed_in_transform=failed_count_in_batch)

                except Exception as batch_proc_err:
                    batch_log.error(\"Critical error processing final ETL batch, skipping.\", error=str(batch_proc_err), exc_info=True)
                    failed_in_this_batch = len(batch_to_process)
                    etl_failure_counter.labels(flow_type=flow_type).inc(failed_in_this_batch)
                    total_failed += failed_in_this_batch

                batch_duration = time.monotonic() - batch_start_time
                batch_log.debug(\"Final ETL Batch processing complete\", duration_sec=f\"{batch_duration:.3f}s\")


            # --- Finaliza\u00e7\u00e3o ---
            run_log.info(\"ETL stream processing finished.\")
            if latest_update_time_in_run and total_loaded > 0: 
                try:
                    buffered_time = latest_update_time_in_run + timedelta(seconds=1)
                    iso_timestamp = buffered_time.strftime('%Y-%m-%dT%H:%M:%SZ')
                    self.client.update_last_timestamp(iso_timestamp)
                    run_log.info(\"Last timestamp updated in cache\", timestamp=iso_timestamp)
                except Exception as cache_err:
                    run_log.error(\"Failed to update last timestamp in cache\", error=str(cache_err), exc_info=True)
            elif total_fetched == 0:
                run_log.info(\"No new records fetched since last run. Last timestamp not updated.\")
            elif total_loaded == 0 and total_fetched > 0:
                 run_log.warning(\"ETL fetched records but loaded none. Last timestamp NOT updated.\",
                                fetched=total_fetched, loaded=total_loaded, failed=total_failed)
            else: 
                run_log.warning(\"ETL finished but could not determine or save the last timestamp reliably. Timestamp NOT updated.\",
                                fetched=total_fetched, loaded=total_loaded, failed=total_failed, last_time=latest_update_time_in_run)

            result.update({
                \"status\": \"success\", \"total_fetched\": total_fetched, \"total_validated\": total_validated,
                \"total_loaded\": total_loaded, \"total_failed\": total_failed,
                \"message\": f\"ETL completed. Fetched={total_fetched}, Validated={total_validated}, Loaded={total_loaded}, Failed={total_failed}.\"
            })
            
            etl_last_successful_run_timestamp.labels(flow_type=flow_type).set(int(run_end_utc.timestamp()))
        except Exception as e:
            if result.get(\"status\") != \"error\": 
                 etl_failure_counter.labels(flow_type=flow_type).inc()
            run_log.critical(\"Critical ETL failure during run_etl\", exc_info=True)
            result[\"status\"] = \"error\"
            if result[\"message\"] == \"ETL process did not complete.\":
                result[\"message\"] = f\"Critical ETL failure: {str(e)}\"
            result[\"total_failed\"] = max(total_failed, total_fetched - total_loaded) 


        finally:
            run_end_time = time.monotonic()
            run_end_utc = datetime.now(timezone.utc)
            duration = run_end_time - run_start_time
            result[\"duration_seconds\"] = round(duration, 3)
            result[\"end_time\"] = run_end_utc.isoformat()
            etl_duration_hist.labels(flow_type=flow_type).observe(duration)
            peak_mem_mb = 0
            etl_heartbeat.labels(flow_type=flow_type).set_to_current_time()
            etl_cpu_usage_percent.labels(flow_type=flow_type).set(psutil.cpu_percent())
            etl_thread_count.labels(flow_type=flow_type).set(len(psutil.Process().threads()))
            etl_disk_usage_bytes.labels(mount_point='/').set(psutil.disk_usage('/').used)
            if tracemalloc.is_tracing():
                try:
                    current_mem, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop()
                    run_log.debug(f\"Final Memory Usage: Current={current_mem/1e6:.2f}MB, Peak={peak_mem_mb:.2f}MB\")
                except Exception as trace_err:
                    run_log.error(\"Error stopping tracemalloc\", error=str(trace_err))
                    if tracemalloc.is_tracing(): tracemalloc.stop()

            result[\"peak_memory_mb\"] = peak_mem_mb

            # Reafirmar contagens finais
            result[\"total_fetched\"] = total_fetched
            result[\"total_validated\"] = total_validated 
            result[\"total_loaded\"] = total_loaded     
            result[\"total_failed\"] = total_failed     

            log_level = run_log.info if result[\"status\"] == \"success\" else run_log.error
            log_level(\"ETL run summary\", **result)
            try:
                print(f\"ETL_COMPLETION_METRICS: {json.dumps(result)}\")
            except TypeError: 
                print(f\"ETL_COMPLETION_METRICS: {str(result)}\")

            self.log.debug(\"Cleared internal maps after ETL run.\")
            return result

    def run_retroactive_backfill(self, deal_ids: List[str]) -> Dict[str, Any]:
        \"\"\"
        Executa o backfill do hist\u00f3rico de stages para uma lista de deal IDs (Fluxo 2).
        \"\"\"
        if not deal_ids:
            self.log.info(\"No deal IDs provided for retroactive backfill.\")
            return {\"status\": \"skipped\", \"processed_deals\": 0, \"updates_generated\": 0}

        run_start_time = time.monotonic()
        run_log = self.log.bind(flow_type=\"backfill\", batch_deal_count=len(deal_ids))
        run_log.info(\"Starting retroactive stage history backfill run.\")

        processed_deals_count = 0
        updates_to_apply: List[Dict[str, Any]] = []
        api_errors = 0
        processing_errors = 0

        stage_column_map = self._stage_id_to_column_name_map
        if not stage_column_map:
            run_log.error(\"Stage ID to normalized name map is empty. Cannot perform backfill.\")
            return {\"status\": \"error\", \"message\": \"Stage ID map is empty.\"}

        for deal_id_str in deal_ids:
            try:
                deal_id = int(deal_id_str)
                deal_log = run_log.bind(deal_id=deal_id)
                deal_log.debug(\"Fetching changelog for deal.\")

                changelog = self.client.fetch_deal_changelog(deal_id)
                processed_deals_count += 1

                if not changelog:
                    deal_log.debug(\"No changelog entries found for deal.\")
                    continue

                stage_changes = []
                for entry in changelog:
                    if entry.get('field_key') == 'stage_id':
                        ts = self._parse_changelog_timestamp(entry.get('time'))
                        new_stage_id = entry.get('new_value')
                        if ts and new_stage_id is not None:
                            try:
                                new_stage_id_int = int(new_stage_id)
                                stage_changes.append({'timestamp': ts, 'stage_id': new_stage_id_int})
                            except (ValueError, TypeError):
                                deal_log.warning(\"Could not parse new_value as int for stage_id change\", entry=entry)

                if not stage_changes:
                    deal_log.debug(\"No 'stage_id' changes found in changelog.\")
                    continue

                stage_changes.sort(key=lambda x: x['timestamp'])
                first_entry_times: Dict[int, datetime] = {}
                for change in stage_changes:
                    stage_id = change['stage_id']
                    timestamp = change['timestamp']
                    if stage_id not in first_entry_times:
                        first_entry_times[stage_id] = timestamp

                for stage_id, first_timestamp in first_entry_times.items():
                    normalized_name = stage_column_map.get(stage_id)
                    if normalized_name:
                        column_name = normalized_name  
                        updates_to_apply.append({
                            'deal_id': deal_id_str,
                            'stage_column': column_name,
                            'timestamp': first_timestamp.strftime('%Y-%m-%d')
                        })
                    else:
                        deal_log.warning(\"Stage ID from changelog not found in current stage map\", stage_id=stage_id)
            except RetryError as retry_err:
                api_errors += 1
                run_log.error(\"API RetryError fetching changelog\", deal_id=deal_id_str, error=str(retry_err.last_attempt.exception()))
            except requests.exceptions.RequestException as req_err:
                api_errors += 1
                run_log.error(\"API RequestException fetching changelog\", deal_id=deal_id_str, error=str(req_err))
            except ValueError:
                processing_errors += 1
                run_log.error(\"Invalid deal_id format, skipping\", deal_id_str=deal_id_str)
            except Exception as e:
                processing_errors += 1
                run_log.error(\"Error processing changelog for deal\", deal_id=deal_id_str, exc_info=True)

        if updates_to_apply:
            run_log.info(
                    \"Preparando para chamar update_stage_history\",
                    total_updates=len(updates_to_apply)
                )
            try:
                self.repository.update_stage_history(updates_to_apply)
                run_log.info(
                    \"update_stage_history foi chamado com sucesso\",
                    affected_deals=processed_deals_count
                )
            except Exception as db_err:
                run_log.error(
                    \"Falha durante a atualiza\u00e7\u00e3o do hist\u00f3rico de stages\",
                    error=str(db_err),
                    exc_info=True
                )
                return {
                    \"status\": \"error\",
                    \"message\": f\"Database update failed: {db_err}\",
                    \"processed_deals\": processed_deals_count,
                    \"updates_generated\": len(updates_to_apply),
                    \"api_errors\": api_errors,
                    \"processing_errors\": processing_errors
                }
        else:
            run_log.info(
                \"Nenhuma mudan\u00e7a de stage para aplicar no backfill\",
                processed_deals=processed_deals_count
            )

        duration = time.monotonic() - run_start_time
        status = \"success\" if api_errors == 0 and processing_errors == 0 else \"partial_success\"
        run_log.info(\"Retroactive backfill run finished.\", status=status, duration_sec=f\"{duration:.3f}s\")
        return {
            \"status\": status,
            \"processed_deals\": processed_deals_count,
            \"updates_generated\": len(updates_to_apply),
            \"api_errors\": api_errors,
            \"processing_errors\": processing_errors,
            \"duration_seconds\": round(duration, 3)
        }"}
,
{"path": "application/ports/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/ports/data_repository_port.py", "encoding": "utf-8", "content": "from abc import ABC, abstractmethod
from typing import List, Dict

class DataRepositoryPort(ABC):
    @abstractmethod
    def save_data(self, data: List[Dict]) -> None:
        \"\"\"
        Implements the logic to save transformed data to the repository.
        (Potentially delegates to save_data_upsert or another method).
        \"\"\"
        pass

    @abstractmethod
    def save_data_upsert(self, data: List[Dict]) -> None:
        \"\"\"
        Implements the logic to save transformed data using an upsert strategy.
        This should merge new records with existing ones based on a primary key.
        \"\"\"
        pass

    @abstractmethod
    def filter_data_by_ids(self, data: List[Dict], id_key: str = \"id\") -> List[Dict]:
        \"\"\"
        Filters a list of dictionaries, returning only those whose IDs
        do *not* exist in the target table.
        \"\"\"
        pass

    @property
    @abstractmethod
    def custom_field_mapping(self) -> Dict[str, str]:
        \"\"\"Returns the custom field mapping used by the repository.\"\"\"
        pass

    @abstractmethod
    def ensure_schema_exists(self) -> None:
        \"\"\"Ensures the target table and necessary indexes exist.\"\"\"
        pass"}
,
{"path": "application/ports/pipedrive_client_port.py", "encoding": "utf-8", "content": "from abc import ABC, abstractmethod
from typing import Dict, List, Generator

class PipedriveClientPort(ABC):
    @abstractmethod
    def fetch_deal_fields_mapping(self) -> Dict[str, str]:
        \"\"\"Fetches the mapping of custom field API keys to normalized names.\"\"\"
        pass

    @abstractmethod
    def fetch_all_deals_stream(self, updated_since: str = None, items_limit: int = None) -> Generator[List[Dict], None, None]:
        \"\"\"
        Fetches deals from Pipedrive in batches as a generator.
        Yields lists (batches) of deal dictionaries.
        \"\"\"
        pass

    @abstractmethod
    def update_last_timestamp(self, new_timestamp: str) -> None:
        \"\"\"Updates the timestamp for the next incremental fetch.\"\"\"
        pass

    @abstractmethod
    def get_last_timestamp(self) -> str | None:
        \"\"\"Gets the last stored timestamp.\"\"\"
        pass"}
,
{"path": "application/utils/replace_nan_with_none_recursive.py", "encoding": "utf-8", "content": "import pandas as pd
import numpy as np

def replace_nan_with_none_recursive(obj):
    if isinstance(obj, dict):
        return {k: replace_nan_with_none_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_nan_with_none_recursive(elem) for elem in obj]
    elif isinstance(obj, float) and np.isnan(obj):
        return None
    elif pd.isna(obj): 
         return None
    else:
        return obj
"}
,
{"path": "application/utils/column_utils.py", "encoding": "utf-8", "content": "from typing import Any, Dict
import unicodedata
import re
import logging

log = logging.getLogger(__name__)

def normalize_column_name(name: str) -> str:
    if not isinstance(name, str):
        return \"\"
    name = name.lower()
    try:
        name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')
    except Exception:
        pass
    name = re.sub(r'[^\w_]+', '_', name)
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    if name and name[0].isdigit():
        name = '_' + name
    return name or \"_invalid_normalized_name\"

ADDRESS_COMPONENT_SUFFIX_MAP = {
    'street_number': 'numero_da_casa',
    'route': 'nome_da_rua',
    'sublocality': 'distrito_sub_localidade',
    'locality': 'cidade_municipio_vila_localidade',
    'admin_area_level_1': 'estado',
    'admin_area_level_2': 'regiao',
    'country': 'pais',
    'postal_code': 'cep_codigo_postal',
    'latitude': 'latitude',
    'longitude': 'longitude',
    # 'formatted_address' mapeado para a coluna principal
}

# Adicione um conjunto de chaves que indicam um campo de endere\u00e7o
ADDRESS_INDICATOR_KEYS = {'formatted_address', 'locality', 'country', 'postal_code'}

def flatten_custom_fields(custom_fields: Dict[str, Any], repo_custom_mapping: Dict[str, str]) -> Dict[str, Any]:
    \"\"\"
    Achata os campos personalizados, tratando campos de endere\u00e7o de forma especial
    para extrair seus componentes em colunas separadas.
    \"\"\"
    flat_dict = {}

    for api_key, normalized_name in repo_custom_mapping.items():
        field_data = custom_fields.get(api_key)

        if normalized_name not in flat_dict:
            flat_dict[normalized_name] = None

        if isinstance(field_data, dict):
            is_likely_address = any(key in field_data for key in ADDRESS_INDICATOR_KEYS)

            if is_likely_address:
                log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as address.\")

                # 1. Pega o valor principal 
                main_address_value = field_data.get('formatted_address') or field_data.get('value')
                flat_dict[normalized_name] = main_address_value

                # 2. Extrai os componentes
                for component_key, suffix in ADDRESS_COMPONENT_SUFFIX_MAP.items():
                    subcol_name = f\"{normalized_name}_{suffix}\"
                    component_value = field_data.get(component_key)
                    flat_dict[subcol_name] = component_value

                expected_address_cols = {f\"{normalized_name}_{suffix}\" for suffix in ADDRESS_COMPONENT_SUFFIX_MAP.values()}
                for key in flat_dict.keys():
                     if key.startswith(normalized_name + \"_\") and key not in expected_address_cols:
                          if key not in flat_dict:
                               flat_dict[key] = None


            else:
                log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as generic dictionary.\")
                flat_dict[normalized_name] = field_data.get('value')

        elif field_data is not None:
            log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as simple value.\")
            flat_dict[normalized_name] = field_data

    return flat_dict"}
,
{"path": "application/utils/batch_optimizer.py", "encoding": "utf-8", "content": "import numpy as np

class DynamicBatchOptimizer:
    def __init__(self, config: dict):
        self.config = config
        self.current_size = config.get('initial_size', 500)
        self.max_size = config.get('max_size', 2000)
        self.min_size = config.get('min_size', 100)
        self.history = []

    def update(self, last_duration: float, memory_usage: float):
        current_mem_mb = memory_usage / 1e6
        self.history.append((last_duration, current_mem_mb))
        
        if current_mem_mb > (self.config['max_memory'] * self.config['memory_threshold']):
            new_size = max(self.min_size, int(self.current_size * self.config['reduce_factor']))
        
        elif len(self.history) > self.config['history_window']:
            avg_duration = np.mean([d for d, _ in self.history[-self.config['history_window']:]])
            
            if avg_duration > self.config['duration_threshold']:
                new_size = max(self.min_size, int(self.current_size * self.config['reduce_factor']))
            else:
                new_size = min(self.max_size, int(self.current_size * self.config['increase_factor']))
        
        self.current_size = new_size
        return self.current_size"}
,
{"path": "application/utils/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/use_cases/process_pipedrive_data.py", "encoding": "utf-8", "content": "from application.services.etl_service import ETLService
from typing import Dict

def run_pipedrive_etl_use_case(etl_service: ETLService) -> Dict[str, object]:
    \"\"\"
    Use case that processes Pipedrive data using the ETL service.

    Parameters:
        etl_service (ETLService): instance of the ETL service.

    Returns:
        Dict: A dictionary containing the results of the ETL run.
    \"\"\"
    result = etl_service.run_etl()
    return result"}
,
{"path": "application/use_cases/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "README.md", "encoding": "utf-8", "content": "# Pipedrive \u21c6 Metabase Integration

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Build](https://img.shields.io/badge/status-production-green)
![Maintained](https://img.shields.io/badge/maintained-yes-brightgreen)

> Integra\u00e7\u00e3o robusta entre o CRM Pipedrive e a ferramenta de BI Metabase, com ETL ass\u00edncrono, orquestra\u00e7\u00e3o Kubernetes e observabilidade com Grafana.

---

## \ud83d\ude80 Features

- \u26a1\ufe0f ETL de alta performance com staging tables UNLOGGED e `COPY`
- \u23f1\ufe0f Atualiza\u00e7\u00f5es quase em tempo real (padr\u00e3o: 30\u202fmin)
- \ud83e\udde0 Schema din\u00e2mico e suporte a campos customizados
- \ud83d\udcca Dashboards plug-and-play no Metabase
- \ud83d\udcc8 Observabilidade com Prometheus + Grafana
- \ud83d\udee0\ufe0f Deploy automatizado com Minikube + Kubernetes
- \ud83d\udd01 Sincroniza\u00e7\u00f5es auxiliares (usu\u00e1rios, pipelines, etc.)
- \ud83d\udd2c Experimentos de otimiza\u00e7\u00e3o de batch-size

---

## \ud83d\udcf8 Demonstra\u00e7\u00e3o

> *(adicione aqui um print ou gif de um dashboard do Metabase ou Grafana)*

---

## \ud83d\udcda \u00cdndice

1. [Vis\u00e3o Geral](#vis\u00e3o-geral)  
2. [Arquitetura](#arquitetura)  
3. [Stack Tecnol\u00f3gico](#stack-tecnol\u00f3gico)  
4. [Estrutura do Reposit\u00f3rio](#estrutura-do-reposit\u00f3rio)  
5. [Pr\u00e9\u2011requisitos](#pr\u00e9\u2011requisitos)  
6. [Guia R\u00e1pido (Minikube)](#guia-r\u00e1pido-minikube)  
7. [Configura\u00e7\u00e3o Detalhada](#configura\u00e7\u00e3o-detalhada)  
8. [Opera\u00e7\u00f5es do Dia\u2011a\u2011dia](#opera\u00e7\u00f5es-do-dia\u2011a\u2011dia)  
9. [Solu\u00e7\u00e3o de Problemas](#solu\u00e7\u00e3o-de-problemas)  
10. [Como Contribuir](#como-contribuir)  
11. [Licen\u00e7a](#licen\u00e7a)  

---

## \ud83c\udf10 Vis\u00e3o Geral

O projeto oferece um pipeline de dados **Kubernetes\u2011nativo**, totalmente automatizado e observ\u00e1vel:

- **ETL de alta performance** com `COPY` + staging tables
- **Atualiza\u00e7\u00e3o cont\u00ednua** para dashboards Metabase
- **M\u00e9tricas detalhadas** de uso, custo de tokens e performance
- **Autoescala e autorecupera\u00e7\u00e3o** via HPA e probes

---

## \ud83e\udde9 Arquitetura

```text
\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     API            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  Pipedrive \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502  ETL Flows   \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502 (Prefect)    \u2502
        \u25b2                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
        \u2502   Lookups / back\u2011fill          \u2502
        \u2502                                 \u25bc
\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  Queries        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  Metabase  \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502   Postgres     \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
        \u25b2                                 \u2502
        \u2502  Dashboards / alerts            \u2502 pushgateway
        \u2502                                 \u25bc
\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 <\u2500\u2500 PromQL \u2500\u2500\u2500\u2500\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510
\u2502  Grafana   \u2502                \u2502 Prometheus     \u2502
\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518
```

---

## \ud83e\uddea Stack Tecnol\u00f3gico

| Camada          | Componentes                            | Observa\u00e7\u00f5es                                                                       |
| --------------- | -------------------------------------- | --------------------------------------------------------------------------------- |
| Orquestra\u00e7\u00e3o    | **Prefect\u00a03 (Orion)**                  | `prefect.yaml` define 6\u00a0deployments                                               |
| Processamento   | Python\u00a03.12, Pandas, Pydantic          | Imagem multi\u2011stage via Poetry                                                     |
| Armazenamento   | **Postgres\u00a014**, **Redis\u00a07**           | PVC de 5\u202fGiB para dados                                                           |
| Observabilidade | **Prometheus + Pushgateway + Grafana** | Coleta m\u00e9tricas em `/metrics` por anota\u00e7\u00f5es                                       |
| Apresenta\u00e7\u00e3o    | **Metabase**                           | Exposto na porta `3000` no cluster                                                |

---

## \ud83d\uddc2\ufe0f Estrutura do Reposit\u00f3rio

```text
.
\u251c\u2500 flows/                    # Flows Prefect (ETL principal, back\u2011fill, syncs\u2026)
\u251c\u2500 infrastructure/
\u2502  \u251c\u2500 k8s/                   # Manifests + scripts entrypoint/wait
\u2502  \u251c\u2500 monitoring/            # Helpers de m\u00e9tricas Prometheus
\u2502  \u2514\u2500 prefect/orion/         # Imagem ultraleve do Orion
\u251c\u2500 application/              # Dom\u00ednio + ports/adapters
\u251c\u2500 run_project               # Build + deploy tudo em um comando
\u251c\u2500 create_secret_block.py    # Registra Secret no Prefect
\u251c\u2500 Dockerfile                # Build da imagem ETL
\u2514\u2500 prefect.yaml              # Deployments do Prefect
```

---

## \ud83e\uddf0 Pr\u00e9\u2011requisitos

- **Docker** \u2265\u00a020.x  
- **Minikube** \u2265\u00a01.31  
- **kubectl** \u2265\u00a01.26  
- **Poetry** \u2265\u00a01.5  
- **Prefect\u00a0CLI** \u2265\u00a02.14  
- GNU `bash`, `make` (para scripts auxiliares)

---

## \u26a1 Guia R\u00e1pido (Minikube)

```bash
# 1. Clone o reposit\u00f3rio e ajuste as vari\u00e1veis
git clone https://github.com/SUA_ORG/pipedrive_metabase_integration.git
cd pipedrive_metabase_integration
cp .env.template .env           # preencha PIPEDRIVE_API_KEY, POSTGRES_*, ...

# 2. Construa e fa\u00e7a o deploy de tudo
./run_project                   # ~10\u201115\u202fmin na primeira execu\u00e7\u00e3o

# 3. Registre o token GitHub como Secret Prefect (opcional)
python create_secret_block.py ghp_<token>

# 4. Inicie um agente Prefect local (caso n\u00e3o rode dentro do cluster)
prefect agent start -q kubernetes
```

| Servi\u00e7o    | URL                                            | Credenciais padr\u00e3o  |
| ---------- | ---------------------------------------------- | ------------------- |
| Prefect UI | [http://localhost:4200](http://localhost:4200) | (sem auth)          |
| Postgres   | `localhost:5432`                               | definidas no `.env` |
| Metabase   | [http://localhost:3000](http://localhost:3000) | definir no 1\u00ba login |
| Grafana    | [http://localhost:3015](http://localhost:3015) | `admin` / `admin`   |

---

## \u2699\ufe0f Configura\u00e7\u00e3o Detalhada

### \ud83d\udd10 Vari\u00e1veis de Ambiente

| Vari\u00e1vel                                            | Descri\u00e7\u00e3o                                   |
| --------------------------------------------------- | ------------------------------------------- |
| `PIPEDRIVE_API_KEY`                                 | Token pessoal do Pipedrive                  |
| `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB` | Credenciais do banco                        |
| `REDIS_URL`                                         | *Optional* \u2013 default `redis://redis:6379/0` |
| `PUSHGATEWAY_ADDRESS`                               | default `pushgateway:9091`                  |

> `entrypoint.sh` aborta se alguma estiver ausente. O script `run_project` gera automaticamente o Secret `app-secrets`.

---

### \ud83d\udc33 Build das Imagens

| Contexto build                   | Tag gerada                                            |
| -------------------------------- | ----------------------------------------------------- |
| reposit\u00f3rio raiz                 | `pipedrive_metabase_integration-etl:latest`           |
| `infrastructure/prefect/orion`   | `pipedrive_metabase_integration-prefect-orion:latest` |
| (metrics reutiliza a imagem ETL) | \u2014                                                     |

---

### \u2638\ufe0f Deploy no Kubernetes

Executado via:

```bash
./run_project deploy_infra
```

Inclui:

1. ConfigMap de observabilidade  
2. Secrets `app-secrets`, `db-secrets`  
3. PVC `pgdata-pvc`  
4. Stack de monitoramento  
5. Deployments + Services via `pipedrive_metabase_integration.yaml`

---

### \ud83e\udde0 Deploy dos Flows Prefect

```bash
./run_project deploy_prefect_flows
```

Inclui:

- ETL principal (cada 30\u202fmin)
- Back\u2011fill (manual)
- Syncs auxiliares em CRON
- Experimento de batch-size
- Deploy no pool `kubernetes-pool`

---

### \ud83d\udcc8 Observabilidade

- Todas as tasks fazem `push_to_gateway` com m\u00e9tricas
- Prometheus faz scrape autom\u00e1tico via `prometheus.io/scrape: true`
- Dashboard em `dashboards/metabase_pipeline.json`

---

## \ud83d\udee0\ufe0f Opera\u00e7\u00f5es do Dia\u2011a\u2011dia

| A\u00e7\u00e3o                | Comando / UI                                      |
| ------------------- | ------------------------------------------------- |
| Ver pipeline        | Grafana \u2192 Metabase Pipeline                       |
| Disparar sync       | Prefect UI \u2192 Deployments \u2192 Run                    |
| Escalar agent       | `kubectl scale deploy/prefect-agent --replicas=N` |
| Logs de m\u00e9tricas    | `kubectl logs -f deploy/metrics`                  |
| Atualizar c\u00f3digo    | `git pull && ./run_project`                       |

---

## \ud83e\uddef Solu\u00e7\u00e3o de Problemas

| Sintoma                        | Diagn\u00f3stico                                                    |
| ------------------------------ | -------------------------------------------------------------- |
| `create_secret_block.py` falha | Orion est\u00e1 online? O secret j\u00e1 existe?                         |
| Flows Pending                  | Agente online? `work_pool` correto?                            |
| ETL aborta por env ausente     | `.env` e Secret `app-secrets` est\u00e3o v\u00e1lidos?                   |
| M\u00e9tricas n\u00e3o aparecem          | Tem annotation `prometheus.io/scrape`? Pushgateway funcionando?|

---

## \ud83e\udd1d Como Contribuir

1. Fa\u00e7a um fork  
2. Crie uma branch de feature ou fix  
3. Commit \u2192 PR explicando claramente o problema ou melhoria

---

## \ud83d\udcc4 Licen\u00e7a

Distribu\u00eddo sob licen\u00e7a **MIT**. Veja `LICENSE` para mais detalhes."}
,
{"path": "pyproject.toml", "encoding": "utf-8", "content": "[tool.poetry]
name = \"pipedrive_metabase_integration\"
version = \"0.1.0\"
description = \"Integra\u00e7\u00e3o do Pipedrive com Metabase utilizando Airflow\"
authors = [\"Pavcob\"]
readme = [\"README.md\"]

[tool.poetry.dependencies]
python = \">=3.12, <3.13\"
prefect = {extras = [\"kubernetes\"], version = \"^3.3.2\"}
requests = \"2.32.3\"
python-decouple = \"3.8\"
python-dotenv = \"1.0.1\"
pydantic = '2.10.6'
prometheus-client = \"0.21.1\"
psycopg2  = '2.9.10'
httpx='0.28.1'
tenacity='9.0.0'
apache_beam='2.63.0'
python-json-logger='3.3.0'
prefect-sqlalchemy = \"^0.5.2\"
pandas = \"^2.2.3\"
numpy = \"^2.2.4\"
structlog = \"^25.2.0\"
geopy = \"^2.4.1\"
pybreaker = \"^1.3.0\"
psutil=\"7.0.0\"

[tool.poetry.group.dev.dependencies]
pytest = \"^7.2.0\"

[build-system]
requires = [\"poetry-core>=1.0.0\"]
build-backend = \"poetry.core.masonry.api\"
"}
,
{"path": "update.sh", "encoding": "utf-8", "content": "#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

##############################
# Configura\u00e7\u00f5es
##############################
# Mapeia nomes amig\u00e1veis para detalhes dos componentes
declare -A COMPONENTS=(
    [\"orion\"]=\"image_base=pipedrive_metabase_integration-prefect-orion;context=./infrastructure/prefect/orion;type=Deployment;manifest_id=prefect-orion\"
    [\"metrics\"]=\"image_base=pipedrive_metabase_integration-metrics;context=.;type=Deployment;manifest_id=metrics\"
    [\"etl\"]=\"image_base=pipedrive_metabase_integration-etl;context=.;type=Job;manifest_id=etl\"
    [\"redis\"]=\"image_base=redis;context=SKIP;type=Deployment;manifest_id=redis\" # SKIP build context for external images
    [\"db\"]=\"image_base=postgres;context=SKIP;type=Deployment;manifest_id=db\"
    [\"metabase\"]=\"image_base=metabase/metabase;context=SKIP;type=Deployment;manifest_id=metabase\"
    [\"grafana\"]=\"image_base=grafana/grafana;context=SKIP;type=Deployment;manifest_id=grafana\"
)

# Arquivo principal de manifesto
MANIFEST_FILE=\"pipedrive_metabase_integration.yaml\"
# Timeout para esperar rollouts/jobs
RESOURCE_TIMEOUT=600 # Reduzido para atualiza\u00e7\u00f5es, ajuste se necess\u00e1rio

##############################
# Fun\u00e7\u00f5es Auxiliares
##############################
log() {
    local LEVEL=\"$1\"; local MESSAGE=\"$2\"
    printf \"[%s] [%s] %s\n\" \"$(date '+%Y-%m-%d %H:%M:%S')\" \"${LEVEL^^}\" \"${MESSAGE}\"
}

fail() {
    log \"error\" \"$1\"; exit 1;
}

check_dependencies() {
    # (Igual ao deploy.sh)
    declare -A DEPS=( [\"docker\"]=\"Docker\" [\"kubectl\"]=\"Kubernetes CLI\" [\"minikube\"]=\"Minikube\" [\"sed\"]=\"sed\" )
    for cmd in \"${!DEPS[@]}\"; do
        if ! command -v \"${cmd}\" &> /dev/null; then
            fail \"${DEPS[$cmd]} n\u00e3o encontrado. Por favor instale primeiro.\"
        fi
    done
}

# Garante que estamos no ambiente docker do minikube
setup_minikube_env() {
    log \"info\" \"Configurando ambiente Docker do Minikube...\"
    eval \"$(minikube -p minikube docker-env)\" || fail \"Falha ao configurar ambiente Docker do Minikube.\"
    log \"info\" \"Verificando contexto kubectl...\"
    kubectl config use-context minikube || fail \"Falha ao definir contexto kubectl para minikube.\"
}

##############################
# Fun\u00e7\u00f5es de Atualiza\u00e7\u00e3o
##############################

# Constr\u00f3i uma imagem espec\u00edfica com uma nova tag
build_component_image() {
    local component_key=\"$1\"
    local details=\"${COMPONENTS[$component_key]}\"
    local new_tag=\"$2\" # Tag \u00fanica gerada
    local image_base context type manifest_id

    # Parse details string
    eval \"$(echo \"$details\" | awk -F';' '{for(i=1;i<=NF;i++) print $i}')\"

    if [[ \"$context\" == \"SKIP\" ]]; then
        log \"info\" \"Skipping build for external image: ${component_key} ($image_base)\"
        return 0 # Sucesso, pois n\u00e3o h\u00e1 o que construir
    fi

    local full_image_tag=\"${image_base}:${new_tag}\"
    log \"info\" \"Construindo imagem para ${component_key}: ${full_image_tag} (Contexto: ${context})\"

    export DOCKER_BUILDKIT=1 # Habilita BuildKit
    docker build \
        --progress=plain \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        # Tenta usar cache da tag 'latest' se existir
        --cache-from \"${image_base}:latest\" \
        -t \"${full_image_tag}\" \
        \"${context}\" || fail \"Falha ao construir imagem para ${component_key}\"

    log \"info\" \"Carregando imagem ${full_image_tag} no Minikube...\"
    minikube image load \"${full_image_tag}\" || fail \"Falha ao carregar imagem ${full_image_tag} no Minikube\"

    log \"success\" \"Imagem para ${component_key} constru\u00edda e carregada: ${full_image_tag}\"
}

# Atualiza a tag da imagem no arquivo MANIFEST_FILE usando sed (FR\u00c1GIL!)
update_manifest_image_tag() {
    local component_key=\"$1\"
    local details=\"${COMPONENTS[$component_key]}\"
    local new_tag=\"$2\"
    local image_base context type manifest_id

    eval \"$(echo \"$details\" | awk -F';' '{for(i=1;i<=NF;i++) print $i}')\"

    local full_image_base=\"${image_base}\" # Assume que image_base j\u00e1 cont\u00e9m o nome completo at\u00e9 o ':'

    log \"warning\" \"Tentando atualizar tag da imagem para ${manifest_id} em ${MANIFEST_FILE} para ${new_tag} usando sed.\"
    log \"warning\" \"Este m\u00e9todo \u00e9 FR\u00c1GIL! Mudan\u00e7as na formata\u00e7\u00e3o do YAML podem quebr\u00e1-lo.\"
    log \"warning\" \"Considere usar 'yq' para uma edi\u00e7\u00e3o de YAML mais robusta.\"
    # Exemplo com yq (requer instala\u00e7\u00e3o):
    # yq e \".spec.template.spec.containers[] |= select(.name == \\"${manifest_id}\\").image = \\"${full_image_base}:${new_tag}\\"\" -i \"${MANIFEST_FILE}\"

    # Tentativa com sed: Encontra o bloco do deployment/job pelo nome e atualiza a pr\u00f3xima linha 'image:'
    # Isso assume que 'image:' est\u00e1 logo ap\u00f3s ou perto de 'name: <manifest_id>' dentro de 'containers:'
    # Precisamos de um padr\u00e3o mais espec\u00edfico se a estrutura for complexa.
    # Tentativa: Encontra 'name: manifest_id', vai at\u00e9 a linha 'image:', e substitui a tag.
    # Este sed \u00e9 complexo e pode falhar facilmente. Use com cautela.
    # sed -i.bak \"/name: ${manifest_id}/,/image:/ s|^\(\s*image:\s*${image_base//\//\\/}:\)[\w.-]*|\1${new_tag}|\" \"${MANIFEST_FILE}\"

    # Tentativa mais simples (e talvez mais perigosa): Substitui a primeira ocorr\u00eancia ap\u00f3s o nome do container
     # Encontra a linha 'name: manifest_id', depois busca a *primeira* linha 'image:' seguinte e troca a tag.
     # Funciona SE houver apenas um container ou se o container certo for o primeiro com essa linha 'image:'
     sed -i.bak \"/name: ${manifest_id}/,/containers:/ { /image:/ { s|^\(\s*image:\s*${image_base//\//\\/}:\)[\w.-]*$|\1${new_tag}| ; T ; b end ; :end } ; }\" \"${MANIFEST_FILE}\" || fail \"Falha ao executar sed para atualizar ${MANIFEST_FILE}\"

    # Verifica se a substitui\u00e7\u00e3o realmente aconteceu (de forma b\u00e1sica)
    if ! grep -q \"${image_base}:${new_tag}\" \"${MANIFEST_FILE}\"; then
         log \"error\" \"Falha ao verificar a atualiza\u00e7\u00e3o da tag no ${MANIFEST_FILE} ap\u00f3s usar sed. Verifique ${MANIFEST_FILE}.bak e corrija manualmente.\"
         # Opcional: Restaurar backup?
         # mv \"${MANIFEST_FILE}.bak\" \"${MANIFEST_FILE}\"
         fail \"Atualiza\u00e7\u00e3o da tag da imagem falhou.\"
    fi
    log \"info\" \"Arquivo ${MANIFEST_FILE} atualizado (esperan\u00e7osamente) com a nova tag ${new_tag} para ${component_key}.\"
    rm -f \"${MANIFEST_FILE}.bak\" # Remove backup se tudo parece ok
}

# Aplica as mudan\u00e7as e espera o rollout (ou job)
apply_and_wait() {
    local component_key=\"$1\"
    local details=\"${COMPONENTS[$component_key]}\"
    local image_base context type manifest_id

    eval \"$(echo \"$details\" | awk -F';' '{for(i=1;i<=NF;i++) print $i}')\"

    if [[ \"$type\" == \"Job\" ]]; then
        log \"info\" \"Deletando Job ${manifest_id} anterior (se existir)...\"
        kubectl delete job \"${manifest_id}\" --ignore-not-found=true || log \"warning\" \"Falha ao deletar job ${manifest_id} anterior, pode j\u00e1 n\u00e3o existir.\"
        # Pequena pausa antes de aplicar o novo
        sleep 3
    fi

    log \"info\" \"Aplicando mudan\u00e7as do manifesto ${MANIFEST_FILE}...\"
    kubectl apply -f \"${MANIFEST_FILE}\" || fail \"Falha ao aplicar ${MANIFEST_FILE}\"

    if [[ \"$type\" == \"Deployment\" ]]; then
        log \"info\" \"Aguardando rollout do deployment/${manifest_id}...\"
        kubectl rollout status \"deployment/${manifest_id}\" \
            --timeout=\"${RESOURCE_TIMEOUT}s\" \
            --watch=true || fail \"Timeout ou erro aguardando rollout do deployment/${manifest_id}\"
        log \"success\" \"Rollout do deployment/${manifest_id} conclu\u00eddo.\"
    elif [[ \"$type\" == \"Job\" ]]; then
        log \"info\" \"Aguardando conclus\u00e3o do job/${manifest_id}...\"
        kubectl wait --for=condition=complete \
            --timeout=\"${RESOURCE_TIMEOUT}s\" \
            job/\"${manifest_id}\" || log \"warning\" \"Job ${manifest_id} n\u00e3o completou dentro do timeout ou falhou. Verifique os logs do job.\"
        # Verifica status ap\u00f3s o wait
        JOB_STATUS=$(kubectl get job \"${manifest_id}\" -o jsonpath='{.status.conditions[?(@.type==\"Complete\")].status}')
        if [[ \"$JOB_STATUS\" == \"True\" ]]; then
            log \"success\" \"Job ${manifest_id} conclu\u00eddo com sucesso.\"
            log \"info\" \"Coletando m\u00e9tricas de execu\u00e7\u00e3o do Job ${manifest_id}...\"
            kubectl logs \"job/${manifest_id}\" --tail=100 | grep \"ETL_COMPLETION_METRICS\" || true
        else
            JOB_FAILED_STATUS=$(kubectl get job \"${manifest_id}\" -o jsonpath='{.status.conditions[?(@.type==\"Failed\")].status}')
            if [[ \"$JOB_FAILED_STATUS\" == \"True\" ]]; then
                 log \"error\" \"Job ${manifest_id} falhou. Verifique os logs: kubectl logs job/${manifest_id}\"
            else
                 log \"warning\" \"Status final do Job ${manifest_id} incerto ap\u00f3s timeout.\"
            fi
        fi
    fi
}

##############################
# Fluxo Principal do Update
##############################

check_dependencies
setup_minikube_env

log \"info\" \"Componentes dispon\u00edveis para atualiza\u00e7\u00e3o:\"
COMPONENT_KEYS=(\"${!COMPONENTS[@]}\") # Array com as chaves
for i in \"${!COMPONENT_KEYS[@]}\"; do
    key=\"${COMPONENT_KEYS[$i]}\"
    details=\"${COMPONENTS[$key]}\"
    eval \"$(echo \"$details\" | awk -F';' '{for(i=1;i<=NF;i++) print $i}')\" # Parse para obter 'type'
    printf \"  %d) %s (%s)\n\" \"$((i+1))\" \"$key\" \"$type\"
done
echo \"  A) Todos os componentes com build (exceto imagens externas)\"
echo \"  N) Nenhum (Sair)\"

read -rp \"Digite os n\u00fameros dos componentes a atualizar (separados por espa\u00e7o), 'A' para Todos, ou 'N' para Sair: \" -a SELECTIONS

declare -a COMPONENTS_TO_UPDATE=()

if [[ \" ${SELECTIONS[@]} \" =~ \" A \" ]] || [[ \" ${SELECTIONS[@]} \" =~ \" a \" ]]; then
    log \"info\" \"Selecionado: Todos os componentes aplic\u00e1veis.\"
    for key in \"${COMPONENT_KEYS[@]}\"; do
        details=\"${COMPONENTS[$key]}\"
        eval \"$(echo \"$details\" | awk -F';' '{for(i=1;i<=NF;i++) print $i}')\"
        if [[ \"$context\" != \"SKIP\" ]]; then # Apenas os que t\u00eam build context
            COMPONENTS_TO_UPDATE+=(\"$key\")
        else
             log \"info\" \"Skipping ${key} (imagem externa) da sele\u00e7\u00e3o 'Todos'.\"
        fi
    done
elif [[ \" ${SELECTIONS[@]} \" =~ \" N \" ]] || [[ \" ${SELECTIONS[@]} \" =~ \" n \" ]]; then
    log \"info\" \"Saindo sem atualiza\u00e7\u00f5es.\"
    exit 0
else
    # Valida sele\u00e7\u00f5es num\u00e9ricas
    for sel in \"${SELECTIONS[@]}\"; do
        if [[ \"$sel\" =~ ^[0-9]+$ ]] && (( sel > 0 && sel <= ${#COMPONENT_KEYS[@]} )); then
            COMPONENTS_TO_UPDATE+=(\"${COMPONENT_KEYS[$((sel-1))]}\")
        else
            log \"warning\" \"Sele\u00e7\u00e3o inv\u00e1lida ignorada: ${sel}\"
        fi
    done
    # Remove duplicados se houver
    COMPONENTS_TO_UPDATE=($(printf \"%s\n\" \"${COMPONENTS_TO_UPDATE[@]}\" | sort -u))
fi

if [[ ${#COMPONENTS_TO_UPDATE[@]} -eq 0 ]]; then
    log \"error\" \"Nenhum componente v\u00e1lido selecionado para atualiza\u00e7\u00e3o.\"
    exit 1
fi

echo # Linha em branco
log \"info\" \"Componentes selecionados para atualiza\u00e7\u00e3o:\"
for comp in \"${COMPONENTS_TO_UPDATE[@]}\"; do
    echo \"  - $comp\"
done
echo # Linha em branco

read -rp \"Confirma a atualiza\u00e7\u00e3o destes componentes? (s/N): \" CONFIRM
if [[ ! \"$CONFIRM\" =~ ^[Ss]$ ]]; then
    log \"info\" \"Atualiza\u00e7\u00e3o cancelada.\"
    exit 0
fi

# Gera uma tag \u00fanica para esta atualiza\u00e7\u00e3o
NEW_IMAGE_TAG=$(date +%Y%m%d%H%M%S)
log \"info\" \"Usando a tag de imagem \u00fanica para esta atualiza\u00e7\u00e3o: ${NEW_IMAGE_TAG}\"

# Processa cada componente selecionado
for comp_key in \"${COMPONENTS_TO_UPDATE[@]}\"; do
    log \"info\" \"--- Iniciando atualiza\u00e7\u00e3o para: ${comp_key} ---\"
    build_component_image \"$comp_key\" \"$NEW_IMAGE_TAG\"
    update_manifest_image_tag \"$comp_key\" \"$NEW_IMAGE_TAG\"
    apply_and_wait \"$comp_key\"
    log \"info\" \"--- Atualiza\u00e7\u00e3o para ${comp_key} conclu\u00edda (ou tentativa feita) ---\"
    echo 
done

log \"success\" \"\u2705 Processo de atualiza\u00e7\u00e3o conclu\u00eddo para os componentes selecionados!\"
log \"info\" \"Verifique os logs e o status dos pods/jobs.\"
"}
,
{"path": "run_project.sh", "encoding": "utf-8", "content": "#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

##############################
# Configura\u00e7\u00f5es
##############################
declare -A IMAGES=(
    [\"etl\"]=\"pipedrive_metabase_integration-etl:latest\"
    [\"orion\"]=\"pipedrive_metabase_integration-prefect-orion:latest\"
)

RESOURCE_TIMEOUT=1800 # 30min
MINUTES=$((RESOURCE_TIMEOUT / 60))
MINIKUBE_CPUS=4
MINIKUBE_MEMORY=10240 # 10GB
MINIKUBE_DRIVER=docker
CLEANUP_NAMESPACES=\"default,kube-system\"
PREFECT_YAML_FILE=\"./infrastructure/k8s/prefect.yaml\"

##############################
# Fun\u00e7\u00f5es Auxiliares
##############################
log() {
    local LEVEL=\"$1\"
    local MESSAGE=\"$2\"
    printf \"[%s] [%s] %s\n\" \"$(date '+%Y-%m-%d %H:%M:%S')\" \"${LEVEL^^}\" \"${MESSAGE}\"
}

fail() {
    log \"error\" \"$1\"
    exit 1
}

cleanup() {
    log \"info\" \"Limpando recursos tempor\u00e1rios...\"
    pkill -P $$ || true
    kubectl delete pods --field-selector=status.phase!=Running,status.phase!=Pending -n default --wait=false || true
}

check_dependencies() {
    declare -A DEPS=(
        [\"docker\"]=\"Docker\"
        [\"kubectl\"]=\"Kubernetes CLI\"
        [\"minikube\"]=\"Minikube\"
        [\"curl\"]=\"cURL\"
        [\"prefect\"]=\"Prefect CLI\" 
    )

    for cmd in \"${!DEPS[@]}\"; do
        if ! command -v \"${cmd}\" &> /dev/null; then
            fail \"${DEPS[$cmd]} n\u00e3o encontrado. Por favor instale/configure primeiro.\"
        fi
    done
    if [[ ! -f \"${PREFECT_YAML_FILE}\" ]]; then
        fail \"Arquivo de configura\u00e7\u00e3o Prefect n\u00e3o encontrado em: ${PREFECT_YAML_FILE}\"
    fi
}


##############################
# Fun\u00e7\u00f5es Principais
##############################
stop_resources() {
    log \"info\" \"Parando todos os recursos...\"

    declare -a KILL_PIDS=(
        \"kubectl port-forward svc/db\"       
        \"kubectl port-forward svc/prefect-orion\"
        \"kubectl port-forward svc/metabase\"
        \"kubectl port-forward svc/grafana\"
    )

    for pid_pattern in \"${KILL_PIDS[@]}\"; do
        pkill -f \"${pid_pattern}\" || true
    done

    kubectl delete --all deployments,services,jobs,hpa,pvc,secrets,configmaps -n default --wait=true --ignore-not-found=true
    minikube addons disable metrics-server || true

    log \"success\" \"Recursos parados com sucesso.\"
    exit 0
}

start_minikube() {
    local STATUS
    STATUS=$(minikube status -o json | jq -r '.Host' 2>/dev/null || echo \"Error\")

    if [[ \"${STATUS}\" != \"Running\" ]]; then
        log \"info\" \"Iniciando Minikube com ${MINIKUBE_CPUS} CPUs e ${MINIKUBE_MEMORY}MB RAM...\"
        minikube start \
            --driver=\"${MINIKUBE_DRIVER}\" \
            --cpus=\"${MINIKUBE_CPUS}\" \
            --memory=\"${MINIKUBE_MEMORY}\" \
            --addons=metrics-server \
            --embed-certs=true \
            --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \
            --extra-config=apiserver.service-account-issuer=kubernetes/serviceaccount || fail \"Falha ao iniciar Minikube\"
    else
        log \"info\" \"Minikube j\u00e1 est\u00e1 rodando. Reutilizando inst\u00e2ncia existente.\"
    fi

    eval \"$(minikube docker-env)\"
    kubectl config use-context minikube
}

build_images() {
    log \"info\" \"Construindo imagens com BuildKit...\"

    export DOCKER_BUILDKIT=1
    for image in \"${!IMAGES[@]}\"; do
        local TAG=\"${IMAGES[$image]}\"
        local BUILD_CONTEXT=\".\"

        if [[ \"${image}\" == \"orion\" ]]; then
            BUILD_CONTEXT=\"./infrastructure/prefect/orion\"
        fi

        log \"info\" \"Building ${TAG} from context ${BUILD_CONTEXT}\"
        docker build \
            --progress=plain \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --cache-from \"${TAG}\" \
            -t \"${TAG}\" \
            \"${BUILD_CONTEXT}\" || fail \"Falha ao construir imagem ${TAG}\"
    done
}

# --- Fun\u00e7\u00e3o para aplicar Deployments Prefect ---
deploy_prefect_flows() {
    local work_pool_name=\"kubernetes-pool\"
    local secret_block_name=\"github-access-token\"
    local db_block_name=\"postgres-pool\"
    local redis_block_name=\"redis-cache\"

    local prefect_api_url_ip=\"http://127.0.0.1:4200/api\"

    # --- Verifica\u00e7\u00e3o Pr\u00e9via da Conex\u00e3o com Orion ---
    log \"debug\" \"Verificando acesso \u00e0 API Prefect via IP (${prefect_api_url_ip}) antes de criar blocos...\"
    sleep 5
    local health_check_url=\"${prefect_api_url_ip%/api}/health\"
    local attempt=0
    local max_attempts=5
    while ! curl --fail --max-time 5 -s \"${health_check_url}\" > /dev/null; do
         attempt=$((attempt + 1))
         if [[ $attempt -ge $max_attempts ]]; then
              log \"error\" \"Falha no teste de conex\u00e3o com ${health_check_url} ap\u00f3s ${max_attempts} tentativas.\"
              fail \"N\u00e3o foi poss\u00edvel conectar ao Prefect API via ${prefect_api_url_ip}. Imposs\u00edvel continuar.\"
         fi
         log \"debug\" \"Tentativa ${attempt}/${max_attempts}: Falha ao conectar a ${health_check_url}. Aguardando 5s...\"
         sleep 5
    done
     log \"debug\" \"Teste de conex\u00e3o com ${health_check_url} bem-sucedido.\"

    # --- Verifica\u00e7\u00e3o das Vari\u00e1veis de Ambiente para Blocos ---
    log \"info\" \"Verificando vari\u00e1veis de ambiente para cria\u00e7\u00e3o dos blocos...\"
    local required_block_vars=(\"GITHUB_PAT\" \"POSTGRES_USER\" \"POSTGRES_PASSWORD\" \"POSTGRES_DB\")
    local missing_vars_msg=\"\"
    for var in \"${required_block_vars[@]}\"; do
         if [[ -z \"${!var:-}\" ]]; then
              missing_vars_msg+=\"- ${var}\n\"
         fi
    done
    if [[ -n \"$missing_vars_msg\" ]]; then
        log \"error\" \"Vari\u00e1veis de ambiente obrigat\u00f3rias para criar blocos n\u00e3o definidas:\n${missing_vars_msg}\"
        fail \"Exporte as vari\u00e1veis necess\u00e1rias antes de rodar o script.\"
    else
        log \"info\" \"Vari\u00e1veis de ambiente para blocos parecem estar definidas.\"
    fi

    # --- Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o dos Blocos Core E INFRA via Script Python ---
    log \"info\" \"Executando script para criar/atualizar Blocos Prefect (${secret_block_name}, ${db_block_name}, ${redis_block_name}, k8s-jobs)...\" 
    export PREFECT_API_URL=\"${prefect_api_url_ip}\"
    log \"debug\" \"PREFECT_API_URL configurada como ${PREFECT_API_URL} para script Python e CLI.\"

    # Executa o script que agora tamb\u00e9m cria os blocos K8sJob
    if ! python create_or_update_core_blocks.py; then
         fail \"Falha ao executar create_or_update_core_blocks.py. Verifique os logs do script Python acima.\"
    fi
    log \"info\" \"Blocos Prefect (incluindo K8sJob) criados/atualizados com sucesso.\"

    # --- Cria\u00e7\u00e3o do Work Pool ---
    log \"info\" \"Verificando/Criando Work Pool Prefect: ${work_pool_name}...\"

    if ! prefect work-pool inspect \"${work_pool_name}\" > /dev/null 2>&1; then
         log \"info\" \"Criando work pool '${work_pool_name}'...\"
         if prefect work-pool create --type kubernetes \"${work_pool_name}\" --overwrite; then
             log \"info\" \"Work Pool '${work_pool_name}' criado com sucesso.\"
         else
             fail \"Falha ao criar work pool '${work_pool_name}'.\"
         fi
    else
         log \"info\" \"Work Pool '${work_pool_name}' j\u00e1 existe.\"
    fi

    # --- Aplica\u00e7\u00e3o dos Deployments ---
    log \"info\" \"Aplicando/Atualizando Deployments Prefect a partir do ${PREFECT_YAML_FILE}...\"
    if prefect deploy --all --prefect-file \"${PREFECT_YAML_FILE}\"; then 
        log \"success\" \"Deployments Prefect aplicados com sucesso via CLI.\"
    else
        fail \"Falha ao aplicar deployments Prefect via CLI. Verifique os logs do comando.\"
    fi
}

start_deploy_pm2() {
    log \"info\" \"Inicializando ambiente PM2 com portas e deploy de flows...\"

    eval \"$(minikube -p minikube docker-env)\"
    kubectl config use-context minikube

    declare -A PORTS=(
        [\"port-prefect\"]=\"kubectl port-forward svc/prefect-orion 4200:4200\"
        [\"port-grafana\"]=\"kubectl port-forward svc/grafana 3015:3015\"
        [\"port-metabase\"]=\"kubectl port-forward svc/metabase 3000:3000\"
        [\"port-db\"]=\"kubectl port-forward svc/db 5432:5432\"
    )

    for PROC in \"${!PORTS[@]}\"; do
        if pm2 list | grep -q \"$PROC\"; then
            log \"info\" \"Reiniciando processo PM2 existente: $PROC\"
            pm2 restart \"$PROC\"
        else
            log \"info\" \"Criando novo processo PM2: $PROC\"
            pm2 start bash --name \"$PROC\" -- -c \"${PORTS[$PROC]}\"
        fi
    done

    pm2 save

    log \"success\" \"Todos processos PM2 configurados. Iniciando deploy de flows Prefect...\"
}

deploy_infra() {
    log \"info\" \"Aplicando configura\u00e7\u00f5es base...\"

    kubectl apply -f infrastructure/k8s/observability-config.yaml --server-side=true || fail \"Falha ao aplicar observability-config.yaml\"
    kubectl apply -f infrastructure/k8s/db-secrets.yaml || fail \"Falha ao aplicar db-secrets.yaml\"
    if kubectl get pvc pgdata-pvc > /dev/null 2>&1 ; then
       log \"info\" \"PVC pgdata-pvc j\u00e1 existe.\"
    else
       kubectl apply -f infrastructure/k8s/persistent-volume-claim.yaml || fail \"Falha ao aplicar persistent-volume-claim.yaml\"
    fi

    kubectl apply -f infrastructure/k8s/prometheus.yml || fail \"Falha ao aplicar prometheus.yml\"
    kubectl apply -f infrastructure/k8s/pushgateway.yaml || fail \"Falha ao aplicar pushgateway.yaml\"

    log \"info\" \"Criando/Atualizando secret 'app-secrets' a partir do .env\"
    kubectl create secret generic app-secrets \
        --from-env-file=.env \
        --dry-run=client \
        -o yaml | kubectl apply -f - || fail \"Falha ao criar/atualizar app-secrets\"

    log \"info\" \"Aplicando manifesto principal (sem o Job 'etl' est\u00e1tico)...\"
    kubectl apply -f infrastructure/k8s/pipedrive_metabase_integration.yaml || fail \"Falha ao aplicar pipedrive_metabase_integration.yaml\"
}

wait_for_rollout() {
    local DEPLOYMENTS=(\"prefect-orion\" \"prefect-agent\" \"pushgateway\" \"prometheus-deployment\" \"redis\" \"db\" \"grafana\")

    for dep in \"${DEPLOYMENTS[@]}\"; do
        if kubectl get deployment \"${dep}\" > /dev/null 2>&1; then
             log \"info\" \"Aguardando rollout do deployment/${dep}...\"
             kubectl rollout status \"deployment/${dep}\" \
                 --timeout=\"${RESOURCE_TIMEOUT}s\" \
                 --watch=true || fail \"Timeout ou erro aguardando rollout do deployment/${dep}\"
        else
             log \"warning\" \"Deployment ${dep} n\u00e3o encontrado, pulando espera do rollout.\"
        fi
    done
    log \"info\" \"Rollout de todos os deployments principais conclu\u00eddo.\"
}

setup_port_forwarding() {
     declare -A PORTS=(
        [\"prefect-orion\"]=\"4200\"
        [\"db\"]=\"5432\"
        [\"metabase\"]=\"3000\"  
        [\"grafana\"]=\"3015\"        
    )

     log \"info\" \"Iniciando port-forward para os services...\"

     for svc in \"${!PORTS[@]}\"; do
         local PORT=\"${PORTS[$svc]}\"
         if kubectl get service \"${svc}\" > /dev/null 2>&1; then
             log \"warn\" \"Removendo port-fowards existentes para ${svc}...\"
             pkill -f \"kubectl port-forward svc/${svc} ${PORT}:${PORT}\" || true
             log \"info\" \"Iniciando port-forward para ${svc} na porta ${PORT}\"
             kubectl port-forward \"svc/${svc}\" \"${PORT}:${PORT}\" &
             sleep 2 
         else
             log \"warning\" \"Servi\u00e7o ${svc} n\u00e3o encontrado, pulando port-forward.\"
         fi
     done
     log \"info\" \"Aguardando alguns segundos para estabilizar os port-forwards...\"
     sleep 10

     log \"info\" \"Port-forwards iniciados (se os servi\u00e7os existirem).\"
     for svc in \"${!PORTS[@]}\"; do
         local PORT=\"${PORTS[$svc]}\"
         if pgrep -f \"kubectl port-forward svc/${svc} ${PORT}:${PORT}\" > /dev/null; then
             log \"info\" \"[${svc}] Iniciado na porta ${PORT}, acesse: http://localhost:${PORT}\"
         fi
     done
}

##############################
# Fluxo Principal
##############################
trap cleanup EXIT # Adiciona trap para limpeza em caso de erro ou interrup\u00e7\u00e3o

case \"${1:-}\" in
    stop)
        stop_resources
        ;;
    port-forward)
        setup_port_forwarding
        ;;
    check)
        check_dependencies
        ;;
    build)
        check_dependencies
        start_minikube
        build_images
        ;;
    deploy-infra)
        deploy_infra
        ;;
    deploy-flows)
        deploy_prefect_flows
        ;;
    wait-rollout)
        wait_for_rollout
        ;;
    start)
        check_dependencies
        start_minikube
        build_images
        deploy_infra
        wait_for_rollout
        deploy_prefect_flows
        ;;
    start-pm2)
        check_dependencies
        start_minikube
        build_images
        deploy_infra
        wait_for_rollout
        start_deploy_pm2
        deploy_prefect_flows
        ;;
    full)
        check_dependencies
        start_minikube
        build_images
        deploy_infra
        wait_for_rollout
        port_forwarding
        deploy_prefect_flows
        log \"success\" \"\u2705 Infraestrutura e fluxos implantados com sucesso!\"
        wait
        ;;
    *)
        echo \"Uso: $0 {start|stop|check|build|deploy-infra|deploy-flows|wait-rollout|port-forward|full}\"
        exit 1
        ;;
esac"}
,
{"path": "tests/test_use_cases.py", "encoding": "base64", "content": ""}
,
{"path": "tests/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "tests/test_infrastructure.py", "encoding": "base64", "content": ""}
,
{"path": ".env", "encoding": "utf-8", "content": "##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DE BANCO DE DADOS                     #
##############################################################################
POSTGRES_DB=pipedrive_metabase_integration_db
POSTGRES_PASSWORD=pipedrive_metabase_integration_db
POSTGRES_USER=pipedrive_metabase_integration_db
POSTGRES_PORT=5432
POSTGRES_HOST=db
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

##############################################################################
#                             CONFIGURA\u00c7\u00d5ES DO REDIS                         #
##############################################################################
REDIS_URL=\"redis://redis:6379/0\"

##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DE APLICACAO                          #
##############################################################################
API_PORT=8080
APP_METRICS_PORT=8082
WEB_SERVER_PORT=8081
FERNET_KEY=\"fH7No6yfy6yhb3fPzgKURIMvA+c5hMnZSD8czvL1S/o=\"

##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DO PIPEDRIVE                          #
##############################################################################
PIPEDRIVE_API_KEY=bb0cf5c38584a41fd54a90503e5767bcd9ed381c

##############################################################################
#                         CONFIGURA\u00c7\u00d5ES DO PREFECT                           #
##############################################################################
PREFECT_PORT=4200
PREFECT_API_URL=http://prefect-orion:4200/api

##############################################################################
#                         CONFIGURA\u00c7\u00d5ES DO GRAFANA                           #
##############################################################################
GF_SERVER_HTTP_PORT=3015
PUSHGATEWAY_ADDRESS=pushgateway:9091"}
]
