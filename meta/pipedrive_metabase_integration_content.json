[
{"path": "Dockerfile", "encoding": "utf-8", "content": "# ---- Etapa 1: Builder ----
    FROM python:3.12-slim as builder

    WORKDIR /app
    ENV INTERNAL_ENV=${INTERNAL_ENV} \
        PYTHONFAULTHANDLER=1 \
        PYTHONUNBUFFERED=1 \
        PYTHONHASHSEED=random \
        PYTHONDONTWRITEBYTECODE=1 \
        PIP_NO_CACHE_DIR=off \
        PIP_DISABLE_PIP_VERSION_CHECK=on \
        PIP_DEFAULT_TIMEOUT=100 \
        POETRY_NO_INTERACTION=1 \
        POETRY_VIRTUALENVS_CREATE=false \
        POETRY_CACHE_DIR='/var/cache/pypoetry' \
        POETRY_HOME='/usr/local' \
        POETRY_VERSION=2.1.1
    
    RUN apt-get update && \
        apt-get install -y curl gcc libpq-dev netcat-openbsd git && \
        rm -rf /var/lib/apt/lists/*
    
    # Instalar o Poetry e configur\u00e1-lo para n\u00e3o criar virtualenv
    RUN curl -sSL https://install.python-poetry.org | python3 -
    ENV PATH=\"$POETRY_HOME/bin:$PATH\"
    RUN poetry config virtualenvs.create false
    
    # Copiar os arquivos de depend\u00eancias e instalar os pacotes
    COPY pyproject.toml poetry.lock* ./
    RUN poetry install --no-root --no-interaction --no-ansi && \
    poetry run pip install prefect-sqlalchemy --no-cache-dir

    # Copiar o c\u00f3digo-fonte completo
    COPY . .
    
    # ---- Etapa 2: Imagem Final ----
    FROM python:3.12-slim
    
    WORKDIR /app
    ENV INTERNAL_ENV_FINAL=${INTERNAL_ENV_FINAL} \
    PYTHONPATH=/app \
    PYTHONFAULTHANDLER=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONHASHSEED=random \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=off \
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    POETRY_NO_INTERACTION=1 \
    POETRY_VIRTUALENVS_CREATE=false
    
    RUN apt-get update && \
        apt-get install -y curl gcc libpq-dev netcat-openbsd git && \
        rm -rf /var/lib/apt/lists/*
    
    # Copiar os pacotes instalados e o c\u00f3digo da etapa builder
    COPY --from=builder /usr/local /usr/local
    COPY --from=builder /app /app
    
    # Garantir que os scripts tenham permiss\u00e3o de execu\u00e7\u00e3o
    COPY infrastructure/k8s/wait-for-it.sh /app/wait-for-it.sh
    COPY infrastructure/k8s/entrypoint.sh /app/entrypoint.sh
    RUN chmod +x /app/wait-for-it.sh
    RUN chmod +x /app/entrypoint.sh
    
    # Comando de entrada
    CMD [\"/app/entrypoint.sh\"]
    "}
,
{"path": "infrastructure/monitoring/metrics_server.py", "encoding": "utf-8", "content": "from prometheus_client import start_http_server
import time
import os
import structlog

log = structlog.get_logger(__name__)

DEFAULT_PORT = 8082

def start_metrics_server(port: int = DEFAULT_PORT):
    \"\"\"Starts the Prometheus metrics HTTP server.\"\"\"
    actual_port = int(os.environ.get(\"APP_METRICS_PORT\", port))
    log.info(f\"Attempting to start Prometheus metrics server on port {actual_port}...\")
    try:

        start_http_server(actual_port)
        log.info(f\"Prometheus metrics server started successfully on port {actual_port}.\")
        while True:
            time.sleep(60)
    except OSError as e:
        log.error(f\"Failed to start metrics server on port {actual_port}. Port likely in use.\", exc_info=True)
        raise
    except Exception as e:
        log.error(\"Metrics server encountered an unexpected error\", exc_info=True)
        raise


if __name__ == \"__main__\":
    start_metrics_server()"}
,
{"path": "infrastructure/monitoring/metrics.py", "encoding": "utf-8", "content": "import os
import structlog
from prometheus_client import (
    Counter,
    Gauge,
    Histogram,
    Summary,
    REGISTRY,
    push_to_gateway as prometheus_push
)

log = structlog.get_logger(__name__)
PUSHGATEWAY_ADDRESS = os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\")
push_log = structlog.get_logger(\"push_metrics\")

# --- Buckets ---
REQUEST_DURATION_BUCKETS = [0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10, 30, 60, 120]
DB_LATENCY_BUCKETS = [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
RECORD_COUNT_BUCKETS = [0, 10, 50, 100, 250, 500, 1000, 2500, 5000, 10000]
TOKEN_COST_BUCKETS = [10, 50, 100, 250, 500, 1000, 2500, 5000, 10000, 20000]
API_CALL_COUNT_BUCKETS = [1, 5, 10, 20, 50, 100, 250, 500]

# --- General ETL Counters ---
etl_runs_total = Counter(
    \"pipedrive_etl_runs_total\",
    \"Total ETL flow executions initiated\",
    [\"flow_type\"]
)
etl_run_failures_total = Counter(
    \"pipedrive_etl_run_failures_total\",
    \"Total ETL flow executions that failed critically\",
    [\"flow_type\"]
)
etl_records_fetched_total = Counter(
    \"pipedrive_etl_records_fetched_total\",
    \"Total number of raw records fetched from the source API\",
    [\"flow_type\"]
)
etl_record_processing_failures_total = Counter(
    \"pipedrive_etl_record_processing_failures_total\",
    \"Total records that failed during processing stages (schema, domain, load)\",
    [\"flow_type\", \"failure_stage\"] # failure_stage: 'schema', 'domain', 'enrich', 'load'
)
etl_records_loaded_total = Counter(
    \"pipedrive_etl_records_loaded_total\",
    \"Total number of records successfully loaded/upserted into the target\",
    [\"flow_type\"]
)
etl_batches_processed_total = Counter(
    \"pipedrive_etl_batches_processed_total\",
    \"Total number of batches processed\",
    [\"flow_type\"]
)
etl_empty_batches_total = Counter(
    \"pipedrive_etl_empty_batches_total\",
    \"Total ETL batches that had no data after fetching/filtering\",
    [\"flow_type\"]
)
etl_skipped_batches_total = Counter(
    \"pipedrive_etl_skipped_batches_total\",
    \"Total batches skipped due to validation errors or other reasons before load\",
    [\"flow_type\"]
)

# --- API Interaction Metrics ---
pipedrive_api_calls_total = Counter(
    \"pipedrive_api_calls_total\",
    \"Total Pipedrive API calls made\",
    [\"endpoint\", \"method\", \"status_code\"]
)
pipedrive_api_errors_total = Counter(
    \"pipedrive_api_errors_total\",
    \"Total Pipedrive API errors by type\",
    [\"endpoint\", \"error_type\", \"status_code\"]
)
pipedrive_api_token_cost_total = Counter(
    \"pipedrive_api_token_cost_total\",
    \"Estimated total token cost consumed for Pipedrive API calls\",
    [\"endpoint\"]
)
pipedrive_api_request_duration_seconds = Histogram(
    \"pipedrive_api_request_duration_seconds\",
    \"Pipedrive API request latency distribution\",
    [\"endpoint\", \"method\", \"status_code\"],
    buckets=REQUEST_DURATION_BUCKETS
)
pipedrive_api_rate_limit_remaining = Gauge(
    \"pipedrive_api_rate_limit_remaining\",
    \"Remaining API quota reported by Pipedrive headers\",
    [\"endpoint\"]
)
pipedrive_api_calls_per_batch_hist = Histogram(
    \"pipedrive_api_calls_per_batch\",
    \"Distribution of Pipedrive API calls made per ETL batch\",
    [\"flow_type\"],
    buckets=API_CALL_COUNT_BUCKETS
)

# --- Cache Interaction Metrics ---
pipedrive_cache_hit_total = Counter(
    \"pipedrive_cache_hit_total\",
    \"Cache hit count for Pipedrive related lookups\",
    [\"entity\", \"source\"] # source: 'redis', 'memory', etc.
)
pipedrive_cache_miss_total = Counter(
    \"pipedrive_cache_miss_total\",
    \"Cache miss count for Pipedrive related lookups\",
    [\"entity\", \"source\"]
)
etl_cache_errors_total = Counter(
    \"pipedrive_etl_cache_errors_total\",
    \"Errors encountered during cache operations\",
    [\"operation\", \"cache_key_type\"] # operation: 'get', 'set', 'delete'
)

# --- Database Interaction Metrics ---
etl_db_operation_duration_seconds = Histogram(
    \"pipedrive_etl_db_operation_duration_seconds\",
    \"Database operation latency distribution\",
    [\"operation\"], # e.g., 'upsert_deals', 'update_history', 'lookup_fetch', 'schema_update'
    buckets=DB_LATENCY_BUCKETS
)
etl_db_batch_latency_seconds = Histogram(
    \"pipedrive_etl_db_batch_latency_seconds\",
    \"Database batch operation latency distribution\",
    [\"operation\", \"batch_size\"],
    buckets=REQUEST_DURATION_BUCKETS # Use request buckets for potentially longer ops
)
etl_db_errors_total = Counter(
    \"pipedrive_etl_db_errors_total\",
    \"Total database errors encountered during ETL operations\",
    [\"flow_type\", \"operation\", \"error_code\"]
)

# --- Data Flow & Processing Metrics ---
etl_batch_processing_phase_duration_seconds = Histogram(
    \"pipedrive_etl_batch_processing_phase_duration_seconds\",
    \"Duration of specific phases within batch processing\",
    [\"flow_type\", \"phase\"], # phase: 'schema_validation', 'domain_mapping', 'persist_mapping', 'enrichment', 'db_load'
    buckets=REQUEST_DURATION_BUCKETS
)
etl_lookup_enrichment_duration_seconds = Histogram(
    \"pipedrive_etl_lookup_enrichment_duration_seconds\",
    \"Time spent enriching data with lookups\",
    [\"flow_type\"],
    buckets=REQUEST_DURATION_BUCKETS
)
etl_loaded_records_per_batch_hist = Histogram(
    \"pipedrive_etl_loaded_records_per_batch_hist\",
    \"Distribution of records loaded per batch\",
    [\"flow_type\"],
    buckets=RECORD_COUNT_BUCKETS
)

# --- Data Quality & Validation Metrics ---
etl_batch_validation_errors_total = Counter(
    \"pipedrive_etl_batch_validation_errors_total\",
    \"Total validation errors per batch processing stage\",
    [\"flow_type\", \"validation_stage\", \"error_detail\"] # stage: 'schema', 'domain'; detail: field path or error type
)
etl_data_consistency_issues_found_total = Counter(
    \"pipedrive_etl_data_consistency_issues_found_total\",
    \"Data consistency issues found by specific checks\",
    [\"flow_type\", \"check_name\"] # e.g., 'date_order'
)
etl_schema_drift_detected_total = Counter(
    \"pipedrive_etl_schema_drift_detected_total\",
    \"Total times dynamic schema changes (column additions) were detected and applied\",
    [\"flow_type\"]
)

# --- Auxiliary Sync Flow Metrics ---
sync_runs_total = Counter(
    \"pipedrive_aux_sync_runs_total\",
    \"Total auxiliary sync flow executions initiated\",
    [\"entity_type\"]
)
sync_run_failures_total = Counter(
    \"pipedrive_aux_sync_run_failures_total\",
    \"Total auxiliary sync flow executions that failed\",
    [\"entity_type\"]
)
sync_records_upserted_total = Counter(
    \"pipedrive_aux_sync_records_upserted_total\",
    \"Total records upserted during auxiliary sync flows\",
    [\"entity_type\"]
)
sync_api_fetch_duration_seconds = Histogram(
    \"pipedrive_aux_sync_api_fetch_duration_seconds\",
    \"Duration of API data fetching for auxiliary syncs\",
    [\"entity_type\"],
    buckets=REQUEST_DURATION_BUCKETS
)
sync_db_upsert_duration_seconds = Histogram(
    \"pipedrive_aux_sync_db_upsert_duration_seconds\",
    \"Duration of database upsert operations for auxiliary syncs\",
    [\"entity_type\"],
    buckets=REQUEST_DURATION_BUCKETS
)

# --- Backfill Flow Metrics ---
backfill_changelog_api_errors_total = Counter(
    \"pipedrive_backfill_changelog_api_errors_total\",
    \"Total API errors encountered while fetching deal changelogs during backfill\"
)
backfill_db_update_errors_total = Counter(
    \"pipedrive_backfill_db_update_errors_total\",
    \"Total database errors encountered while updating stage history during backfill\"
)
backfill_deals_remaining_gauge = Gauge(
    \"pipedrive_backfill_deals_remaining_estimated\",
    \"Estimated number of deals remaining for stage history backfill\"
)

# --- Experiment Metrics ---
batch_experiment_runs_total = Counter(
    \"pipedrive_batch_experiment_runs_total\",
    \"Total batch size experiment executions initiated\",
    [\"experiment\", \"batch_size\", \"flow_run_id\"]
)
batch_experiment_best_score = Gauge(
    \"pipedrive_batch_experiment_best_score\",
    \"Best score calculated in the batch size experiment\",
    [\"flow_run_id\", \"metric\"] # metric: e.g., 'overall_score'
)
batch_experiment_success_rate = Gauge(
    \"pipedrive_batch_experiment_success_rate\",
    \"Success rate observed for a specific batch size during experiment\",
    [\"batch_size\", \"flow_run_id\"]
)

# --- System Health & Status Gauges ---
etl_process_memory_mbytes = Gauge(
    \"pipedrive_etl_process_memory_mbytes\",
    \"Peak memory usage (MB) reported by the ETL process\",
    [\"flow_type\"]
)
etl_process_cpu_percent = Gauge(
    \"pipedrive_etl_process_cpu_percent\",
    \"CPU usage percentage reported by the ETL process\",
    [\"flow_type\"]
)
etl_process_thread_count = Gauge(
    \"pipedrive_etl_process_thread_count\",
    \"Number of active threads reported by the ETL process\",
    [\"flow_type\"]
)
etl_process_disk_usage_bytes = Gauge(
    \"pipedrive_etl_process_disk_usage_bytes\",
    \"Disk usage (bytes) reported by the ETL process for a specific mount point\",
    [\"mount_point\"]
)
etl_last_successful_run_timestamp = Gauge(
    \"pipedrive_etl_last_successful_run_timestamp\",
    \"Unix timestamp of the last successfully completed ETL flow run\",
    [\"flow_type\"]
)
etl_heartbeat = Gauge(
    \"pipedrive_etl_heartbeat\",
    \"Unix timestamp of the last heartbeat (run completion or check-in)\",
    [\"flow_type\"]
)
etl_pushgateway_up = Gauge(
    \"pipedrive_etl_pushgateway_up\",
    \"Indicates if the last attempt to push to Pushgateway was successful (1=OK, 0=Fail)\",
    [\"instance\"]
)
etl_component_initialization_status = Gauge(
    \"pipedrive_etl_component_initialization_status\",
    \"Status of component initialization (1=OK, 0=Fail)\",
    [\"component_name\"]
)
batch_size_gauge = Gauge(
    \"pipedrive_etl_batch_size\",
    \"Number of records in the current processing batch (gauge for current value)\",
    [\"flow_type\"]
)


# --- Fun\u00e7\u00e3o de Push ---
def push_metrics_to_gateway(job_name=\"pipedrive_etl_job\", grouping_key=None):
    try:
        push_log.info(\"Attempting to push metrics to Pushgateway...\",
                      address=PUSHGATEWAY_ADDRESS,
                      job=job_name,
                      grouping_key=grouping_key
                      )
        prometheus_push(gateway=PUSHGATEWAY_ADDRESS,
                        job=job_name,
                        registry=REGISTRY,
                        grouping_key=grouping_key
                        )
        etl_pushgateway_up.labels(instance=PUSHGATEWAY_ADDRESS).set(1)
        push_log.info(\"Successfully pushed metrics to Pushgateway.\")
    except Exception as push_err:
        etl_pushgateway_up.labels(instance=PUSHGATEWAY_ADDRESS).set(0)
        push_log.error(\"Failed to push metrics to Pushgateway\",
                       error=str(push_err),
                       address=PUSHGATEWAY_ADDRESS,
                       job=job_name,
                       exc_info=True)"}
,
{"path": "infrastructure/monitoring/__init__.py", "encoding": "utf-8", "content": "from .metrics import *"}
,
{"path": "infrastructure/config/settings.py", "encoding": "utf-8", "content": "import os
from dotenv import load_dotenv

load_dotenv()

class Settings:
    \"\"\"
    Configura\u00e7\u00f5es da aplica\u00e7\u00e3o, lidas de vari\u00e1veis de ambiente.
    \"\"\"
    PIPEDRIVE_API_KEY = os.getenv(\"PIPEDRIVE_API_KEY\")

    POSTGRES_DB = os.getenv(\"POSTGRES_DB\")
    POSTGRES_USER = os.getenv(\"POSTGRES_USER\")
    POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")
    POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\")
    POSTGRES_PORT = os.getenv(\"POSTGRES_PORT\")
    DATABASE_URL = os.getenv(\"DATABASE_URL\")

    REDIS_URL = os.getenv(\"REDIS_URL\")
    
    BATCH_OPTIMIZER_CONFIG = {
        'memory_threshold': 0.8,
        'reduce_factor': 0.7,
        'duration_threshold': 30,
        'increase_factor': 1.2,
        'history_window': 5
    }

settings = Settings()
"}
,
{"path": "infrastructure/config/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/repository_impl/config_repository.py", "encoding": "utf-8", "content": "from typing import Dict, Any, Optional
from psycopg2 import sql, pool, extras
import structlog
import json
from datetime import datetime, timezone

from infrastructure.db.schema_manager import SchemaManager

class ConfigRepository:
    \"\"\"Repository for managing key-value configurations stored in the database.\"\"\"
    TABLE_NAME = \"config\"
    COLUMNS = {
        \"key\": \"TEXT PRIMARY KEY\",
        \"value\": \"JSONB\",
        \"updated_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
    }

    def __init__(self, db_pool: pool.SimpleConnectionPool, schema_manager: SchemaManager):
        self.db_pool = db_pool
        self.schema_manager = schema_manager
        self.log = structlog.get_logger(__name__).bind(repository=self.__class__.__name__)

    def initialize_schema(self):
        \"\"\"Initializes the schema for the config table.\"\"\"
        self.schema_manager.ensure_table_exists(
            self.TABLE_NAME,
            list(self.COLUMNS.items()),
            primary_key=\"key\"
        )
        self.log.info(\"Schema initialized\", table_name=self.TABLE_NAME)

    def save_configuration(self, key: str, value: Dict[str, Any]):
        \"\"\"Saves or updates a configuration value (JSONB) in the database.\"\"\"
        conn = None
        log_ctx = self.log.bind(config_key=key)
        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                config_table_id = sql.Identifier(self.TABLE_NAME)
                upsert_sql = sql.SQL(\"\"\"
                    INSERT INTO {config_table} (key, value, updated_at)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (key) DO UPDATE SET
                        value = EXCLUDED.value,
                        updated_at = EXCLUDED.updated_at;
                \"\"\").format(config_table=config_table_id)

                now_utc = datetime.now(timezone.utc)
                value_to_save = value.copy()
                if 'updated_at' not in value_to_save:
                     value_to_save['updated_at'] = now_utc.isoformat()

                try:
                    json_value = json.dumps(value_to_save)
                except TypeError as json_err:
                    log_ctx.error(\"Configuration value is not JSON serializable\", error=str(json_err))
                    raise ValueError(\"Configuration value must be JSON serializable\") from json_err

                cur.execute(upsert_sql, (key, json_value, now_utc))
                conn.commit()
                log_ctx.info(\"Configuration saved successfully\")
        except Exception as e:
            if conn: conn.rollback()
            log_ctx.error(\"Failed to save configuration\", error=str(e), exc_info=True)
            raise
        finally:
            if conn:
                self.db_pool.putconn(conn)

    def get_configuration(self, config_key: str) -> Optional[Dict[str, Any]]:
        \"\"\"Retrieves a configuration value (JSONB) from the database.\"\"\"
        conn = None
        log_ctx = self.log.bind(config_key=config_key)
        try:
            conn = self.db_pool.getconn()
            with conn.cursor(cursor_factory=extras.DictCursor) as cur:
                query = sql.SQL(\"SELECT value, updated_at FROM {config_table} WHERE key = %s LIMIT 1\").format(
                    config_table=sql.Identifier(self.TABLE_NAME)
                )
                cur.execute(query, (config_key,))
                result = cur.fetchone()
                if result:
                    log_ctx.debug(\"Configuration retrieved\", updated_at=result['updated_at'])
                    return result['value']
                else:
                    log_ctx.debug(\"Configuration key not found\")
                    return None
        except Exception as e:
            log_ctx.error(\"Failed to get configuration\", error=str(e), exc_info=True)
            return None
        finally:
            if conn:
                self.db_pool.putconn(conn)"}
,
{"path": "infrastructure/repository_impl/base_lookup_repository.py", "encoding": "utf-8", "content": "from typing import List, Dict, Optional, Set, Any
from psycopg2 import sql, pool, extras
import structlog
import time

from infrastructure.db.schema_manager import SchemaManager

UNKNOWN_NAME = \"Desconhecido\"

class BaseLookupRepository:
    \"\"\"
    Base class for lookup table repositories providing common upsert and query logic.
    Subclasses must define TABLE_NAME, ID_COLUMN, COLUMNS, and optionally NAME_COLUMN, INDEXES.
    \"\"\"
    TABLE_NAME: str = \"\"
    ID_COLUMN: str = \"\"
    NAME_COLUMN: Optional[str] = None 
    COLUMNS: Dict[str, str] = {} # { column_name: type_definition_str }
    INDEXES: Dict[str, str] = {} # { index_name: column_definition_str }

    def __init__(self, db_pool: pool.SimpleConnectionPool, schema_manager: SchemaManager):
        if not self.TABLE_NAME or not self.ID_COLUMN or not self.COLUMNS:
            raise NotImplementedError(
                f\"{self.__class__.__name__} must define TABLE_NAME, ID_COLUMN, and COLUMNS\"
            )
        self.db_pool = db_pool
        self.schema_manager = schema_manager
        self.log = structlog.get_logger(__name__).bind(repository=self.__class__.__name__)

    def initialize_schema(self):
        \"\"\"Initializes the schema for this lookup table using SchemaManager.\"\"\"
        self.schema_manager.ensure_table_exists(
            table_name=self.TABLE_NAME,
            column_definitions=list(self.COLUMNS.items()),
            primary_key=self.ID_COLUMN,
            indexes=self.INDEXES
        )
        self.log.info(\"Schema initialized\", table_name=self.TABLE_NAME)

    def _upsert_lookup_data(self, data: List[Dict[str, Any]], data_columns: List[str]) -> int:
        \"\"\"
        Generic upsert logic for lookup tables using ON CONFLICT DO UPDATE.
        Handles updating 'last_synced_at' if present.
        Returns the number of rows affected by the upsert operation.
        \"\"\"
        if not data:
            self.log.debug(\"No data provided to upsert\", table_name=self.TABLE_NAME)
            return 0

        conn = None
        rows_affected = 0
        start_time = time.monotonic()

        valid_data_columns = [col for col in data_columns if col in self.COLUMNS]
        if len(valid_data_columns) != len(data_columns):
             self.log.warning(\"Some provided data columns are not in the table schema definition\",
                              provided=data_columns, defined=list(self.COLUMNS.keys()))

        if not valid_data_columns:
             self.log.error(\"No valid columns found for upsert after schema check.\")
             return 0

        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                table_id = sql.Identifier(self.TABLE_NAME)
                col_ids = sql.SQL(', ').join(map(sql.Identifier, valid_data_columns))

                update_cols_list = [
                    sql.SQL(\"{col} = EXCLUDED.{col}\").format(col=sql.Identifier(col))
                    for col in valid_data_columns if col != self.ID_COLUMN
                ]

                if \"last_synced_at\" in self.COLUMNS:
                    update_cols_list.append(sql.SQL(\"last_synced_at = NOW()\"))

                if not update_cols_list:
                     self.log.warning(\"No columns specified for update in ON CONFLICT clause.\", table_name=self.TABLE_NAME)
                     pass

                update_assignments = sql.SQL(', ').join(update_cols_list)

                upsert_sql_template = sql.SQL(\"\"\"
                    INSERT INTO {table} ({insert_cols}) VALUES %s
                    ON CONFLICT ({pk_col}) DO UPDATE SET {update_assignments}
                \"\"\").format(
                    table=table_id,
                    insert_cols=col_ids,
                    pk_col=sql.Identifier(self.ID_COLUMN),
                    update_assignments=update_assignments if update_cols_list else sql.SQL(\"NOTHING\") 
                )

                values_tuples = []
                for d in data:
                     row = tuple(d.get(col) for col in valid_data_columns)
                     values_tuples.append(row)

                extras.execute_values(cur, upsert_sql_template.as_string(cur), values_tuples, page_size=500)
                rows_affected = cur.rowcount 
                conn.commit()
                duration = time.monotonic() - start_time
                self.log.info(
                    \"Upsert successful\", table_name=self.TABLE_NAME,
                    records_in=len(data), rows_affected=rows_affected, duration_sec=f\"{duration:.3f}s\"
                )
                return rows_affected
        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Upsert failed\", table_name=self.TABLE_NAME, error=str(e), exc_info=True)
            raise 
        finally:
            if conn:
                self.db_pool.putconn(conn)

    def get_name_map_for_ids(self, ids: Set[int]) -> Dict[int, str]:
        \"\"\"
        Fetches a mapping of ID to Name for the given set of IDs.
        Requires NAME_COLUMN to be defined in the subclass.
        \"\"\"
        if not self.NAME_COLUMN:
            self.log.warning(\"NAME_COLUMN not defined, cannot fetch name map.\", table_name=self.TABLE_NAME)
            return {}
        if not ids:
            return {}

        conn = None
        start_time = time.monotonic()
        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                query = sql.SQL(\"SELECT {id_col}, {name_col} FROM {table} WHERE {id_col} = ANY(%s)\").format(
                    id_col=sql.Identifier(self.ID_COLUMN),
                    name_col=sql.Identifier(self.NAME_COLUMN),
                    table=sql.Identifier(self.TABLE_NAME)
                )
                cur.execute(query, (list(ids),))
                results = {row[0]: str(row[1] or UNKNOWN_NAME) for row in cur.fetchall()} # Ensure name is string, handle None
                duration = time.monotonic() - start_time
                self.log.debug(\"Fetched name map\", table_name=self.TABLE_NAME, ids_count=len(ids), found_count=len(results), duration_sec=f\"{duration:.3f}s\")
                return results
        except Exception as e:
            self.log.error(\"Failed to get name map\", table_name=self.TABLE_NAME, error=str(e), exc_info=True)
            return {}
        finally:
            if conn:
                self.db_pool.putconn(conn)"}
,
{"path": "infrastructure/repository_impl/lookup_repositories.py", "encoding": "utf-8", "content": "from typing import List, Dict, Any

from .base_lookup_repository import BaseLookupRepository, UNKNOWN_NAME
from application.utils.column_utils import normalize_column_name


# --- Constants for Table Names ---
LOOKUP_TABLE_USERS = \"pipedrive_users\"
LOOKUP_TABLE_PERSONS = \"pipedrive_persons\"
LOOKUP_TABLE_STAGES = \"pipedrive_stages\"
LOOKUP_TABLE_PIPELINES = \"pipedrive_pipelines\"
LOOKUP_TABLE_ORGANIZATIONS = \"pipedrive_organizations\"


# --- User Repository ---
class UserRepository(BaseLookupRepository):
    TABLE_NAME = LOOKUP_TABLE_USERS
    ID_COLUMN = \"user_id\"
    NAME_COLUMN = \"user_name\"
    COLUMNS = {
        \"user_id\": \"INTEGER PRIMARY KEY\",
        \"user_name\": \"TEXT\",
        \"is_active\": \"BOOLEAN\",
        \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
    }
    INDEXES = {
        f\"idx_{TABLE_NAME}_name\": \"user_name text_pattern_ops\" 
    }

    def upsert_users(self, raw_api_data: List[Dict[str, Any]]):
        \"\"\"Maps raw Pipedrive User API data and upserts into the database.\"\"\"
        mapped_data = []
        cols_to_insert = [\"user_id\", \"user_name\", \"is_active\"] 
        for r in raw_api_data:
            if 'id' in r:
                mapped_data.append({
                    'user_id': r['id'],
                    'user_name': r.get('name', UNKNOWN_NAME),
                    # Check API response for correct active flag key (e.g., 'active_flag')
                    'is_active': r.get('active_flag', True)
                })
            else:
                self.log.warning(\"User record missing 'id' in raw API data\", record_preview=str(r)[:100])

        return self._upsert_lookup_data(mapped_data, cols_to_insert)


# --- Person Repository ---
class PersonRepository(BaseLookupRepository):
    TABLE_NAME = LOOKUP_TABLE_PERSONS
    ID_COLUMN = \"person_id\"
    NAME_COLUMN = \"person_name\"
    COLUMNS = {
        \"person_id\": \"INTEGER PRIMARY KEY\",
        \"person_name\": \"TEXT\",
        \"org_id\": \"INTEGER\", 
        \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
    }
    INDEXES = {
        f\"idx_{TABLE_NAME}_name\": \"person_name text_pattern_ops\",
        f\"idx_{TABLE_NAME}_org_id\": \"org_id\"
    }

    def upsert_persons(self, raw_api_data: List[Dict[str, Any]]):
        \"\"\"Maps raw Pipedrive Person API data and upserts into the database.\"\"\"
        mapped_data = []
        cols_to_insert = [\"person_id\", \"person_name\", \"org_id\"]
        for r in raw_api_data:
            if 'id' in r:
                org_id_value = None
                raw_org_id = r.get('org_id')
                if isinstance(raw_org_id, dict):
                    org_id_value = raw_org_id.get('value')
                elif isinstance(raw_org_id, int):
                     org_id_value = raw_org_id

                try:
                    org_id_int = int(org_id_value) if org_id_value is not None else None
                except (ValueError, TypeError):
                     self.log.warning(\"Could not parse org_id for person\", person_id=r['id'], raw_org_id=raw_org_id)
                     org_id_int = None

                mapped_data.append({
                    'person_id': r['id'],
                    'person_name': r.get('name', UNKNOWN_NAME),
                    'org_id': org_id_int
                })
            else:
                self.log.warning(\"Person record missing 'id' in raw API data\", record_preview=str(r)[:100])

        return self._upsert_lookup_data(mapped_data, cols_to_insert)


# --- Stage Repository ---
class StageRepository(BaseLookupRepository):
    TABLE_NAME = LOOKUP_TABLE_STAGES
    ID_COLUMN = \"stage_id\"
    NAME_COLUMN = \"stage_name\" 
    COLUMNS = {
        \"stage_id\": \"INTEGER PRIMARY KEY\",
        \"stage_name\": \"TEXT\",
        \"normalized_name\": \"TEXT\", 
        \"pipeline_id\": \"INTEGER\",
        \"order_nr\": \"INTEGER\",
        \"is_active\": \"BOOLEAN\",
        \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
    }
    INDEXES = {
        f\"idx_{TABLE_NAME}_norm_name\": \"normalized_name\",
        f\"idx_{TABLE_NAME}_pipeline_id\": \"pipeline_id\"
    }

    def upsert_stages(self, raw_api_data: List[Dict[str, Any]]):
        \"\"\"Maps raw Pipedrive Stage API data and upserts into the database.\"\"\"
        mapped_data = []
        cols_to_insert = [\"stage_id\", \"stage_name\", \"normalized_name\", \"pipeline_id\", \"order_nr\", \"is_active\"]
        for r in raw_api_data:
            if 'id' in r:
                stage_name = r.get('name', UNKNOWN_NAME)
                normalized = normalize_column_name(stage_name) if stage_name != UNKNOWN_NAME else None
                if not normalized or normalized == \"_invalid_normalized_name\":
                    self.log.warning(\"Could not normalize stage name\", stage_id=r['id'], original_name=stage_name)
                    normalized = f\"invalid_stage_{r['id']}\"

                mapped_data.append({
                    'stage_id': r['id'],
                    'stage_name': stage_name,
                    'normalized_name': normalized,
                    'pipeline_id': r.get('pipeline_id'),
                    'order_nr': r.get('order_nr'),
                    'is_active': not r.get('is_deleted', False)
                })
            else:
                self.log.warning(\"Stage record missing 'id' in raw API data\", record_preview=str(r)[:100])

        return self._upsert_lookup_data(mapped_data, cols_to_insert)


# --- Pipeline Repository ---
class PipelineRepository(BaseLookupRepository):
    TABLE_NAME = LOOKUP_TABLE_PIPELINES
    ID_COLUMN = \"pipeline_id\"
    NAME_COLUMN = \"pipeline_name\"
    COLUMNS = {
        \"pipeline_id\": \"INTEGER PRIMARY KEY\",
        \"pipeline_name\": \"TEXT\",
        \"is_active\": \"BOOLEAN\",
        \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
    }
    INDEXES = {
         f\"idx_{TABLE_NAME}_name\": \"pipeline_name text_pattern_ops\"
    }

    def upsert_pipelines(self, raw_api_data: List[Dict[str, Any]]):
        \"\"\"Maps raw Pipedrive Pipeline API data and upserts into the database.\"\"\"
        mapped_data = []
        cols_to_insert = [\"pipeline_id\", \"pipeline_name\", \"is_active\"]
        for r in raw_api_data:
            if 'id' in r:
                mapped_data.append({
                    'pipeline_id': r['id'],
                    'pipeline_name': r.get('name', UNKNOWN_NAME),
                    'is_active': not r.get('is_deleted', False)
                })
            else:
                 self.log.warning(\"Pipeline record missing 'id' in raw API data\", record_preview=str(r)[:100])

        return self._upsert_lookup_data(mapped_data, cols_to_insert)


# --- Organization Repository ---
class OrganizationRepository(BaseLookupRepository):
    TABLE_NAME = LOOKUP_TABLE_ORGANIZATIONS
    ID_COLUMN = \"org_id\"
    NAME_COLUMN = \"org_name\"
    COLUMNS = {
        \"org_id\": \"INTEGER PRIMARY KEY\",
        \"org_name\": \"TEXT\",
        \"last_synced_at\": \"TIMESTAMPTZ DEFAULT NOW()\"
        # Add other relevant org fields here if needed later (e.g., address, owner_id)
        # \"address\": \"TEXT\",
        # \"owner_user_id\": \"INTEGER\",
    }
    INDEXES = {
         f\"idx_{TABLE_NAME}_name\": \"org_name text_pattern_ops\"
    }

    def upsert_organizations(self, raw_api_data: List[Dict[str, Any]]):
        \"\"\"Maps raw Pipedrive Organization API data and upserts into the database.\"\"\"
        mapped_data = []
        cols_to_insert = [\"org_id\", \"org_name\"]
        for r in raw_api_data:
             if 'id' in r:
                 mapped_data.append({
                     'org_id': r['id'],
                     'org_name': r.get('name', UNKNOWN_NAME)
                     # Map other fields here if added to COLUMNS
                     # 'address': r.get('address'),
                     # 'owner_user_id': r.get('owner_id') # API might use 'owner_id' for user ID
                 })
             else:
                  self.log.warning(\"Organization record missing 'id' in raw API data\", record_preview=str(r)[:100])

        return self._upsert_lookup_data(mapped_data, cols_to_insert)"}
,
{"path": "infrastructure/repository_impl/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/repository_impl/pipedrive_repository.py", "encoding": "utf-8", "content": "import time
import random
import csv
import json
from io import StringIO
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, timezone, date, time as time_obj 
from decimal import Decimal

from psycopg2 import sql, pool, extras
import pandas as pd 
import numpy as np
import structlog

from application.ports.data_repository_port import DataRepositoryPort
from infrastructure.db.schema_manager import SchemaManager
from application.utils.column_utils import normalize_column_name 

log = structlog.get_logger(__name__)

BASE_COLUMNS_DEFS = {
    \"id\": \"TEXT PRIMARY KEY\", \"titulo\": \"TEXT\", \"creator_user_id\": \"INTEGER\",
    \"creator_user_name\": \"TEXT\", \"person_id\": \"INTEGER\", \"person_name\": \"TEXT\",
    \"stage_id\": \"INTEGER\", \"stage_name\": \"TEXT\", \"pipeline_id\": \"INTEGER\",
    \"pipeline_name\": \"TEXT\", \"owner_id\": \"INTEGER\", \"owner_name\": \"TEXT\",
    \"status\": \"TEXT\", \"value\": \"NUMERIC(18, 2)\", \"currency\": \"VARCHAR(10)\",
    \"add_time\": \"TIMESTAMPTZ\", \"update_time\": \"TIMESTAMPTZ\",
    \"org_id\": \"INTEGER\", \"org_name\": \"TEXT\", \"lost_reason\": \"TEXT\", \"visible_to\": \"TEXT\",
    \"close_time\": \"TIMESTAMPTZ\", \"won_time\": \"TIMESTAMPTZ\", \"lost_time\": \"TIMESTAMPTZ\",
    \"first_won_time\": \"TIMESTAMPTZ\", \"expected_close_date\": \"DATE\", \"probability\": \"NUMERIC(5,2)\",
    \"label\": \"TEXT\"
}

# Define main table indexes
MAIN_TABLE_INDEXES = {
    \"idx_pipedrive_data_update_time\": \"update_time DESC\",
    \"idx_pipedrive_data_stage_id\": \"stage_id\",
    \"idx_pipedrive_data_pipeline_id\": \"pipeline_id\",
    \"idx_pipedrive_data_person_id\": \"person_id\",
    \"idx_pipedrive_data_org_id\": \"org_id\",
    \"idx_pipedrive_data_owner_id\": \"owner_id\",
    \"idx_pipedrive_data_creator_user_id\": \"creator_user_id\",
    \"idx_pipedrive_data_status\": \"status\",
    \"idx_pipedrive_data_add_time\": \"add_time DESC\",
    \"idx_pipedrive_data_active_deals_update\": \"(update_time DESC) WHERE status NOT IN ('Ganho', 'Perdido', 'Deletado')\"
}

class PipedriveDataRepository(DataRepositoryPort):
    \"\"\"
    Repository implementation focused solely on the main 'pipedrive_data' table.
    Handles dynamic schema updates for custom fields and uses staging tables
    with COPY for efficient upserts. Leverages SchemaManager for DDL operations.
    \"\"\"
    TABLE_NAME = \"pipedrive_data\"
    STAGING_TABLE_PREFIX = \"staging_pipedrive_\"
    STAGE_HISTORY_COLUMN_PREFIX = \"moved_to_stage_\" 

    def __init__(
        self,
        db_pool: pool.SimpleConnectionPool,
        schema_manager: SchemaManager,
        all_stages_details: List[Dict]
    ):
        self.db_pool = db_pool
        self.schema_manager = schema_manager
        self.log = log.bind(repository=self.__class__.__name__)
        self._all_stages_details = all_stages_details
        self._stage_history_columns_dict = self._prepare_stage_history_columns()
        self._stage_id_to_column_name_map = self._build_stage_id_to_col_map()
        self._cached_target_column_types: Optional[Dict[str, str]] = None
        
    def _refresh_column_type_cache(self) -> bool:
         \"\"\"Fetches and updates the cached column types for the target table.\"\"\"
         self.log.debug(\"Refreshing target column type cache\", table_name=self.TABLE_NAME)
         try:
              types = self.schema_manager._get_table_column_types(self.TABLE_NAME)
              if not types:
                   self.log.error(\"Failed to fetch column types, cache not updated.\")
                   self._cached_target_column_types = None 
                   return False
              self._cached_target_column_types = types
              self.log.info(\"Target column type cache refreshed\", count=len(types))
              return True
         except Exception as e:
              self.log.error(\"Error refreshing column type cache\", error=str(e), exc_info=True)
              self._cached_target_column_types = None
              return False

    def _build_stage_id_to_col_map(self) -> Dict[int, str]:
        \"\"\"Builds the map of Stage ID -> Stage History Column Name.\"\"\"
        id_map = {}
        for col_name in self._stage_history_columns_dict.keys():
             try:
                 parts = col_name.split('_')
                 if len(parts) > 1 and parts[-1].isdigit():
                      stage_id = int(parts[-1])
                      id_map[stage_id] = col_name
             except (ValueError, IndexError):
                 self.log.warning(\"Could not parse stage_id from history column name\", column_name=col_name)
        return id_map

    def _prepare_stage_history_columns(self) -> Dict[str, str]:
        \"\"\"Generates the dictionary {history_column_name: \"TIMESTAMPTZ\"}.\"\"\"
        stage_history_cols = {}
        base_col_set = set(BASE_COLUMNS_DEFS.keys())
        processed_normalized_ids = set() 

        for stage in self._all_stages_details:
            stage_id = stage.get(\"id\")
            stage_name = stage.get(\"name\")
            if not stage_id or not stage_name: continue

            normalized = normalize_column_name(stage_name)
            if not normalized or normalized == \"_invalid_normalized_name\": continue

            final_col_name = f\"{self.STAGE_HISTORY_COLUMN_PREFIX}{normalized}_{stage_id}\"
            lookup_key = f\"{normalized}_{stage_id}\"

            if final_col_name not in base_col_set and lookup_key not in processed_normalized_ids:
                 stage_history_cols[final_col_name] = \"TIMESTAMPTZ\"
                 processed_normalized_ids.add(lookup_key)
            elif final_col_name in base_col_set:
                 self.log.error(\"Stage history column name conflicts with base column\", col_name=final_col_name)

        return stage_history_cols

    def initialize_schema(self) -> None:
        \"\"\"Initializes schema and populates the column type cache.\"\"\"
        all_columns_with_types = {**BASE_COLUMNS_DEFS, **self._stage_history_columns_dict}
        self.schema_manager.ensure_table_exists(
            table_name=self.TABLE_NAME,
            column_definitions=list(all_columns_with_types.items()),
            primary_key=\"id\",
            indexes=MAIN_TABLE_INDEXES
        )
        self._refresh_column_type_cache() 
        self.log.info(\"Schema initialized\", table_name=self.TABLE_NAME)

    def _format_value_for_csv(self, value: Any) -> str:
         \"\"\"Formats a Python value into a string suitable for COPY FROM STDIN.\"\"\"
         if value is None or value is pd.NaT or (isinstance(value, float) and np.isnan(value)):
             return '\\N'
         elif isinstance(value, bool):
             return 't' if value else 'f'
         elif isinstance(value, datetime):
             if value.tzinfo is None:
                 dt_aware = value.replace(tzinfo=timezone.utc)
             else:
                 dt_aware = value.astimezone(timezone.utc)
             return dt_aware.isoformat(timespec='microseconds')
         elif isinstance(value, (date, time_obj, Decimal)):
             return str(value)
         elif isinstance(value, (dict, list, set, tuple)):
              try:
                 json_str = json.dumps(value)
                 return json_str.replace('\\', '\\\\').replace('|', '\\|').replace('\n', '\\n').replace('\r', '\\r')
              except TypeError:
                  self.log.warning(\"Could not JSON serialize value for CSV, using str()\", value_type=type(value))
                  str_value = str(value)
         else:
             str_value = str(value)

         return str_value.replace('\\', '\\\\').replace('|', '\\|').replace('\n', '\\n').replace('\r', '\\r')


    def save_data_upsert(self, data: List[Dict[str, Any]]):
        \"\"\"Saves data using staging table, COPY, dynamic columns, and cached types.\"\"\"
        if not data:
            self.log.debug(\"No data provided to save_data_upsert, skipping.\")
            return

        start_time = time.monotonic()
        record_count = len(data)
        log_ctx = self.log.bind(record_count=record_count, target_table=self.TABLE_NAME)

        all_keys_in_batch = set()
        for record in data:
            all_keys_in_batch.update(record.keys())
        if 'id' not in all_keys_in_batch:
             log_ctx.error(\"Input data for upsert is missing the 'id' key.\")
             raise ValueError(\"Upsert data must contain the 'id' key.\")
        final_columns_for_batch = sorted(list(all_keys_in_batch))

        try:
            schema_changed = self.schema_manager.ensure_columns_exist(self.TABLE_NAME, all_keys_in_batch)
            if schema_changed or self._cached_target_column_types is None:
                if not self._refresh_column_type_cache():
                     raise RuntimeError(f\"Failed to refresh column types for {self.TABLE_NAME} after schema change.\")
        except Exception as schema_update_err:
            log_ctx.error(\"Failed operation related to ensuring columns exist\", columns=final_columns_for_batch, error=str(schema_update_err))
            raise

        target_column_types = self._cached_target_column_types
        if not target_column_types: 
             log_ctx.error(\"Target column type cache is not populated. Aborting UPSERT.\")
             raise RuntimeError(f\"Column type cache unavailable for table {self.TABLE_NAME}\")

        conn = None
        staging_table_name = f\"{self.STAGING_TABLE_PREFIX}{int(time.time())}_{random.randint(1000, 9999)}\"
        staging_table_id = sql.Identifier(staging_table_name)
        target_table_id = sql.Identifier(self.TABLE_NAME)
        log_ctx = log_ctx.bind(staging_table=staging_table_name)

        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                staging_col_defs = [sql.SQL(\"{} TEXT\").format(sql.Identifier(col)) for col in final_columns_for_batch]
                create_staging_sql = sql.SQL(\"CREATE UNLOGGED TABLE {staging_table} ({columns})\").format(
                    staging_table=staging_table_id, columns=sql.SQL(', ').join(staging_col_defs)
                )
                log_ctx.debug(\"Creating dynamic staging table\", columns=final_columns_for_batch)
                cur.execute(create_staging_sql)

                buffer = StringIO()
                copy_failed_records = 0
                try:
                    writer = csv.writer(buffer, delimiter='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\n', escapechar='\\', doublequote=False)
                    for i, record in enumerate(data):
                        row = []
                        try:
                            for field in final_columns_for_batch:
                                value = record.get(field)
                                formatted_value = self._format_value_for_csv(value)
                                row.append(formatted_value)
                            writer.writerow(row)
                        except Exception as row_err:
                            copy_failed_records += 1
                            log_ctx.error(\"Error preparing record for COPY\", record_index=i, error=str(row_err), record_id=record.get('id', 'N/A'), exc_info=True)
                    buffer.seek(0)

                    if copy_failed_records > 0:
                        log_ctx.warning(\"Some records failed CSV preparation\", failed_count=copy_failed_records)

                    copy_sql = sql.SQL(\"COPY {staging_table} ({fields}) FROM STDIN WITH (FORMAT CSV, DELIMITER '|', NULL '\\N', ENCODING 'UTF8')\").format(
                        staging_table=staging_table_id,
                        fields=sql.SQL(', ').join(map(sql.Identifier, final_columns_for_batch))
                    )
                    log_ctx.debug(\"Executing COPY to staging table.\")
                    cur.copy_expert(copy_sql, buffer)
                    copy_row_count = cur.rowcount
                    log_ctx.debug(\"COPY command executed\", copied_rows=copy_row_count, expected_rows=record_count - copy_failed_records)

                finally:
                    buffer.close()

                # 5. Execute UPSERT with dynamic columns and CASTING
                target_column_types = self.schema_manager._get_table_column_types(self.TABLE_NAME) 
                if not target_column_types:
                     log_ctx.error(\"Could not fetch target column types for casting. Aborting UPSERT.\")
                     raise RuntimeError(f\"Failed to get column types for table {self.TABLE_NAME}\")

                insert_fields = sql.SQL(', ').join(map(sql.Identifier, final_columns_for_batch))
                update_assignments_list = []
                select_expressions = []

                for col in final_columns_for_batch:
                    # Build UPDATE SET assignments
                    if col != 'id': # Don't update the primary key
                         if col == 'add_time':
                             update_assignments_list.append(
                                 sql.SQL(\"{col} = COALESCE({target}.{col}, EXCLUDED.{col})\").format(
                                     col=sql.Identifier(col), target=target_table_id
                                 )
                             )
                         else:
                             update_assignments_list.append(sql.SQL(\"{col} = EXCLUDED.{col}\").format(col=sql.Identifier(col)))

                    # Build SELECT with CASTING logic
                    target_type = target_column_types.get(col, \"TEXT\").upper()
                    base_pg_type = target_type.split('(')[0] #

                    if base_pg_type in ('INTEGER', 'BIGINT', 'NUMERIC', 'DECIMAL', 'REAL', 'DOUBLE PRECISION', 'SMALLINT'):
                         # Try to cast numeric types, handle empty strings as NULL
                         select_expressions.append(
                             sql.SQL(\"NULLIF(TRIM({col}), '')::{type}\").format(
                                 col=sql.Identifier(col), type=sql.SQL(base_pg_type)
                             )
                         )
                    elif base_pg_type in ('TIMESTAMP', 'TIMESTAMPTZ', 'DATE', 'TIME'):
                         # Try to cast date/time types, handle empty strings as NULL
                         select_expressions.append(
                             sql.SQL(\"NULLIF(TRIM({col}), '')::{type}\").format(
                                 col=sql.Identifier(col), type=sql.SQL(base_pg_type)
                             )
                         )
                    elif base_pg_type == 'BOOLEAN':
                        # Handle 't'/'f' from COPY
                        select_expressions.append(
                            sql.SQL(\"CASE WHEN TRIM(LOWER({col})) = 't' THEN TRUE WHEN TRIM(LOWER({col})) = 'f' THEN FALSE ELSE NULL END\").format(
                                col=sql.Identifier(col)
                            )
                        )
                    elif base_pg_type == 'JSONB':
                         # Try casting to JSONB, handle empty/invalid as NULL
                         select_expressions.append(
                             sql.SQL(\"(CASE WHEN TRIM({col}) = '' THEN NULL ELSE NULLIF(TRIM({col}), '')::JSONB END)\").format(
                                col=sql.Identifier(col)
                             )
                         )
                    else: # TEXT, VARCHAR, etc.
                        # Trim whitespace but keep empty strings if they are meaningful
                        select_expressions.append(sql.SQL(\"TRIM({col})\").format(col=sql.Identifier(col)))

                update_assignments = sql.SQL(', ').join(update_assignments_list)
                select_clause = sql.SQL(', ').join(select_expressions)

                upsert_sql = sql.SQL(\"\"\"
                    INSERT INTO {target_table} ({insert_fields})
                    SELECT {select_clause} FROM {staging_table}
                    ON CONFLICT (id) DO UPDATE SET {update_assignments}
                    WHERE {target_table}.update_time IS NULL OR EXCLUDED.update_time >= {target_table}.update_time
                \"\"\").format(
                    target_table=target_table_id, insert_fields=insert_fields, select_clause=select_clause,
                    staging_table=staging_table_id, update_assignments=update_assignments
                )
                log_ctx.debug(\"Executing UPSERT from staging table.\")
                cur.execute(upsert_sql)
                upserted_count = cur.rowcount
                conn.commit()
                duration = time.monotonic() - start_time
                log_ctx.info(\"Upsert completed successfully\", affected_rows=upserted_count, duration_sec=f\"{duration:.3f}s\")

        except Exception as e:
            if conn: conn.rollback()
            log_ctx.error(\"Upsert failed\", error=str(e), exc_info=True)
            raise 
        finally:
            # 6. Drop staging table and release connection
            if conn:
                try:
                    with conn.cursor() as final_cur:
                        drop_sql = sql.SQL(\"DROP TABLE IF EXISTS {staging_table}\").format(staging_table=staging_table_id)
                        log_ctx.debug(\"Dropping staging table.\")
                        final_cur.execute(drop_sql)
                    conn.commit() 
                except Exception as drop_err:
                    log_ctx.error(\"Failed to drop staging table\", error=str(drop_err))
                finally:
                     self.db_pool.putconn(conn) 


    # --- Implementa\u00e7\u00f5es dos outros m\u00e9todos da interface ---
    def get_record_by_id(self, record_id: str) -> Optional[Dict]:
        \"\"\"Fetches a single complete record by its ID.\"\"\"
        conn = None
        log_ctx = self.log.bind(deal_id=record_id)
        try:
            conn = self.db_pool.getconn()
            with conn.cursor(cursor_factory=extras.DictCursor) as cur:
                # Select all columns currently in the table dynamically
                cur.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = %s;\", (self.TABLE_NAME,))
                columns = [row[0] for row in cur.fetchall()]
                if not columns:
                     log_ctx.error(\"Could not retrieve columns for table\", table_name=self.TABLE_NAME)
                     return None

                select_cols_sql = sql.SQL(', ').join(map(sql.Identifier, columns))
                query = sql.SQL(\"SELECT {cols} FROM {table} WHERE id = %s\").format(
                    cols=select_cols_sql,
                    table=sql.Identifier(self.TABLE_NAME)
                )
                cur.execute(query, (str(record_id),)) 
                row = cur.fetchone()
                log_ctx.debug(\"Record fetched by ID\", found=row is not None)
                return dict(row) if row else None
        except Exception as e:
            log_ctx.error(\"Failed to get record by ID\", error=str(e), exc_info=True)
            return None
        finally:
            if conn: self.db_pool.putconn(conn)

    def get_all_ids(self) -> Set[str]:
        \"\"\"Returns a set of all existing deal IDs.\"\"\"
        conn = None
        ids = set()
        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                cur.execute(sql.SQL(\"SELECT id FROM {table}\").format(table=sql.Identifier(self.TABLE_NAME)))
                while True:
                    rows = cur.fetchmany(50000)
                    if not rows: break
                    ids.update(row[0] for row in rows)
                self.log.debug(\"Fetched all IDs\", count=len(ids))
                return ids
        except Exception as e:
            self.log.error(\"Failed to get all IDs\", error=str(e), exc_info=True)
            return set() 
        finally:
            if conn: self.db_pool.putconn(conn)

    def count_records(self) -> int:
        \"\"\"Counts the total number of records in the main data table.\"\"\"
        conn = None
        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                cur.execute(sql.SQL(\"SELECT COUNT(*) FROM {table}\").format(table=sql.Identifier(self.TABLE_NAME)))
                count = cur.fetchone()[0]
                self.log.debug(\"Counted records\", total=count)
                return count or 0
        except Exception as e:
            self.log.error(\"Failed to count records\", error=str(e), exc_info=True)
            return -1 
        finally:
            if conn: self.db_pool.putconn(conn)

    def get_deals_needing_history_backfill(self, limit: int) -> List[str]:
        \"\"\"Finds deal IDs potentially needing stage history backfill.\"\"\"
        conn = None
        history_cols = list(self._stage_history_columns_dict.keys())
        if not history_cols:
            self.log.warning(\"No stage history columns defined, cannot find deals for backfill.\")
            return []

        where_conditions = [sql.SQL(\"{} IS NULL\").format(sql.Identifier(col)) for col in history_cols]
        where_clause = sql.SQL(\" OR \").join(where_conditions)

        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                query = sql.SQL(\"\"\"
                    SELECT id FROM {table}
                    WHERE {conditions}
                    ORDER BY add_time ASC NULLS FIRST, id -- Stable sort order
                    LIMIT %s
                \"\"\").format(
                    table=sql.Identifier(self.TABLE_NAME),
                    conditions=where_clause
                )
                cur.execute(query, (limit,))
                deal_ids = [row[0] for row in cur.fetchall()]
                self.log.info(\"Fetched deal IDs needing history backfill\", count=len(deal_ids), limit=limit)
                return deal_ids
        except Exception as e:
            self.log.error(\"Failed to get deals for history backfill\", error=str(e), exc_info=True)
            return []
        finally:
            if conn: self.db_pool.putconn(conn)

    def update_stage_history(self, updates: List[Dict[str, Any]]) -> None:
        \"\"\"Applies stage history timestamp updates using UPDATE FROM VALUES.\"\"\"
        if not updates:
            self.log.debug(\"No stage history updates to apply.\")
            return

        conn = None
        start_time = time.monotonic()
        total_updated_count = 0
        valid_stage_columns = set(self._stage_history_columns_dict.keys())

        # Group updates by the column they affect for batching
        updates_by_column: Dict[str, List[Tuple[str, datetime]]] = {}
        for update in updates:
            deal_id = str(update.get('deal_id'))
            stage_column = update.get('stage_column')
            timestamp_val = update.get('timestamp') 

            if not deal_id or not stage_column or not isinstance(timestamp_val, datetime):
                 self.log.warning(\"Invalid data in stage history update\", update_data=update)
                 continue
            if stage_column not in valid_stage_columns:
                 self.log.warning(\"Attempted update on non-existent history column\", column=stage_column, deal_id=deal_id)
                 continue

            if stage_column not in updates_by_column:
                updates_by_column[stage_column] = []
            updates_by_column[stage_column].append((deal_id, timestamp_val))

        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                for stage_column, column_updates in updates_by_column.items():
                    if not column_updates: continue
                    column_id = sql.Identifier(stage_column)
                    table_id = sql.Identifier(self.TABLE_NAME)
                    values_tuples = [(upd[0], upd[1]) for upd in column_updates]

                    # Update only if the current value is NULL
                    update_sql = sql.SQL(\"\"\"
                        UPDATE {table} AS t
                        SET {column_to_update} = v.ts
                        FROM (VALUES %s) AS v(id, ts)
                        WHERE t.id = v.id AND t.{column_to_update} IS NULL
                    \"\"\").format(
                        table=table_id,
                        column_to_update=column_id
                    )

                    try:
                        log_ctx = self.log.bind(stage_column=stage_column, batch_size=len(values_tuples))
                        log_ctx.debug(\"Executing stage history update batch\")
                        extras.execute_values(cur, update_sql.as_string(cur), values_tuples, page_size=1000)
                        batch_updated_count = cur.rowcount
                        total_updated_count += batch_updated_count
                        log_ctx.debug(\"Stage history update batch executed\", affected_rows=batch_updated_count)
                    except Exception as exec_err:
                        self.log.error(\"Failed executing batch update for stage history\", stage_column=stage_column, error=str(exec_err), exc_info=True)
                        conn.rollback()
                        raise exec_err 

                conn.commit() 
                duration = time.monotonic() - start_time
                self.log.info(
                    \"Stage history update run completed.\",
                    total_updates_provided=len(updates),
                    total_rows_affected=total_updated_count,
                    columns_updated=list(updates_by_column.keys()),
                    duration_sec=f\"{duration:.3f}s\"
                )

        except Exception as e:
            if conn: conn.rollback() 
            self.log.error(\"Failed to update stage history\", error=str(e), total_updates=len(updates), exc_info=True)
        finally:
            if conn:
                self.db_pool.putconn(conn)

    def count_deals_needing_backfill(self) -> int:
        \"\"\"Counts how many deals potentially need backfill.\"\"\"
        conn = None
        history_cols = list(self._stage_history_columns_dict.keys())
        if not history_cols:
            self.log.warning(\"No stage history columns defined, cannot count deals for backfill.\")
            return -1

        where_conditions = [sql.SQL(\"{} IS NULL\").format(sql.Identifier(col)) for col in history_cols]
        where_clause = sql.SQL(\" OR \").join(where_conditions)

        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                query = sql.SQL(\"SELECT COUNT(*) FROM {table} WHERE {conditions}\").format(
                    table=sql.Identifier(self.TABLE_NAME),
                    conditions=where_clause
                )
                cur.execute(query)
                count = cur.fetchone()[0]
                self.log.info(\"Counted deals needing history backfill\", count=count)
                return count or 0
        except Exception as e:
            self.log.error(\"Failed to count deals for history backfill\", error=str(e), exc_info=True)
            return -1
        finally:
            if conn: self.db_pool.putconn(conn)

    def validate_date_consistency(self) -> int:
         \"\"\"Checks basic date consistency, returns number of issues.\"\"\"
         conn = None
         try:
             conn = self.db_pool.getconn()
             with conn.cursor() as cur:
                 cur.execute(sql.SQL(\"\"\"
                     SELECT COUNT(*) FROM {table}
                     WHERE add_time > NOW() -- Added in the future?
                       OR (update_time IS NOT NULL AND update_time < add_time)
                       OR (close_time IS NOT NULL AND close_time < add_time)
                       OR (won_time IS NOT NULL AND won_time < add_time)
                       OR (lost_time IS NOT NULL AND lost_time < add_time)
                 \"\"\").format(table=sql.Identifier(self.TABLE_NAME)))
                 count = cur.fetchone()[0]
                 if count > 0:
                      self.log.warning(\"Date consistency issues found\", count=count)
                 return count or 0
         except Exception as e:
             self.log.error(\"Failed to validate date consistency\", error=str(e), exc_info=True)
             return -1
         finally:
             if conn: self.db_pool.putconn(conn)"}
,
{"path": "infrastructure/db/db_pool.py", "encoding": "utf-8", "content": "import psycopg2.pool

class DBConnectionPool:
    def __init__(self, dsn: str, minconn=1, maxconn=10):
        self.pool = psycopg2.pool.SimpleConnectionPool(
            minconn=minconn,
            maxconn=maxconn,
            dsn=dsn
        )

    def get_connection(self):
        return self.pool.getconn()

    def release_connection(self, conn):
        self.pool.putconn(conn)

    def closeall(self):
        self.pool.closeall()"}
,
{"path": "infrastructure/db/schema_manager.py", "encoding": "utf-8", "content": "import time
from typing import List, Dict, Optional, Tuple, Set
from psycopg2 import sql, pool
import structlog

class SchemaManager:
    SCHEMA_LOCK_ID = 47835 

    def __init__(self, db_pool: pool.SimpleConnectionPool):
        self.db_pool = db_pool
        self.log = structlog.get_logger(__name__).bind(service=\"SchemaManager\")

    def _execute_with_lock(self, func, *args, **kwargs):
        \"\"\"Executes a function within an advisory lock.\"\"\"
        conn = None
        locked = False
        try:
            conn = self.db_pool.getconn()
            with conn.cursor() as cur:
                cur.execute(\"SELECT pg_advisory_lock(%s)\", (self.SCHEMA_LOCK_ID,))
                locked = True
                self.log.debug(\"Acquired schema lock\", lock_id=self.SCHEMA_LOCK_ID)
                result = func(cur, *args, **kwargs)
                conn.commit()
                return result
        except Exception as e:
            if conn: conn.rollback()
            self.log.error(\"Schema operation failed\", error=str(e), exc_info=True)
            raise
        finally:
            if conn:
                if locked:
                    try:
                        with conn.cursor() as unlock_cur:
                           unlock_cur.execute(\"SELECT pg_advisory_unlock(%s)\", (self.SCHEMA_LOCK_ID,))
                        conn.commit() 
                        self.log.debug(\"Released schema lock\", lock_id=self.SCHEMA_LOCK_ID)
                    except Exception as unlock_err:
                         self.log.error(\"Failed to release schema lock\", error=str(unlock_err))
                self.db_pool.putconn(conn)

    def ensure_table_exists(
        self,
        table_name: str,
        column_definitions: List[Tuple[str, str]], 
        primary_key: Optional[str] = None,
        indexes: Optional[Dict[str, str]] = None 
    ):
        \"\"\"Ensures a table exists with the specified columns, PK, and indexes.\"\"\"
        return self._execute_with_lock(
            self._ensure_table_exists_internal,
            table_name, column_definitions, primary_key, indexes
        )

    def _ensure_table_exists_internal(
        self, cur, table_name: str, column_definitions: List[Tuple[str, str]],
        primary_key: Optional[str], indexes: Optional[Dict[str, str]]
    ):
        table_id = sql.Identifier(table_name)
        exists = self._check_table_exists(cur, table_name)

        if not exists:
            self._create_table(cur, table_id, column_definitions, primary_key)
        else:
            self._add_missing_columns(cur, table_id, column_definitions)

        if indexes:
            self._create_indexes(cur, table_id, indexes)

    def ensure_columns_exist(self, table_name: str, expected_cols: Set[str]):
         \"\"\"Adds missing columns to an existing table, defaulting to TEXT type.\"\"\"
         # This simplified version just ensures existence, defaulting type
         # A more robust version might take Dict[str, str] with types
         return self._execute_with_lock(self._ensure_columns_exist_internal, table_name, expected_cols)

    def _ensure_columns_exist_internal(self, cur, table_name: str, expected_cols: Set[str]) -> bool:
         table_id = sql.Identifier(table_name)
         existing_cols = self._get_existing_columns(cur, table_name)
         missing_cols = expected_cols - existing_cols
         schema_changed = False
         if missing_cols:
             schema_changed = self._add_missing_columns(cur, table_id, [(col, \"TEXT\") for col in missing_cols])
         return schema_changed 

    def _check_table_exists(self, cur, table_name: str) -> bool:
        cur.execute(\"\"\"
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = 'public' AND table_name = %s
            );
        \"\"\", (table_name,))
        return cur.fetchone()[0]

    def _get_existing_columns(self, cur, table_name: str) -> Set[str]:
        cur.execute(\"\"\"
            SELECT column_name FROM information_schema.columns
            WHERE table_schema = 'public' AND table_name = %s;
        \"\"\", (table_name,))
        return {row[0] for row in cur.fetchall()}

    def _get_table_column_types(self, table_name: str) -> Dict[str, str]:
        \"\"\"Helper to get column types for casting during UPSERT.\"\"\"
        conn = None
        try:
             conn = self.db_pool.getconn()
             with conn.cursor() as cur:
                 cur.execute(\"\"\"
                     SELECT column_name, data_type
                     FROM information_schema.columns
                     WHERE table_schema = 'public' AND table_name = %s;
                 \"\"\", (table_name,))
                 return {row[0]: row[1].upper() for row in cur.fetchall()}
        except Exception as e:
            self.log.error(\"Failed to get column types\", table_name=table_name, error=str(e))
            return {}
        finally:
             if conn: self.db_pool.putconn(conn)


    def _create_table(self, cur, table_id: sql.Identifier, column_definitions: List[Tuple[str, str]], primary_key: Optional[str]):
        \"\"\"Creates the table.\"\"\"
        col_defs_sql = [
            sql.SQL(\"{} {}\").format(sql.Identifier(name), sql.SQL(ctype))
            for name, ctype in column_definitions
        ]
        if primary_key:
             found = False
             for i, (name, ctype) in enumerate(column_definitions):
                 if name == primary_key:
                     col_defs_sql[i] = sql.SQL(\"{} {} PRIMARY KEY\").format(sql.Identifier(name), sql.SQL(ctype.replace(\" PRIMARY KEY\", \"\")))
                     found = True
                     break
             if not found: 
                col_defs_sql.append(sql.SQL(\"PRIMARY KEY ({})\").format(sql.Identifier(primary_key)))

        query = sql.SQL(\"CREATE TABLE {table} ({columns})\").format(
            table=table_id, columns=sql.SQL(', ').join(col_defs_sql)
        )
        self.log.info(\"Creating table\", table_name=table_id.strings[0])
        cur.execute(query)

    def _add_missing_columns(self, cur, table_id: sql.Identifier, columns_to_add: List[Tuple[str, str]]) -> bool:
        \"\"\"Adds columns if they don't exist. Returns True if any columns were added.\"\"\"
        if not columns_to_add:
            return False

        existing_cols = self._get_existing_columns(cur, table_id.strings[0])
        missing_cols_final = [(name, ctype) for name, ctype in columns_to_add if name not in existing_cols]

        if not missing_cols_final:
             return False

        alter_statements = [
            sql.SQL(\"ADD COLUMN IF NOT EXISTS {} {}\").format(sql.Identifier(name), sql.SQL(ctype))
            for name, ctype in missing_cols_final
        ]
        if alter_statements:
            query = sql.SQL(\"ALTER TABLE {table} \").format(table=table_id) + sql.SQL(', ').join(alter_statements)
            self.log.info(\"Adding missing columns\", table_name=table_id.strings[0], columns=[name for name, _ in missing_cols_final])
            cur.execute(query)
            return True
        return False

    def _create_indexes(self, cur, table_id: sql.Identifier, indexes: Dict[str, str]):
        \"\"\"Creates indexes if they don't exist.\"\"\"
        for idx_name, idx_definition in indexes.items():
            cur.execute(\"SELECT EXISTS (SELECT FROM pg_class WHERE relname = %s);\", (idx_name,))
            if not cur.fetchone()[0]:
                 query = sql.SQL(\"CREATE INDEX IF NOT EXISTS {} ON {} ({})\").format(
                     sql.Identifier(idx_name), table_id, sql.SQL(idx_definition)
                 )
                 self.log.info(\"Creating index\", index_name=idx_name, table_name=table_id.strings[0])
                 cur.execute(query)"}
,
{"path": "infrastructure/db/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/cache.py", "encoding": "utf-8", "content": "import json
import redis
from datetime import timedelta

class RedisCache:
    def __init__(self, connection_string: str):
        self.client = redis.Redis.from_url(
            connection_string,
            decode_responses=True
        )

    def get(self, key):
        value = self.client.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return value
        return None

    def set(self, key, value, ex_seconds=3600):
        if isinstance(value, (dict, list)):
            value = json.dumps(value)
        self.client.setex(
            name=key,
            time=timedelta(seconds=ex_seconds),
            value=value
        )

    def delete(self, key):
        self.client.delete(key)"}
,
{"path": "infrastructure/prefect/orion/Dockerfile", "encoding": "utf-8", "content": "FROM prefecthq/prefect:3.2.14-python3.12
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

CMD [\"prefect\", \"server\", \"start\", \"--host\", \"0.0.0.0\"]
"}
,
{"path": "infrastructure/k8s/pipedrive_metabase_integration.yaml", "encoding": "utf-8", "content": "apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-orion
  labels:
    app: prefect-orion
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prefect-orion
  template:
    metadata:
      labels:
        app: prefect-orion
    spec:
      containers:
      - name: prefect-orion
        image: pipedrive_metabase_integration-prefect-orion:latest 
        imagePullPolicy: Never 
        ports:
        - containerPort: 4200
          name: http 
        readinessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5
        env:
        - name: APP_ROLE
          value: \"orion\"
        envFrom:
        - configMapRef:
            name: observability-config
        resources:
          requests:
            memory: \"1Gi\"
            cpu: \"500m\"
          limits:
            memory: \"1.5Gi\"
            cpu: \"1.5\"
---
apiVersion: v1
kind: Service
metadata:
  name: prefect-orion
spec:
  selector:
    app: prefect-orion
  ports:
  - protocol: TCP
    port: 4200
    targetPort: http 
  type: ClusterIP 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics
  labels:
    app: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics
  template:
    metadata:
      labels:
        app: metrics
      annotations: 
         prometheus.io/scrape: 'true'
         prometheus.io/port: '8082' 
         prometheus.io/path: '/metrics' 
    spec:
      containers:
      - name: metrics
        envFrom:
        - secretRef:
            name: app-secrets
        - secretRef:
            name: db-secrets
        image: pipedrive_metabase_integration-etl:latest 
        imagePullPolicy: Never
        ports:
        - containerPort: 8082
          name: metrics-port
        readinessProbe:
          httpGet:
            path: /metrics
            port: metrics-port
          initialDelaySeconds: 5
          periodSeconds: 15
        livenessProbe:
          tcpSocket: 
            port: metrics-port
          initialDelaySeconds: 30
          periodSeconds: 30
        resources:
          requests:
            memory: \"64Mi\"
            cpu: \"50m\"
          limits:
            memory: \"128Mi\"
            cpu: \"100m\"
        env:
          - name: APP_ROLE
            value: \"metrics\"
          - name: APP_METRICS_PORT 
            value: \"8082\"
---
apiVersion: v1
kind: Service
metadata:
  name: metrics
spec:
  selector:
    app: metrics
  ports:
  - protocol: TCP
    port: 8082
    targetPort: metrics-port 
  type: ClusterIP 
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine 
        ports:
        - containerPort: 6379
          name: redis-port 
        readinessProbe:
          tcpSocket:
            port: redis-port
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          exec:
            command: [\"redis-cli\", \"ping\"] 
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
        resources:
          requests:
            memory: \"128Mi\"
            cpu: \"100m\"
          limits:
            memory: \"512Mi\"
            cpu: \"500m\"
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: redis-port
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: db
  labels:
    app: db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
      - name: db
        image: postgres:14-alpine 
        ports:
        - containerPort: 5432
          name: pgsql 
        envFrom:
          - secretRef:
              name: db-secrets
        volumeMounts:
        - name: pgdata
          mountPath: /var/lib/postgresql/data
        readinessProbe:
          tcpSocket:
              port: pgsql 
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        livenessProbe:
          tcpSocket:
            port: pgsql 
          initialDelaySeconds: 60
          periodSeconds: 30
        resources:
          requests:
            memory: \"1Gi\"
            cpu: \"500m\"
          limits:
            memory: \"1.5Gi\"
            cpu: \"1.5\"
      volumes:
      - name: pgdata
        persistentVolumeClaim:
          claimName: pgdata-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  selector:
    app: db
  ports:
  - protocol: TCP
    port: 5432
    targetPort: pgsql 
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana-oss:latest 
        ports:
        - containerPort: 3015
          name: http
        env:
        - name: GF_SERVER_HTTP_PORT
          value: \"3015\"
        readinessProbe:
          httpGet:
            path: /api/health
            port: http 
          initialDelaySeconds: 10
        livenessProbe:
          httpGet:
            path: /api/health
            port: http 
          initialDelaySeconds: 60 
          periodSeconds: 30
        resources:
          requests:
            memory: \"256Mi\"
            cpu: \"100m\"
          limits:
            memory: \"1Gi\"
            cpu: \"500m\"

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  selector:
    app: grafana
  ports:
  - protocol: TCP
    port: 3015
    targetPort: http 
  type: ClusterIP 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-agent
  labels:
    app: prefect-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prefect-agent
  template:
    metadata:
      labels:
        app: prefect-agent
    spec:
      containers:
      - name: agent
        image: pipedrive_metabase_integration-etl:latest
        imagePullPolicy: Never 
        command: [\"prefect\", \"worker\", \"start\", \"--pool\", \"kubernetes-pool\"]
        envFrom: 
        - secretRef:
            name: app-secrets
        - secretRef:
            name: db-secrets
        env:
          - name: PREFECT_API_URL
            value: \"http://prefect-orion:4200/api\"
        resources:
          requests:
            memory: \"512Mi\"
            cpu: \"200m\"
          limits:
            memory: \"1Gi\"
            cpu: \"500m\"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metabase
  labels:
    app: metabase
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metabase
  template:
    metadata:
      labels:
        app: metabase
    spec:
      containers:
      - name: metabase
        image: metabase/metabase:latest 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 3000
          name: http 
        readinessProbe:
          httpGet:
            path: /api/health # Endpoint de health do Metabase
            port: http
          initialDelaySeconds: 45 # Metabase pode demorar um pouco mais para carregar JVM/etc
          periodSeconds: 20
          timeoutSeconds: 10
          failureThreshold: 6 
        livenessProbe:
          httpGet:
            path: /api/health
            port: http
          initialDelaySeconds: 180
          periodSeconds: 30
          failureThreshold: 5
        resources:
          requests:
            memory: \"1.5Gi\"
            cpu: \"500m\"
          limits:
            memory: \"2Gi\"
            cpu: \"1.5\" 
---
apiVersion: v1
kind: Service
metadata:
  name: metabase
spec:
  selector:
    app: metabase
  ports:
  - protocol: TCP
    port: 3000       
    targetPort: http 
  type: ClusterIP"}
,
{"path": "infrastructure/k8s/observability-config.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: ConfigMap
metadata:
  name: observability-config
data:
  LOG_LEVEL: \"INFO\"
  METRICS_ENDPOINT: \"/metrics\" "}
,
{"path": "infrastructure/k8s/prometheus.yml", "encoding": "utf-8", "content": "---
# --- 1. ConfigMap com prometheus.yml ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s # Intervalo padr\u00e3o de coleta
      evaluation_interval: 15s

    scrape_configs:
      # Coleta m\u00e9tricas do pr\u00f3prio Prometheus
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      - job_name: 'pushgateway'
        honor_labels: true # Importante para manter labels do job que fez o push
        static_configs:
          - targets: ['pushgateway:9091']

      # Coleta m\u00e9tricas de servi\u00e7os Kubernetes que possuem anota\u00e7\u00f5es espec\u00edficas
      - job_name: 'kubernetes-services'
        kubernetes_sd_configs:
          - role: service # Descobre servi\u00e7os
        relabel_configs:
          # Seleciona apenas servi\u00e7os com a anota\u00e7\u00e3o prometheus.io/scrape=true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          # (Opcional) Usa a anota\u00e7\u00e3o prometheus.io/path se definida, sen\u00e3o usa /metrics
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Usa a anota\u00e7\u00e3o prometheus.io/port se definida, sen\u00e3o usa a porta do servi\u00e7o
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          # Define o label 'job' como o nome do servi\u00e7o
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: job
          # Define o label 'instance' como o IP:Porta do pod/endpoint
          # (Pode ser mais \u00fatil mudar role para 'pod' ou 'endpoints' para ter labels de pod)
          # Para simplificar, vamos manter o servi\u00e7o por enquanto.
          # - source_labels: [__address__]
          #   action: replace
          #   target_label: instance

---
# --- 2. RBAC: Service Account, ClusterRole, ClusterRoleBinding ---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-sa
  labels:
    app: prometheus

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-cr
rules:
- apiGroups: [\"\"]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: [\"get\", \"list\", \"watch\"]
- apiGroups: [\"extensions\", \"networking.k8s.io\"]
  resources:
  - ingresses
  verbs: [\"get\", \"list\", \"watch\"]
- nonResourceURLs: [\"/metrics\"]
  verbs: [\"get\"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-crb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-cr
subjects:
- kind: ServiceAccount
  name: prometheus-sa
  namespace: default

---
# --- 3. PersistentVolumeClaim ---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  labels:
    app: prometheus
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi 
      memory: \"1Gi\"  
      cpu: \"500m\"   
    limits:
      memory: \"2Gi\"  
      cpu: \"1\"     

---
# --- 4. Deployment do Prometheus Server ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus-sa
      containers:
        - name: prometheus
          image: prom/prometheus:latest 
          args:
            - \"--config.file=/etc/prometheus/prometheus.yml\"
            - \"--storage.tsdb.path=/prometheus/\"
            - \"--web.console.libraries=/usr/share/prometheus/console_libraries\"
            - \"--web.console.templates=/usr/share/prometheus/consoles\"
            - \"--web.enable-lifecycle\"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: config-volume
              mountPath: /etc/prometheus/
            - name: storage-volume
              mountPath: /prometheus/
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-pvc 

---
# --- 5. Service para expor o Prometheus internamente ---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service 
  labels:
    app: prometheus
spec:
  selector:
    app: prometheus 
  ports:
    - protocol: TCP
      port: 9090      
      targetPort: 9090  
  type: ClusterIP "}
,
{"path": "infrastructure/k8s/pushgateway.yaml", "encoding": "utf-8", "content": "apiVersion: apps/v1
kind: Deployment
metadata:
  name: pushgateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pushgateway
  template:
    metadata:
      labels:
        app: pushgateway
    spec:
      containers:
      - name: pushgateway
        image: prom/pushgateway:latest 
        ports:
        - containerPort: 9091
---
apiVersion: v1
kind: Service
metadata:
  name: pushgateway
  labels:
    app: pushgateway
  annotations:
     prometheus.io/scrape: 'true'
     prometheus.io/port: '9091'
spec:
  selector:
    app: pushgateway
  ports:
  - protocol: TCP
    port: 9091
    targetPort: 9091"}
,
{"path": "infrastructure/k8s/persistent-volume-claim.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pgdata-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
"}
,
{"path": "infrastructure/k8s/prefect.yaml", "encoding": "utf-8", "content": "# yaml-language-server: $schema=https://raw.githubusercontent.com/PrefectHQ/prefect/main/src/prefect/server/schemas/core/deployment.json
prefect-version: 3.3.2

name: pipedrive-etl

deployments:
# --- Main ETL Flow ---
- name: Pipedrive Sync
  version: '2.0'
  tags:
  - pipedrive
  - sync
  - etl
  - main
  description: Sincroniza deals recentes do Pipedrive com o banco de dados, usando
    lookups no DB.
  entrypoint: flows/pipedrive_metabase_etl.py:main_etl_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  concurrency_limit: 1
  schedules:
  - interval: 1800.0
    anchor_date: '2025-04-10T12:00:00+00:00'
    timezone: America/Sao_Paulo
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
- name: Pipedrive Backfill Stage History
  version: '1.0'
  tags:
  - pipedrive
  - backfill
  - history
  description: Preenche o hist\u00f3rico de stages para deals antigos.
  entrypoint: flows/pipedrive_metabase_etl.py:backfill_stage_history_flow
  parameters:
    daily_deal_limit: 10000
    db_batch_size: 1000
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  schedules: []
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Batch Size Experiment
  version: '1.0'
  tags:
  - experiment
  - batch-size
  - optimization
  description: Testa diferentes tamanhos de batch, calcula e salva o \u00f3timo na config.
  entrypoint: flows/pipedrive_metabase_etl.py:batch_size_experiment_flow
  parameters:
    batch_sizes:
    - 300
    - 500
    - 750
    - 1000
    - 1500
    test_data_size: 10000
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/experiment-k8s-job
  schedules: []
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Users
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - users
  description: Sincroniza a tabela pipedrive_users com a API.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_users_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/light-sync-k8s-job
  schedules:
  - cron: 0 3 * * *
    timezone: America/Sao_Paulo
    day_or: true
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Persons and Orgs
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - persons
  - orgs
  description: Sincroniza as tabelas pipedrive_persons e pipedrive_organizations.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_persons_orgs_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/default-k8s-job
  schedules:
  - interval: 14400.0
    anchor_date: '2025-04-10T12:00:00+00:00'
    timezone: America/Sao_Paulo
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
- name: Sync Pipedrive Stages and Pipelines
  version: '1.0'
  tags:
  - pipedrive
  - sync
  - aux
  - stages
  - pipelines
  description: Sincroniza as tabelas pipedrive_stages e pipedrive_pipelines.
  entrypoint: flows/pipedrive_sync_aux.py:sync_pipedrive_stages_pipelines_flow
  parameters: {}
  work_pool:
    name: kubernetes-pool
    work_queue_name: kubernetes
    job_variables: {}
  infrastructure:
    type: kubernetes-job
    block: kubernetes-job/light-sync-k8s-job
  schedules:
  - cron: 0 4 * * *
    timezone: America/Sao_Paulo
    day_or: true
    active: true
  pull:
  - prefect.deployments.steps.git_clone:
      repository: https://github.com/MrSchrodingers/pipedrive_metabase_integration.git
      branch: v2_implementation
      access_token: '{{ prefect.blocks.secret.github-access-token }}'
  concurrency_limit: 1
"}
,
{"path": "infrastructure/k8s/wait-for-it.sh", "encoding": "utf-8", "content": "#!/usr/bin/env bash
host=\"$1\"
port=\"$2\"
shift 2
cmd=\"$@\"

echo \"Aguardando Orion em http://$host:$port/api/health...\"
until curl -s \"http://$host:$port/api/health\" >/dev/null 2>&1; do
  echo \"Ainda aguardando...\"
  sleep 1
done

echo \"Orion est\u00e1 dispon\u00edvel. Executando comando: $cmd\"
exec $cmd
"}
,
{"path": "infrastructure/k8s/db-secrets.yaml", "encoding": "utf-8", "content": "apiVersion: v1
kind: Secret
metadata:
  name: db-secrets
type: Opaque
data:
  POSTGRES_USER: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
  POSTGRES_PASSWORD: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
  POSTGRES_DB: cGlwZWRyaXZlX21ldGFiYXNlX2ludGVncmF0aW9uX2Ri
"}
,
{"path": "infrastructure/k8s/entrypoint.sh", "encoding": "utf-8", "content": "#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'

##############################
# Configura\u00e7\u00f5es
##############################
declare -A APP_PORTS=(
    [\"orion\"]=\"4200\"
    [\"metrics\"]=\"8082\"
)

##############################
# Fun\u00e7\u00f5es Auxiliares
##############################
log() {
    local LEVEL=\"$1\"
    local MESSAGE=\"$2\"
    printf \"[%s] [%s] %s\n\" \"$(date '+%Y-%m-%d %H:%M:%S')\" \"${LEVEL^^}\" \"${MESSAGE}\"
}

validate_env() {
    local REQUIRED_ENV=(\"POSTGRES_USER\" \"POSTGRES_PASSWORD\" \"PIPEDRIVE_API_KEY\")
    
    for var in \"${REQUIRED_ENV[@]}\"; do
        if [[ -z \"${!var:-}\" ]]; then
            log \"error\" \"Vari\u00e1vel de ambiente obrigat\u00f3ria n\u00e3o definida: $var\"
            exit 1
        fi
    done
}

start_server() {
    local APP=\"$1\"
    shift
    
    log \"info\" \"Iniciando $APP...\"
    exec \"$@\"
}

##############################
# Fluxo Principal
##############################
cd /app

case \"${APP_ROLE:-}\" in
    etl)
        validate_env

        # Executar fluxo ETL
        log \"info\" \"Iniciando fluxo ETL (depend\u00eancias esperadas via Init Containers)...\"
        poetry run python -u flows/pipedrive_metabase_etl.py
        ;;
    metrics)
        start_server \"metrics server\" \
            python -m infrastructure.monitoring.metrics_server
        ;;
    orion)
        start_server \"Prefect Orion\" \
            prefect orion start \
                --host 0.0.0.0 \
                --port \"${APP_PORTS[orion]}\" \
                --log-level WARNING
        ;;
    *)
        log \"error\" \"APP_ROLE inv\u00e1lido ou n\u00e3o definido. Valores permitidos: etl, metrics, orion\"
        exit 1
        ;;
esac"}
,
{"path": "infrastructure/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/api_clients/pipedrive_api_client.py", "encoding": "utf-8", "content": "import time
import requests
import structlog
from tenacity import RetryError, retry, retry_if_exception, stop_after_attempt, wait_exponential, retry_if_exception_type
from pybreaker import CircuitBreaker
from typing import Dict, List, Optional, Generator, Any, Set, Tuple

from infrastructure.config.settings import settings
from infrastructure.cache import RedisCache
from application.ports.pipedrive_client_port import PipedriveClientPort
from application.utils.column_utils import normalize_column_name 
from infrastructure.monitoring.metrics import (
    api_request_duration_hist,
    api_errors_counter,
    pipedrive_api_token_cost_total,
    pipedrive_api_call_total,
    pipedrive_api_cache_hit_total,
    pipedrive_api_rate_limit_remaining
)


log = structlog.get_logger(__name__)
api_breaker = CircuitBreaker(fail_max=3, reset_timeout=60)

class PipedriveAPIClient(PipedriveClientPort):
    BASE_URL_V1 = \"https://api.pipedrive.com/v1\"
    BASE_URL_V2 = \"https://api.pipedrive.com/api/v2\"
    DEFAULT_TIMEOUT = 45
    DEFAULT_V2_LIMIT = 500
    MAX_V1_PAGINATION_LIMIT = 500
    CHANGELOG_PAGE_LIMIT = 500
    DEFAULT_MAP_CACHE_TTL_SECONDS = 300 * 12
    STAGE_DETAILS_CACHE_TTL_SECONDS = 300 * 6
    DEAL_FIELDS_CACHE_TTL_SECONDS = 86400 # Cache deal fields for 24h

    ENDPOINT_COSTS = {
        '/deals/detail/changelog': 20,
        '/dealFields': 20,
        '/users': 20,
        '/users/detail': 5,
        '/deals': 10,
        '/stages': 5,
        '/pipelines': 5,
        '/persons/detail': 1,
        '/persons': 10,
        '/organizations': 10
    }
    DEFAULT_ENDPOINT_COST = 10
    UNKNOWN_NAME = \"Desconhecido\"

    def __init__(self, cache: RedisCache):
        self.api_key = settings.PIPEDRIVE_API_KEY
        if not self.api_key:
            log.error(\"PIPEDRIVE_API_KEY is not set!\")
            raise ValueError(\"Pipedrive API Key is required.\")

        self.session = requests.Session()
        self.session.headers.update({\"Accept\": \"application/json\"})
        self.cache = cache
        self.log = log.bind(client=\"PipedriveAPIClient\")
    
    def _normalize_endpoint_for_metrics(self, url_path: str) -> str:
        \"\"\"Normaliza o path da URL para usar como label na m\u00e9trica, tratando IDs etc.\"\"\"
        base_url_v1_len = len(self.BASE_URL_V1)
        base_url_v2_len = len(self.BASE_URL_V2)
        path = url_path

        if url_path.startswith(self.BASE_URL_V1):
            path = url_path[base_url_v1_len:]
        elif url_path.startswith(self.BASE_URL_V2):
            path = url_path[base_url_v2_len:]

        path = path.split('?')[0].strip('/')
        parts = path.split('/')

        if not parts or not parts[0]:
            return \"/\"

        resource = parts[0]
        normalized_path = f\"/{resource}\"

        # /resource/{id} -> /resource/detail
        if len(parts) > 1 and parts[1].isdigit():
            normalized_path += \"/detail\"
            # /resource/{id}/subresource -> /resource/detail/subresource
            if len(parts) > 2:
                # (exemplo: /deals/detail/changelog)
                known_subresources = [
                    'changelog', 'followers', 'activities', 'files',
                    'mailMessages', 'participants', 'products'
                ]
                if parts[2] in known_subresources:
                    normalized_path += f\"/{parts[2]}\"
                # /resource/{id}/products/{product_id} => /resource/detail/products/detail
                elif len(parts) > 3 and parts[3].isdigit():
                    normalized_path += f\"/{parts[2]}/detail\"

        # /resource/subresource
        elif len(parts) > 1:
            known_actions = ['search', 'summary', 'timeline', 'collection', 'products', 'installments']
            if parts[1] in known_actions:
                normalized_path += f\"/{parts[1]}\"

        self.log.debug(\"Normalized endpoint\", original=url_path, normalized=normalized_path)
        return normalized_path
    
    @api_breaker
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=(
            retry_if_exception_type(requests.exceptions.Timeout) |
            retry_if_exception_type(requests.exceptions.ConnectionError) |
            retry_if_exception_type(requests.exceptions.ChunkedEncodingError) |
            retry_if_exception(lambda e: isinstance(e, requests.exceptions.HTTPError)
                               and getattr(e.response, 'status_code', None) >= 500) |
            retry_if_exception(lambda e: isinstance(e, requests.exceptions.HTTPError)
                               and getattr(e.response, 'status_code', None) == 429)
        ),
        reraise=True
    )
    def _get(self, url: str, params: Optional[Dict[str, Any]] = None) -> requests.Response:
        \"\"\"
        \u00danico m\u00e9todo que realmente faz uma requisi\u00e7\u00e3o GET ao Pipedrive.
        - S\u00f3 aqui incrementamos o custo de tokens.
        - Se algo vier do cache, n\u00e3o passar\u00e1 por aqui.
        \"\"\"
        effective_params = params.copy() if params else {}
        if \"api_token\" not in effective_params:
            effective_params[\"api_token\"] = self.api_key

        start_time = time.monotonic()
        error_type = \"success\"
        status_code = None
        response = None
        log_params = {k: v for k, v in effective_params.items() if k != 'api_token'}

        raw_endpoint_path = url
        normalized_endpoint_label = self._normalize_endpoint_for_metrics(raw_endpoint_path)
        request_log = self.log.bind(endpoint=normalized_endpoint_label, method='GET', params=log_params)

        try:
            request_log.debug(\"Making API GET request\", url=url)
            response = self.session.get(url, params=effective_params, timeout=self.DEFAULT_TIMEOUT)
            status_code = response.status_code
            pipedrive_api_call_total.labels(
                endpoint=normalized_endpoint_label,
                method='GET',
                status_code=str(status_code)
            ).inc()

            # Rate limit handling
            if status_code == 429:
                retry_after = response.headers.get(\"Retry-After\")
                request_log.warning(\"Rate limit hit (429)\", retry_after=retry_after)

            if not response.ok:
                error_type = f\"http_{status_code}\"
                try:
                    response.raise_for_status()
                except requests.exceptions.HTTPError as e:
                    snippet = e.response.text[:200] if e.response else \"N/A\"
                    log_method = request_log.error if status_code >= 500 else request_log.warning
                    log_method(\"API request failed with HTTP error\", 
                               status_code=status_code, 
                               response_text=snippet, 
                               error=str(e))
                    raise e

            # --- Se chegou aqui, \u00e9 2xx (ok) ou 3xx sem raise_for_status ---
            cost = self.ENDPOINT_COSTS.get(normalized_endpoint_label, self.DEFAULT_ENDPOINT_COST)
            request_log.debug(\"Incrementing API token cost\", cost=cost, endpoint=normalized_endpoint_label)
            pipedrive_api_token_cost_total.labels(endpoint=normalized_endpoint_label).inc(cost)
            remaining = response.headers.get('X-RateLimit-Remaining')
            if remaining and remaining.isdigit():
                pipedrive_api_rate_limit_remaining.labels(endpoint=normalized_endpoint_label).set(int(remaining))


            duration = time.monotonic() - start_time
            api_request_duration_hist.labels(
                endpoint=normalized_endpoint_label, 
                method='GET', 
                status_code=status_code
            ).observe(duration)

            request_log.debug(\"API GET request successful\", status_code=status_code, duration_sec=f\"{duration:.3f}s\")
            return response

        except requests.exceptions.Timeout as e:
            error_type = \"timeout\"
            request_log.warning(\"API request timed out\", error=str(e))
            raise
        except requests.exceptions.ConnectionError as e:
            error_type = \"connection_error\"
            request_log.warning(\"API connection error\", error=str(e))
            raise
        except requests.exceptions.RequestException as e:
            error_type = \"request_exception\"
            request_log.error(\"API request failed (RequestException)\", exc_info=True)

        # Caso de exce\u00e7\u00e3o
        current_status_code = status_code
        if hasattr(e, 'response') and e.response is not None:
            current_status_code = e.response.status_code
        elif isinstance(e, RetryError) and hasattr(e.cause, 'response') and e.cause.response is not None:
            current_status_code = e.cause.response.status_code

        final_status_code_label = str(current_status_code) if current_status_code else 'N/A'
        api_errors_counter.labels(endpoint=normalized_endpoint_label, 
                                  error_type=error_type, 
                                  status_code=final_status_code_label).inc()
        request_log.debug(\"API Error counter incremented\", 
                          error_type=error_type, 
                          status_code=final_status_code_label)
        raise

    def _fetch_paginated_v1(self, url: str, params: Optional[Dict[str, Any]] = None) -> List[Dict]:
        \"\"\"Helper para buscar todos os itens de um endpoint V1 paginado (start/limit).\"\"\"
        all_data = []; start = 0
        base_params = params or {}; base_params[\"limit\"] = self.MAX_V1_PAGINATION_LIMIT
        endpoint_name = url.split(self.BASE_URL_V1)[-1] if self.BASE_URL_V1 in url else url
        page_num = 0

        while True:
            page_num += 1
            current_params = base_params.copy(); current_params[\"start\"] = start
            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, start=start, limit=current_params[\"limit\"])
            page_log.debug(\"Fetching V1 page\")
            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"API response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.debug(\"No more V1 data found on this page.\"); break
                all_data.extend(current_data); page_log.debug(f\"Fetched {len(current_data)} V1 items for this page.\")
                pagination_info = json_response.get(\"additional_data\", {}).get(\"pagination\", {}); more_items = pagination_info.get(\"more_items_in_collection\", False)
                if more_items:
                    next_start = pagination_info.get(\"next_start\")
                    if next_start is not None: start = next_start
                    else: page_log.warning(\"API indicates more items but no 'next_start'. Stopping.\", pagination=pagination_info); break
                else: page_log.debug(\"API indicates no more items in collection.\"); break
            except Exception as e: page_log.error(\"Error during V1 fetching page\", exc_info=True); break 
        self.log.info(f\"V1 Paginated fetch complete.\", endpoint=endpoint_name, total_items=len(all_data), total_pages=page_num)
        return all_data

    def _fetch_paginated_v2(self, url: str, params: Optional[Dict[str, Any]] = None) -> List[Dict]:
        \"\"\"Helper para buscar todos os itens de um endpoint V2 paginado (cursor).\"\"\"
        all_data = []; next_cursor: Optional[str] = None
        base_params = params or {}; base_params[\"limit\"] = self.DEFAULT_V2_LIMIT
        normalized_endpoint_label = self._normalize_endpoint_for_metrics(url)
        page_num = 0

        while True:
            page_num += 1
            current_params = base_params.copy()
            if next_cursor: current_params[\"cursor\"] = next_cursor
            elif \"cursor\" in current_params: del current_params[\"cursor\"]
            page_log = self.log.bind(endpoint=normalized_endpoint_label, page=page_num, limit=current_params[\"limit\"], cursor=next_cursor)
            page_log.debug(\"Fetching V2 page\")
            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"API response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.debug(\"No more V2 data found on this page.\"); break
                all_data.extend(current_data); page_log.debug(f\"Fetched {len(current_data)} V2 items for this page.\")
                additional_data = json_response.get(\"additional_data\", {}); next_cursor = additional_data.get(\"next_cursor\") 
                if not next_cursor: page_log.debug(\"No 'next_cursor' found. Ending pagination.\"); break
            except Exception as e: page_log.error(\"Error during V2 fetching page\", exc_info=True); break 
        self.log.info(f\"V2 Paginated fetch complete.\", endpoint=normalized_endpoint_label, total_items=len(all_data), total_pages=page_num)
        return all_data

    # --- M\u00e9todos de busca de mapas ---
    def fetch_all_users_map(self) -> Dict[int, str]:
        cache_key = \"pipedrive:all_users_map\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"users\", source=\"redis\").inc()
        if cached and isinstance(cached, dict): self.log.info(\"Users map retrieved from cache.\", cache_hit=True, map_size=len(cached)); return cached
        self.log.info(\"Fetching users map from API (V1).\", cache_hit=False)
        url = f\"{self.BASE_URL_V1}/users\"
        try:
            all_users = self._fetch_paginated_v1(url)
            user_map = {user['id']: user.get('name', self.UNKNOWN_NAME)
                        for user in all_users if user and 'id' in user}
            if user_map: self.cache.set(cache_key, user_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS); self.log.info(\"Users map fetched and cached.\", map_size=len(user_map))
            else: self.log.warning(\"Fetched user list was empty or malformed.\")
            return user_map
        except Exception as e: self.log.error(\"Failed to fetch/process users map\", exc_info=True); return {}

    def fetch_all_pipelines_map(self) -> Dict[int, str]:
        cache_key = \"pipedrive:all_pipelines_map\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"pipelines\", source=\"redis\").inc()
        if cached and isinstance(cached, dict): self.log.info(\"Pipelines map retrieved from cache.\", cache_hit=True, map_size=len(cached)); return cached
        self.log.info(\"Fetching pipelines map from API (V2).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/pipelines\"
        try:
            all_pipelines = self._fetch_paginated_v2(url)
            pipeline_map = {p['id']: p.get('name', self.UNKNOWN_NAME)
                            for p in all_pipelines if p and 'id' in p}
            if pipeline_map: self.cache.set(cache_key, pipeline_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS); self.log.info(\"Pipelines map fetched and cached.\", map_size=len(pipeline_map))
            else: self.log.warning(\"Fetched pipeline list was empty or malformed.\")
            return pipeline_map
        except Exception as e: self.log.error(\"Failed to fetch/process pipelines map\", exc_info=True); return {}

    def fetch_all_persons_map(self) -> Dict[int, str]:
        \"\"\"
        Busca todos os persons usando pagina\u00e7\u00e3o V2 via stream
        e armazena em cache de forma eficiente em mem\u00f3ria.
        \"\"\"
        cache_key = \"pipedrive:all_persons_map\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"persons\", source=\"redis\").inc()
        if cached and isinstance(cached, dict):
            self.log.info(\"Persons map retrieved from cache.\", cache_hit=True, map_size=len(cached))
            return cached

        self.log.info(\"Fetching persons map from API (V2 - streamed).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/persons\"
        person_map: Dict[int, str] = {}
        total_persons_processed = 0

        try:
            person_stream = self._fetch_paginated_v2_stream(url)

            for person in person_stream:
                total_persons_processed += 1
                person_id = person.get('id')
                person_name = person.get('name', '').strip() 
                if person_id and person_name:
                    person_map[person_id] = person_name
                elif person_id:
                    self.log.debug(\"Person found with empty name, skipping map entry.\", person_id=person_id)

            if person_map:
                self.cache.set(cache_key, person_map, ex_seconds=self.DEFAULT_MAP_CACHE_TTL_SECONDS)
                self.log.info(\"Persons map fetched via stream and cached.\", map_size=len(person_map), total_persons_processed=total_persons_processed)
            else:
                self.log.warning(\"Fetched person stream resulted in an empty map (no valid names found?).\", total_persons_processed=total_persons_processed)

            return person_map

        except Exception as e:
            self.log.error(\"Failed to fetch/process persons map via stream\", exc_info=True)
            return {}
        
    def fetch_all_stages_details(self) -> List[Dict]:
        \"\"\"Busca detalhes de todos os stages (necess\u00e1rio para nomes normalizados).\"\"\"
        cache_key = \"pipedrive:all_stages_details\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"stages\", source=\"redis\").inc()
        if cached and isinstance(cached, list):
            self.log.info(\"Stage details retrieved from cache.\", cache_hit=True, count=len(cached))
            return cached

        self.log.info(\"Fetching stage details from API (V2).\", cache_hit=False)
        url = f\"{self.BASE_URL_V2}/stages\"
        try:
            all_stages = self._fetch_paginated_v2(url)
            if all_stages:
                self.cache.set(cache_key, all_stages, ex_seconds=self.STAGE_DETAILS_CACHE_TTL_SECONDS)
                self.log.info(\"Stage details fetched and cached.\", count=len(all_stages))
            else:
                self.log.warning(\"Fetched stage list was empty.\")
            return all_stages
        except Exception as e:
            self.log.error(\"Failed to fetch/process stage details\", exc_info=True)
            return []
        
    def fetch_deal_fields_mapping(self) -> Dict[str, str]:
        cache_key = \"pipedrive:deal_fields_mapping\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"deal_fields\", source=\"redis_map\").inc()
        if cached and isinstance(cached, dict):
            self.log.info(\"Deal fields mapping retrieved from cache.\", cache_hit=True, map_size=len(cached))
            return cached

        self.log.info(\"Fetching deal fields definitions to build mapping (V1).\", cache_hit=False)
        try:
            # Fetch full definitions first (uses cache internally)
            all_fields_data = self.fetch_deal_fields()
            if not all_fields_data:
                self.log.warning(\"Received no data for deal fields when building mapping.\")
                return {}

            # Define standard fields to exclude from custom mapping
            non_custom_keys = {
                 \"id\", \"creator_user_id\", \"person_id\", \"org_id\",
                 \"stage_id\", \"pipeline_id\", \"title\", \"value\", \"currency\", \"add_time\",
                 \"update_time\", \"status\", \"lost_reason\", \"visible_to\", \"close_time\",
                 \"won_time\", \"lost_time\", \"first_won_time\", \"products_count\",
                 \"files_count\", \"notes_count\", \"followers_count\", \"email_messages_count\",
                 \"activities_count\", \"done_activities_count\", \"undone_activities_count\",
                 \"participants_count\", \"expected_close_date\", \"probability\",
                 \"next_activity_date\", \"next_activity_time\", \"next_activity_id\",
                 \"last_activity_id\", \"last_activity_date\", \"stage_change_time\",
                 \"last_incoming_mail_time\", \"last_outgoing_mail_time\",
                 \"label\", \"stage_order_nr\", \"person_name\", \"org_name\", \"next_activity_subject\",
                 \"next_activity_type\", \"next_activity_duration\", \"next_activity_note\",
                 \"formatted_value\", \"weighted_value\", \"formatted_weighted_value\",
                 \"weighted_value_currency\", \"rotten_time\", \"owner_name\", \"cc_email\"
            }
            mapping = {}
            for field in all_fields_data:
                api_key = field.get(\"key\")
                name = field.get(\"name\")
                if api_key and name and api_key not in non_custom_keys:
                    normalized = normalize_column_name(name)
                    if normalized and normalized != \"_invalid_normalized_name\":
                         if normalized in mapping.values():
                             self.log.warning(\"Normalized custom field name collision detected.\",
                                              conflicting_api_key=api_key, conflicting_name=name,
                                              normalized_name=normalized)
                         mapping[api_key] = normalized
                    else:
                         self.log.warning(\"Failed to normalize custom field name.\", api_key=api_key, original_name=name)

            # Cache the generated mapping
            self.cache.set(cache_key, mapping, ex_seconds=self.DEAL_FIELDS_CACHE_TTL_SECONDS)
            self.log.info(\"Deal fields mapping built and cached.\", custom_mapping_count=len(mapping))
            return mapping
        except Exception as e:
            self.log.error(\"Failed to fetch and process deal fields mapping\", exc_info=True)
            return {}

    def fetch_deal_fields(self) -> List[Dict]:
        cache_key = \"pipedrive:deal_fields_definitions\"
        cached = self.cache.get(cache_key)
        pipedrive_api_cache_hit_total.labels(entity=\"deal_fields\", source=\"redis_defs\").inc()
        if cached and isinstance(cached, list):
            self.log.info(\"Deal fields definitions retrieved from cache.\", cache_hit=True, count=len(cached))
            return cached

        self.log.info(\"Fetching deal fields definitions from API (V1).\", cache_hit=False)
        url = f\"{self.BASE_URL_V1}/dealFields\"
        try:
            all_fields_data = self._fetch_paginated_v1(url)
            if all_fields_data:
                self.cache.set(cache_key, all_fields_data, ex_seconds=self.DEAL_FIELDS_CACHE_TTL_SECONDS)
                self.log.info(\"Deal fields definitions fetched and cached.\", count=len(all_fields_data))
            else:
                self.log.warning(\"Received no data for deal fields definitions from API.\")
            return all_fields_data or []
        except Exception as e:
            self.log.error(\"Failed to fetch deal fields definitions\", exc_info=True)
            return []


    def get_last_timestamp(self) -> str | None:
        cache_key = \"pipedrive:last_update_timestamp\"
        timestamp = self.cache.get(cache_key)
        if timestamp and isinstance(timestamp, str):
             self.log.debug(\"Last timestamp retrieved from cache\", timestamp=timestamp)
             return timestamp
        self.log.info(\"No last update timestamp found in cache.\"); return None 

    def _fetch_paginated_v2_stream(self, url: str, params: Optional[Dict[str, Any]] = None) -> Generator[Dict, None, None]:
        \"\"\"Helper generator para buscar itens V2 um por um via cursor.\"\"\"
        next_cursor: Optional[str] = None
        base_params = params or {}; base_params[\"limit\"] = self.DEFAULT_V2_LIMIT
        endpoint_name = url.split(self.BASE_URL_V2)[-1] if self.BASE_URL_V2 in url else url
        endpoint_name = endpoint_name.split('?')[0].strip('/') 
        items_yielded = 0; page_num = 0
        log_every_n_pages = 50

        while True:
            page_num += 1
            current_params = base_params.copy()
            if next_cursor: current_params[\"cursor\"] = next_cursor
            elif \"cursor\" in current_params: del current_params[\"cursor\"]

            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, limit=current_params[\"limit\"], cursor=next_cursor)

            if page_num == 1 or page_num % log_every_n_pages == 0:
                 page_log.info(\"Fetching V2 page for stream\", items_yielded_so_far=items_yielded)
            else:
                 page_log.debug(\"Fetching V2 page for stream\", items_yielded_so_far=items_yielded)


            try:
                response = self._get(url, params=current_params); json_response = response.json()
                if not json_response or not json_response.get(\"success\"): page_log.warning(\"V2 API stream response indicates failure or empty data\", response_preview=str(json_response)[:200]); break
                current_data = json_response.get(\"data\", [])
                if not current_data: page_log.info(\"No more V2 stream data found on this page.\"); break

                for item in current_data: items_yielded += 1; yield item

                additional_data = json_response.get(\"additional_data\", {}); next_cursor = additional_data.get(\"next_cursor\")
                if not next_cursor: page_log.info(\"No 'next_cursor' found. Ending pagination stream.\"); break
            except Exception as e: page_log.error(\"Error during V2 stream fetching page, stopping stream.\", exc_info=True); break

        self.log.info(f\"V2 Stream fetch complete.\", endpoint=endpoint_name, total_items_yielded=items_yielded, total_pages=page_num)
        
    def _fetch_paginated_v1_stream_adapter(self, url: str, params: Optional[Dict[str, Any]] = None) -> Generator[Dict, None, None]:
        \"\"\"
        Gera itens de endpoints V1 paginados usando start/limit.
        \u00datil para consumir dados sem carregar tudo na mem\u00f3ria.
        \"\"\"
        start = 0
        base_params = params.copy() if params else {}
        base_params[\"limit\"] = self.MAX_V1_PAGINATION_LIMIT
        endpoint_name = url.split(self.BASE_URL_V1)[-1] if self.BASE_URL_V1 in url else url
        page_num = 0
        items_yielded = 0

        while True:
            page_num += 1
            current_params = base_params.copy()
            current_params[\"start\"] = start

            page_log = self.log.bind(endpoint=endpoint_name, page=page_num, start=start, limit=current_params[\"limit\"])

            try:
                response = self._get(url, params=current_params)
                json_response = response.json()

                if not json_response or not json_response.get(\"success\"):
                    page_log.warning(\"V1 stream response indicates failure or empty data\", response_preview=str(json_response)[:200])
                    break

                current_data = json_response.get(\"data\", [])
                if not current_data:
                    page_log.info(\"No more V1 stream data found.\")
                    break

                for item in current_data:
                    items_yielded += 1
                    yield item

                pagination_info = json_response.get(\"additional_data\", {}).get(\"pagination\", {})
                more_items = pagination_info.get(\"more_items_in_collection\", False)
                if more_items:
                    next_start = pagination_info.get(\"next_start\")
                    if next_start is not None:
                        start = next_start
                    else:
                        page_log.warning(\"Missing 'next_start' despite 'more_items_in_collection' being true.\")
                        break
                else:
                    page_log.info(\"Pagination completed via stream.\")
                    break

            except Exception as e:
                page_log.error(\"Error during V1 stream pagination\", exc_info=True)
                break

        self.log.info(\"V1 stream fetch completed.\", endpoint=endpoint_name, items_yielded=items_yielded, total_pages=page_num)

    def fetch_all_deals_stream(self, updated_since: str = None, items_limit: int = None) -> Generator[Dict, None, None]:
        \"\"\"Busca deals (V2) com limite opcional usando pagina\u00e7\u00e3o por cursor.\"\"\"
        url = f\"{self.BASE_URL_V2}/deals\"
        params = {\"sort_by\": \"update_time\", \"sort_direction\": \"asc\"}
        if updated_since:
            params[\"updated_since\"] = updated_since
        if items_limit:
            params[\"limit\"] = min(items_limit, self.DEFAULT_V2_LIMIT)
        
        count = 0
        for deal in self._fetch_paginated_v2_stream(url, params):
            if items_limit and count >= items_limit:
                break
            count += 1
            yield from self._fetch_paginated_v2_stream(url, params=params)


    def update_last_timestamp(self, new_timestamp: str):
        \"\"\"Armazena o \u00faltimo timestamp processado no cache.\"\"\"
        cache_key = \"pipedrive:last_update_timestamp\"; cache_ttl_seconds = 2592000 # 30 dias
        try: self.cache.set(cache_key, new_timestamp, ex_seconds=cache_ttl_seconds); self.log.info(\"Updated last update timestamp in cache\", timestamp=new_timestamp)
        except Exception as e: self.log.error(\"Failed to store last update timestamp in cache\", timestamp=new_timestamp, exc_info=True)
        
    def fetch_deal_changelog(self, deal_id: int) -> List[Dict]:
        url = f\"{self.BASE_URL_V1}/deals/{deal_id}/changelog\"
        self.log.debug(\"Fetching deal changelog from API (V1)\", deal_id=deal_id)
        try:
            return self._fetch_paginated_v1(url, params={\"limit\": self.CHANGELOG_PAGE_LIMIT})
        except Exception as e:
            self.log.error(\"Failed to fetch deal changelog\", deal_id=deal_id, error=str(e), exc_info=True)
            return []
            
    def fetch_all_users(self) -> List[Dict]:
        url = f\"{self.BASE_URL_V1}/users\"
        self.log.info(\"Fetching all users from API (V1).\")
        try:
            return self._fetch_paginated_v1(url)
        except Exception as e:
            self.log.error(\"Failed to fetch all users\", exc_info=True)
            return []

    def fetch_all_persons(self) -> List[Dict]:
        url = f\"{self.BASE_URL_V2}/persons\"
        self.log.info(\"Fetching all persons from API (V2).\")
        try:
            # Usar _fetch_paginated_v2 que retorna lista completa
            return self._fetch_paginated_v2(url)
        except Exception as e:
            self.log.error(\"Failed to fetch all persons\", exc_info=True)
            return []

    def fetch_all_stages_details(self) -> List[Dict]:
        # Reutiliza implementa\u00e7\u00e3o existente que j\u00e1 usa cache e paginated_v2
        cache_key = \"pipedrive:all_stages_details\"
        cached = self.cache.get(cache_key)
        if cached and isinstance(cached, list):
             self.log.info(\"Stage details retrieved from cache.\", count=len(cached))
             return cached
        self.log.info(\"Fetching stage details from API (V2).\")
        url = f\"{self.BASE_URL_V2}/stages\"
        try:
            all_stages = self._fetch_paginated_v2(url)
            if all_stages:
                 self.cache.set(cache_key, all_stages, ex_seconds=self.STAGE_DETAILS_CACHE_TTL_SECONDS)
                 self.log.info(\"Stage details fetched and cached.\", count=len(all_stages))
            else:
                 self.log.warning(\"Fetched stage list was empty.\")
            return all_stages or []
        except Exception as e:
            self.log.error(\"Failed to fetch stage details\", exc_info=True)
            return []

    def fetch_all_pipelines(self) -> List[Dict]:
        url = f\"{self.BASE_URL_V2}/pipelines\"
        self.log.info(\"Fetching all pipelines from API (V2).\")
        try:
            return self._fetch_paginated_v2(url)
        except Exception as e:
            self.log.error(\"Failed to fetch all pipelines\", exc_info=True)
            return []

    def fetch_all_organizations(self) -> List[Dict]:
        url = f\"{self.BASE_URL_V2}/organizations\"
        self.log.info(\"Fetching all organizations from API (V2).\")
        try:
             return self._fetch_paginated_v2(url)
        except Exception as e:
            self.log.error(\"Failed to fetch all organizations\", exc_info=True)
            return []"}
,
{"path": "infrastructure/api_clients/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "infrastructure/logging_config.py", "encoding": "utf-8", "content": "import logging
import sys
import structlog
import os 


def setup_logging(level=logging.INFO, force_json=False):
    \"\"\"
    Configura\u00e7\u00e3o robusta e testada para structlog + logging padr\u00e3o
    \"\"\"
    # 1. Processadores comuns para todos os loggers
    common_processors = [
        structlog.contextvars.merge_contextvars,
        structlog.threadlocal.merge_threadlocal,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt=\"iso\", utc=True),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
    ]

    # 2. Escolher renderizador final
    if not force_json and sys.stdout.isatty():
        renderer = structlog.dev.ConsoleRenderer(colors=True)
    else:
        renderer = structlog.processors.JSONRenderer()

    # 3. Configura\u00e7\u00e3o completa do structlog
    structlog.configure(
        processors=common_processors + [renderer],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # 4. Configurar logging padr\u00e3o para usar structlog
    formatter = structlog.stdlib.ProcessorFormatter(
        processor=renderer,
        foreign_pre_chain=common_processors,
    )

    handler = logging.StreamHandler()
    handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.handlers = [handler]
    root_logger.setLevel(level)

    # 5. Configurar n\u00edveis para bibliotecas ruidosas
    for lib in [\"httpx\", \"httpcore\", \"urllib3\"]:
        logging.getLogger(lib).setLevel(logging.WARNING)

    # Log inicial
    logger = structlog.get_logger(__name__)
    logger.info(\"Logging configurado com sucesso\", level=level)"}
,
{"path": ".gitignore", "encoding": "utf-8", "content": "# Ignorar todos os diret\u00f3rios __pycache__
**/__pycache__/
/__pycache/

# Ignorar arquivos espec\u00edficos
.qodo
.env

db-secrets.yaml
kubectl*

"}
,
{"path": "flows/pipedrive_sync_aux.py", "encoding": "utf-8", "content": "from typing import Any, Generator, List, Dict, Optional, Tuple
import structlog
from prefect import flow, task, get_run_logger

from application.ports.pipedrive_client_port import PipedriveClientPort
from infrastructure.api_clients.pipedrive_api_client import PipedriveAPIClient 
from infrastructure.repository_impl.lookup_repositories import (
    UserRepository, PersonRepository, StageRepository, PipelineRepository, OrganizationRepository
)


from flows.pipedrive_metabase_etl import initialize_components

from infrastructure.monitoring.metrics import (
    push_metrics_to_gateway,
    sync_records_upserted_total,
    sync_runs_total,             
    sync_run_failures_total    
)

log = structlog.get_logger(__name__)

# --- Tasks Gen\u00e9ricas de Sincroniza\u00e7\u00e3o ---
@task(name=\"Sync Pipedrive Entity Task\", retries=2, retry_delay_seconds=60, cache_policy=None)
def sync_entity_task(
    entity_type: str,
    client: PipedriveClientPort,
    repository: (UserRepository | PersonRepository | StageRepository | PipelineRepository | OrganizationRepository)
    ) -> int:
    \"\"\"
    Task gen\u00e9rica para buscar dados de uma entidade Pipedrive via stream
    e fazer upsert no reposit\u00f3rio de lookup correspondente.
    \"\"\"
    logger = get_run_logger()
    logger.info(f\"Starting sync task for entity: {entity_type}\")
    sync_runs_total.labels(entity_type=entity_type).inc()
    total_synced = 0
    processed_count = 0
    batch_size = 500 
    data_batch: List[Dict] = []
    stream: Optional[Generator[Dict, None, None]] = None
    upsert_method: Optional[callable] = None

    if entity_type == \"users\" and isinstance(repository, UserRepository):
        stream = client._fetch_paginated_v1_stream_adapter(f\"{client.BASE_URL_V1}/users\")
        upsert_method = repository.upsert_users
    elif entity_type == \"persons\" and isinstance(repository, PersonRepository):
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/persons\")
        upsert_method = repository.upsert_persons
    elif entity_type == \"stages\" and isinstance(repository, StageRepository):
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/stages\")
        upsert_method = repository.upsert_stages
    elif entity_type == \"pipelines\" and isinstance(repository, PipelineRepository):
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/pipelines\")
        upsert_method = repository.upsert_pipelines
    elif entity_type == \"organizations\" and isinstance(repository, OrganizationRepository):
        stream = client._fetch_paginated_v2_stream(f\"{client.BASE_URL_V2}/organizations\")
        upsert_method = repository.upsert_organizations
    else:
        logger.error(f\"Unknown entity type or mismatched repository for sync task: {entity_type}, repo_type={type(repository).__name__}\")
        raise ValueError(f\"Unknown entity type or repository mismatch: {entity_type}\")

    if not stream or not upsert_method:
         logger.error(f\"Stream or upsert method could not be determined for entity: {entity_type}\")
         raise NotImplementedError(f\"Sync logic error for {entity_type}\")

    upserted_in_last_batch = 0

    try:
        for item in stream:
            data_batch.append(item)
            processed_count += 1
            if len(data_batch) >= batch_size:
                logger.debug(f\"Upserting batch of {len(data_batch)} {entity_type}...\")
                upserted_in_last_batch = upsert_method(data_batch)
                total_synced += upserted_in_last_batch
                sync_records_upserted_total.labels(entity_type=entity_type).inc(upserted_in_last_batch)
                data_batch = []
                upserted_in_last_batch = 0

        if data_batch:
            logger.debug(f\"Upserting final batch of {len(data_batch)} {entity_type}...\")
            upserted_in_last_batch = upsert_method(data_batch)
            total_synced += upserted_in_last_batch
            sync_records_upserted_total.labels(entity_type=entity_type).inc(upserted_in_last_batch)

        logger.info(f\"Sync task completed for {entity_type}\", total_processed=processed_count, total_upserted=total_synced)
        return total_synced

    except Exception as e:
        logger.error(f\"Sync task failed for {entity_type}\", exc_info=True)
        sync_run_failures_total.labels(entity_type=entity_type).inc()
        raise e

# --- Fluxos Espec\u00edficos de Sincroniza\u00e7\u00e3o ---

def _initialize_and_get_repo(entity_type: str, logger: structlog.BoundLoggerBase) -> Tuple[PipedriveAPIClient, Any]:
    logger.info(f\"Initializing components for {entity_type} sync using main initialize_components...\")
    try:
        components: Tuple = initialize_components()

        # Mapeia o entity_type para o \u00edndice correto na tupla retornada por initialize_components
        # (client, data_repo, etl_service, user_repo, person_repo, stage_repo, pipeline_repo, org_repo, config_repo)
        # \u00cdndices:   0        1          2            3          4            5           6             7           8
        repo_map = {
            \"users\": 3,
            \"persons\": 4,
            \"stages\": 5,
            \"pipelines\": 6,
            \"organizations\": 7
        }
        repo_index = repo_map.get(entity_type)

        if repo_index is None:
            raise ValueError(f\"Entity type '{entity_type}' not mapped to a known repository index.\")

        client = components[0]      
        repository = components[repo_index]

        logger.info(f\"Components initialized for {entity_type}. Using repository: {type(repository).__name__}\")
        return client, repository
    except Exception as init_err:
        logger.critical(f\"Failed to initialize components for {entity_type}\", error=str(init_err), exc_info=True)
        raise

@flow(name=\"Sync Pipedrive Users\", log_prints=True)
def sync_pipedrive_users_flow():
    logger = get_run_logger()
    entity_type = \"users\"
    logger.info(f\"Starting Pipedrive {entity_type} sync flow...\")
    flow_run_id = getattr(logger, \"extra\", {}).get(\"flow_run_id\", f\"local_sync_{entity_type}\")
    try:
        client, repository = _initialize_and_get_repo(entity_type, logger)
        sync_entity_task(entity_type=entity_type, client=client, repository=repository)
        logger.info(f\"Pipedrive {entity_type} sync flow finished successfully.\")
    except Exception as e:
         logger.critical(f\"Pipedrive {entity_type} sync flow failed at the flow level. Check previous logs.\")
    finally:
        logger.info(f\"Pushing metrics to Pushgateway for {entity_type} sync flow.\")
        push_metrics_to_gateway(job_name=f\"pipedrive_sync_{entity_type}\", grouping_key={'flow_run_id': str(flow_run_id)})


@flow(name=\"Sync Pipedrive Persons and Orgs\", log_prints=True)
def sync_pipedrive_persons_orgs_flow():
    logger = get_run_logger()
    logger.info(\"Starting Pipedrive Persons & Orgs sync flow...\")
    flow_run_id = get_run_logger().extra.get(\"flow_run_id\", \"local_sync_persons_orgs\")
    success = True 
    try:
        # Persons
        entity_type_person = \"persons\"
        logger.info(f\"--- Starting {entity_type_person} sync part ---\")
        try:
            client_p, repo_p = _initialize_and_get_repo(entity_type_person, logger)
            sync_entity_task(entity_type=entity_type_person, client=client_p, repository=repo_p)
            logger.info(f\"--- Finished {entity_type_person} sync part successfully ---\")
        except Exception as e_p:
            logger.error(f\"--- Failed {entity_type_person} sync part ---\", error=str(e_p))
            success = False 

        # Organizations
        entity_type_org = \"organizations\"
        logger.info(f\"--- Starting {entity_type_org} sync part ---\")
        try:
            client_o, repo_o = _initialize_and_get_repo(entity_type_org, logger)
            sync_entity_task(entity_type=entity_type_org, client=client_o, repository=repo_o)
            logger.info(f\"--- Finished {entity_type_org} sync part successfully ---\")
        except Exception as e_o:
            logger.error(f\"--- Failed {entity_type_org} sync part ---\", error=str(e_o))
            success = False 

        if success:
             logger.info(\"Pipedrive Persons & Orgs sync flow finished successfully.\")
        else:
             logger.warning(\"Pipedrive Persons & Orgs sync flow finished with one or more errors.\")

    except Exception as e: 
         logger.critical(\"Pipedrive Persons & Orgs sync flow failed unexpectedly at flow level.\", exc_info=True)
         success = False
    finally:
        logger.info(\"Pushing metrics to Pushgateway for Persons & Orgs sync flow.\")
        push_metrics_to_gateway(job_name=\"pipedrive_sync_persons_orgs\", grouping_key={'flow_run_id': str(flow_run_id)})


@flow(name=\"Sync Pipedrive Stages and Pipelines\", log_prints=True)
def sync_pipedrive_stages_pipelines_flow():
    logger = get_run_logger()
    logger.info(\"Starting Pipedrive Stages & Pipelines sync flow...\")
    flow_run_id = get_run_logger().extra.get(\"flow_run_id\", \"local_sync_stages_pipelines\")
    success = True 
    try:
        # Stages
        entity_type_stage = \"stages\"
        logger.info(f\"--- Starting {entity_type_stage} sync part ---\")
        try:
            client_s, repo_s = _initialize_and_get_repo(entity_type_stage, logger)
            sync_entity_task(entity_type=entity_type_stage, client=client_s, repository=repo_s)
            logger.info(f\"--- Finished {entity_type_stage} sync part successfully ---\")
        except Exception as e_s:
             logger.error(f\"--- Failed {entity_type_stage} sync part ---\", error=str(e_s))
             success = False

        # Pipelines
        entity_type_pipe = \"pipelines\"
        logger.info(f\"--- Starting {entity_type_pipe} sync part ---\")
        try:
            client_pl, repo_pl = _initialize_and_get_repo(entity_type_pipe, logger)
            sync_entity_task(entity_type=entity_type_pipe, client=client_pl, repository=repo_pl)
            logger.info(f\"--- Finished {entity_type_pipe} sync part successfully ---\")
        except Exception as e_pl:
             logger.error(f\"--- Failed {entity_type_pipe} sync part ---\", error=str(e_pl))
             success = False

        if success:
            logger.info(\"Pipedrive Stages & Pipelines sync flow finished successfully.\")
        else:
            logger.warning(\"Pipedrive Stages & Pipelines sync flow finished with one or more errors.\")

    except Exception as e: 
         logger.critical(\"Pipedrive Stages & Pipelines sync flow failed unexpectedly at flow level.\", exc_info=True)
         success = False
    finally:
        logger.info(\"Pushing metrics to Pushgateway for Stages & Pipelines sync flow.\")
        push_metrics_to_gateway(job_name=\"pipedrive_sync_stages_pipelines\", grouping_key={'flow_run_id': str(flow_run_id)})"}
,
{"path": "flows/pipedrive_metabase_etl.py", "encoding": "utf-8", "content": "import datetime
import time
import tracemalloc
from typing import Any, Dict, List, Tuple
import pandas as pd
from prefect import flow, get_run_logger, task, context
from prefect.blocks.system import JSON
import psutil # Importa\u00e7\u00e3o de psutil est\u00e1 correta

from application.services.etl_service import ETLService
from infrastructure.api_clients.pipedrive_api_client import PipedriveAPIClient
from infrastructure.cache import RedisCache
from infrastructure.db.db_pool import DBConnectionPool
from infrastructure.db.schema_manager import SchemaManager
from infrastructure.repository_impl.lookup_repositories import (
    UserRepository, PersonRepository, StageRepository, PipelineRepository, OrganizationRepository
)
from infrastructure.repository_impl.config_repository import ConfigRepository
from application.mappers import deal_mapper

# Corre\u00e7\u00e3o: Removidas importa\u00e7\u00f5es de m\u00e9tricas n\u00e3o definidas/gen\u00e9ricas
from infrastructure.monitoring.metrics import (
    push_metrics_to_gateway,
    # etl_counter, <--- Removido
    # etl_failure_counter, <--- Removido
    backfill_deals_remaining_gauge,
    etl_heartbeat,
    etl_process_memory_mbytes,
    etl_process_cpu_percent,
    etl_process_thread_count,
    etl_run_failures_total, # M\u00e9trica espec\u00edfica para falhas
    etl_runs_total # M\u00e9trica espec\u00edfica para execu\u00e7\u00f5es
)
from infrastructure.repository_impl.pipedrive_repository import PipedriveDataRepository

DEFAULT_MAIN_FLOW_TIMEOUT = 9000
DEFAULT_BACKFILL_FLOW_TIMEOUT = 10800
BACKFILL_DAILY_LIMIT = 2000
BACKFILL_DB_BATCH_SIZE = 1000
DEFAULT_TASK_RETRIES = 3
DEFAULT_TASK_RETRY_DELAY = 60
DEFAULT_OPTIMAL_BATCH_SIZE = 1000

@task(name=\"Initialize ETL Components\", retries=2, retry_delay_seconds=30)
def initialize_components() -> Tuple[
    PipedriveAPIClient, PipedriveDataRepository, ETLService,
    UserRepository, PersonRepository, StageRepository, PipelineRepository, OrganizationRepository,
    ConfigRepository
]:
    task_log = get_run_logger()
    task_log.info(\"Initializing ETL components...\")

    try:
        postgres_config = JSON.load(\"postgres-pool\").value
        redis_config = JSON.load(\"redis-cache\").value
    except Exception as block_err:
        task_log.error(\"Failed to load Prefect Blocks (JSON/Secret)\", error=str(block_err))
        raise RuntimeError(\"Missing required Prefect Blocks (postgres-pool, redis-cache)\") from block_err

    db_pool = DBConnectionPool(
        minconn=postgres_config.get(\"minconn\", 1),
        maxconn=postgres_config.get(\"maxconn\", 10),
        dsn=postgres_config[\"dsn\"]
    )
    redis_cache = RedisCache(connection_string=redis_config[\"connection_string\"])
    pipedrive_client = PipedriveAPIClient(cache=redis_cache)

    schema_manager = SchemaManager(db_pool=db_pool)

    all_stages = pipedrive_client.fetch_all_stages_details()
    if not all_stages:
         task_log.warning(\"Fetched stage details list is empty during initialization!\")

    data_repo = PipedriveDataRepository(db_pool, schema_manager, all_stages)
    user_repo = UserRepository(db_pool, schema_manager)
    person_repo = PersonRepository(db_pool, schema_manager)
    stage_repo = StageRepository(db_pool, schema_manager)
    pipeline_repo = PipelineRepository(db_pool, schema_manager)
    org_repo = OrganizationRepository(db_pool, schema_manager)
    config_repo = ConfigRepository(db_pool, schema_manager)

    try:
        task_log.info(\"Initializing database schemas...\")
        data_repo.initialize_schema()
        user_repo.initialize_schema()
        person_repo.initialize_schema()
        stage_repo.initialize_schema()
        pipeline_repo.initialize_schema()
        org_repo.initialize_schema()
        config_repo.initialize_schema()
        task_log.info(\"Database schemas initialized.\")
    except Exception as schema_err:
         task_log.error(\"Failed during schema initialization\", error=str(schema_err), exc_info=True)
         raise RuntimeError(\"Database schema initialization failed\") from schema_err

    optimal_batch_size = DEFAULT_OPTIMAL_BATCH_SIZE
    try:
        config_val = config_repo.get_configuration(\"optimal_batch_size\")
        if config_val and isinstance(config_val.get(\"value\"), int) and config_val[\"value\"] > 0:
             optimal_batch_size = config_val[\"value\"]
             task_log.info(f\"Using optimal batch size from config: {optimal_batch_size}\")
        else:
             task_log.warning(f\"Optimal batch size not found/invalid in config, using default: {optimal_batch_size}\")
    except Exception as config_err:
         task_log.warning(f\"Failed to read optimal batch size config, using default: {optimal_batch_size}\", error=str(config_err))


    etl_service = ETLService(
        client=pipedrive_client,
        data_repository=data_repo,
        user_repository=user_repo,
        person_repository=person_repo,
        stage_repository=stage_repo,
        pipeline_repository=pipeline_repo,
        org_repository=org_repo,
        mapper_module=deal_mapper,
        batch_size=optimal_batch_size
    )

    task_log.info(\"ETL components initialized successfully.\")
    return pipedrive_client, data_repo, etl_service, user_repo, person_repo, stage_repo, pipeline_repo, org_repo, config_repo

@flow(
    name=\"Pipedrive to Database ETL Flow (Main Sync)\",
    log_prints=True,
    timeout_seconds=DEFAULT_MAIN_FLOW_TIMEOUT
)
def main_etl_flow():
    flow_log = get_run_logger()
    flow_run_ctx = context.get_run_context().flow_run
    flow_run_id = str(flow_run_ctx.id) if flow_run_ctx else \"local_main_sync\"
    flow_type = \"sync\"
    flow_log.info(f\"Starting flow run '{flow_run_ctx.name if flow_run_ctx else 'main_etl_flow'}'...\", extra={\"flow_run_id\": flow_run_id})
    # Corre\u00e7\u00e3o: Usar a m\u00e9trica espec\u00edfica etl_runs_total
    etl_runs_total.labels(flow_type=flow_type).inc()

    result = {}
    if not tracemalloc.is_tracing(): # Iniciar tracemalloc se n\u00e3o estiver ativo
        tracemalloc.start()

    try:
        _, _, etl_service, _, _, _, _, _, _ = initialize_components()
        result = etl_service.run_etl(flow_type=flow_type)

        if result.get(\"status\", \"error\") not in (\"success\", \"success_no_new_data\"):
            message = result.get(\"message\", \"Unknown error in ETL service\")
            flow_log.error(f\"ETL Service reported failure/errors: {message}\", etl_result=result)
            # Incrementar falha aqui se o servi\u00e7o indicar erro, mesmo sem exce\u00e7\u00e3o
            etl_run_failures_total.labels(flow_type=flow_type).inc()
            raise RuntimeError(f\"ETL task failed: {message}\")

        flow_log.info(\"Main ETL flow completed.\", status=result.get(\"status\"))

    except Exception as e:
        flow_log.critical(f\"Main ETL flow failed critically: {str(e)}\", exc_info=True)
        # Corre\u00e7\u00e3o: Remover chamada para etl_failure_counter
        # etl_failure_counter.labels(flow_type=flow_type).inc() <-- Removido
        # Corre\u00e7\u00e3o: Usar a m\u00e9trica espec\u00edfica etl_run_failures_total
        etl_run_failures_total.labels(flow_type=flow_type).inc()
        raise

    finally:
        peak_mem_mb = 0
        cpu_perc = 0
        thread_cnt = 0

        try:
            p = psutil.Process()
            with p.oneshot(): # Otimiza\u00e7\u00e3o para m\u00faltiplas chamadas psutil
                cpu_perc = p.cpu_percent(interval=None)
                thread_cnt = p.num_threads()

            # Usar tracemalloc para PICO de mem\u00f3ria
            if tracemalloc.is_tracing():
                try:
                    current_mem, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop()
                    tracemalloc.clear_traces()
                    flow_log.debug(f\"Mem\u00f3ria (tracemalloc): Pico={peak_mem_mb:.2f}MB\")
                except Exception as trace_err:
                    flow_log.error(\"Erro ao obter/parar tracemalloc\", error=str(trace_err))
                    if tracemalloc.is_tracing(): tracemalloc.stop(); tracemalloc.clear_traces()

            # Adiciona m\u00e9tricas de recurso ao dict de resultado, se desejar
            if isinstance(result, dict):
                result[\"peak_memory_mb\"] = peak_mem_mb
                result[\"cpu_percent_final\"] = cpu_perc
                result[\"thread_count_final\"] = thread_cnt
            else: # Caso result n\u00e3o seja um dict (ex: falha antes da inicializa\u00e7\u00e3o)
                 flow_log.warning(\"Resultado do fluxo n\u00e3o \u00e9 um dicion\u00e1rio, n\u00e3o \u00e9 poss\u00edvel adicionar m\u00e9tricas de recurso.\")

            # Define as m\u00e9tricas Gauge
            if peak_mem_mb > 0:
                etl_process_memory_mbytes.labels(flow_type=flow_type).set(peak_mem_mb)
            if cpu_perc >= 0:
                etl_process_cpu_percent.labels(flow_type=flow_type).set(cpu_perc)
            if thread_cnt > 0:
                etl_process_thread_count.labels(flow_type=flow_type).set(thread_cnt)

            flow_log.debug(\"M\u00e9tricas de recursos coletadas\",
                           memory_peak_mb=peak_mem_mb,
                           cpu_percent=cpu_perc,
                           threads=thread_cnt)

        except Exception as psutil_err:
            flow_log.error(\"Falha ao coletar m\u00e9tricas de recursos com psutil/tracemalloc\", error=str(psutil_err))

        etl_heartbeat.labels(flow_type=flow_type).set_to_current_time()
        flow_log.info(\"Pushing metrics to Pushgateway for main sync flow.\")
        push_metrics_to_gateway(job_name=\"pipedrive_sync_job\", grouping_key={'flow_run_id': flow_run_id})

@task(name=\"Get Deals for Backfill Task\", retries=1)
def get_deals_for_backfill_task(data_repo: PipedriveDataRepository, limit: int) -> List[str]:
    task_log = get_run_logger()
    task_log.info(f\"Fetching up to {limit} deal IDs for history backfill.\")
    ids = data_repo.get_deals_needing_history_backfill(limit=limit)
    task_log.info(f\"Found {len(ids)} deals for this backfill batch.\")
    return ids

@task(name=\"Get Backfill Remaining Count Task\", retries=1)
def get_initial_backfill_count_task(data_repo: PipedriveDataRepository) -> int:
    task_log = get_run_logger()
    task_log.info(\"Counting total deals needing history backfill.\")
    count = data_repo.count_deals_needing_backfill()
    if count >= 0:
        task_log.info(f\"Estimated {count} deals remaining for backfill.\")
        backfill_deals_remaining_gauge.set(count)
    else:
        task_log.warning(\"Failed to get backfill remaining count.\")
        backfill_deals_remaining_gauge.set(-1)
    return count

@task(
    name=\"Run Backfill Batch Task\",
    retries=DEFAULT_TASK_RETRIES,
    retry_delay_seconds=DEFAULT_TASK_RETRY_DELAY,
    log_prints=True,
)
def run_backfill_batch_task(etl_service: ETLService, deal_ids: List[str]) -> Dict[str, Any]:
    task_log = get_run_logger()
    if not deal_ids:
        task_log.info(\"No deals in this batch to backfill.\")
        return {\"status\": \"skipped\", \"processed_deals\": 0}

    task_log.info(f\"Running backfill for {len(deal_ids)} deals.\")
    result = etl_service.run_retroactive_backfill(deal_ids)
    task_log.info(\"Backfill batch finished.\", extra=result)
    return result

@flow(
    name=\"Pipedrive Stage History Backfill Flow\",
    log_prints=True,
    timeout_seconds=DEFAULT_BACKFILL_FLOW_TIMEOUT
)
def backfill_stage_history_flow(
    daily_deal_limit: int = BACKFILL_DAILY_LIMIT,
    db_batch_size: int = BACKFILL_DB_BATCH_SIZE
):
    flow_log = get_run_logger()
    flow_run_ctx = context.get_run_context().flow_run
    flow_run_id = str(flow_run_ctx.id) if flow_run_ctx else \"local_backfill\"
    flow_type = \"backfill\"
    flow_log.info(f\"Starting {flow_run_ctx.name if flow_run_ctx else 'backfill_flow'}\", extra={\"flow_run_id\": flow_run_id})
    flow_log.info(f\"Daily limit: {daily_deal_limit}, DB fetch batch size: {db_batch_size}\")
    # Corre\u00e7\u00e3o: Remover etl_counter e usar etl_runs_total
    # etl_counter.labels(flow_type=flow_type).inc() # <--- Removido
    etl_runs_total.labels(flow_type=flow_type).inc()

    total_processed_today = 0
    final_status = \"completed\"
    backfill_completed_successfully = False
    result_payload = {}
    if not tracemalloc.is_tracing(): # Iniciar tracemalloc
        tracemalloc.start()

    try:
        _, data_repo, etl_service, _, _, _, _, _, _ = initialize_components()
        initial_count = get_initial_backfill_count_task(data_repo=data_repo)

        while total_processed_today < daily_deal_limit:
            remaining_limit = daily_deal_limit - total_processed_today
            current_batch_limit = min(db_batch_size, remaining_limit)
            if current_batch_limit <= 0:
                flow_log.info(\"Daily limit reached.\")
                break

            flow_log.info(f\"Attempting to fetch next batch of deals (limit: {current_batch_limit}).\")
            deal_ids_batch = get_deals_for_backfill_task(data_repo=data_repo, limit=current_batch_limit)

            if not deal_ids_batch:
                flow_log.info(\"No more deals found needing backfill.\")
                backfill_deals_remaining_gauge.set(0)
                backfill_completed_successfully = True
                break

            batch_result = run_backfill_batch_task(etl_service=etl_service, deal_ids=deal_ids_batch)
            processed_in_batch = batch_result.get(\"processed_deals\", 0)
            total_processed_today += processed_in_batch

            if batch_result.get(\"status\") != \"success\" and batch_result.get(\"status\") != \"skipped\":
                 final_status = \"completed_with_errors\"
                 # Considerar incrementar falha se um batch falhar, mesmo que o fluxo continue
                 # etl_run_failures_total.labels(flow_type=flow_type).inc() # Opcional aqui

            if initial_count >= 0:
                 current_remaining = max(0, initial_count - total_processed_today)
                 backfill_deals_remaining_gauge.set(current_remaining)

            flow_log.info(f\"Backfill batch completed. Processed so far today: {total_processed_today}/{daily_deal_limit}\")
            time.sleep(2)

        flow_log.info(\"Backfill flow processing loop finished for today.\", final_status=final_status, total_processed=total_processed_today)

        final_remaining_count = data_repo.count_deals_needing_backfill()
        if final_remaining_count == 0 and final_status == \"completed\":
             backfill_completed_successfully = True
        elif final_remaining_count < 0:
             final_status = \"completed_with_errors\" # Erro ao contar no final

        result_payload = {
            \"status\": final_status,
            \"total_processed_deals\": total_processed_today,
            \"backfill_complete\": backfill_completed_successfully,
            \"estimated_remaining\": final_remaining_count
        }
        flow_log.info(\"Final backfill run result\", **result_payload)

        # Incrementar falha apenas se o status final n\u00e3o for 'completed' E o backfill n\u00e3o estiver completo
        if final_status != \"completed\" and not backfill_completed_successfully:
             # Corre\u00e7\u00e3o: Remover etl_failure_counter
             # etl_failure_counter.labels(flow_type=flow_type).inc() # <--- Removido
             # J\u00e1 incrementado no except principal
             pass # A falha cr\u00edtica \u00e9 capturada no except abaixo

    except Exception as e:
        # Corre\u00e7\u00e3o: Remover etl_failure_counter
        # etl_failure_counter.labels(flow_type=flow_type).inc() # <--- Removido
        etl_run_failures_total.labels(flow_type=flow_type).inc() # Correto usar este
        flow_log.critical(\"Backfill flow failed critically.\", exc_info=True)
        final_status = \"failed\"
        result_payload = {\"status\": final_status, \"error\": str(e)}
        raise

    finally:
        peak_mem_mb = 0
        cpu_perc = 0
        thread_cnt = 0

        try:
            p = psutil.Process()
            with p.oneshot():
                cpu_perc = p.cpu_percent(interval=None)
                thread_cnt = p.num_threads()

            if tracemalloc.is_tracing():
                try:
                    current_mem, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop()
                    tracemalloc.clear_traces()
                    flow_log.debug(f\"Mem\u00f3ria (tracemalloc): Pico={peak_mem_mb:.2f}MB\")
                except Exception as trace_err:
                    flow_log.error(\"Erro ao obter/parar tracemalloc\", error=str(trace_err))
                    if tracemalloc.is_tracing(): tracemalloc.stop(); tracemalloc.clear_traces()

            # Adiciona ao payload final, se existir
            if isinstance(result_payload, dict):
                result_payload[\"peak_memory_mb\"] = peak_mem_mb
                result_payload[\"cpu_percent_final\"] = cpu_perc
                result_payload[\"thread_count_final\"] = thread_cnt

            if peak_mem_mb > 0:
                etl_process_memory_mbytes.labels(flow_type=flow_type).set(peak_mem_mb)
            if cpu_perc >= 0:
                etl_process_cpu_percent.labels(flow_type=flow_type).set(cpu_perc)
            if thread_cnt > 0:
                etl_process_thread_count.labels(flow_type=flow_type).set(thread_cnt)

            flow_log.debug(\"M\u00e9tricas de recursos coletadas\",
                           memory_peak_mb=peak_mem_mb,
                           cpu_percent=cpu_perc,
                           threads=thread_cnt)

        except Exception as psutil_err:
            flow_log.error(\"Falha ao coletar m\u00e9tricas de recursos com psutil/tracemalloc\", error=str(psutil_err))

        etl_heartbeat.labels(flow_type=flow_type).set_to_current_time()
        flow_log.info(\"Pushing metrics to Pushgateway for backfill flow.\")
        push_metrics_to_gateway(job_name=\"pipedrive_backfill_job\", grouping_key={'flow_run_id': flow_run_id})

@flow(
    name=\"Batch Size Experiment Flow\",
    log_prints=True,
    timeout_seconds=10800
)
def batch_size_experiment_flow(
    batch_sizes: List[int] = [300, 500, 750, 1000, 1500, 2000],
    test_data_size: int = 5000
):
    flow_log = get_run_logger()
    flow_run_ctx = context.get_run_context().flow_run
    flow_run_id = str(flow_run_ctx.id) if flow_run_ctx else \"local_batch_experiment\"
    flow_type=\"experiment\" # Definir flow_type aqui
    flow_log.info(\"Starting batch size experiment flow.\", extra={\"flow_run_id\": flow_run_id})

    results = []
    optimal_size = DEFAULT_OPTIMAL_BATCH_SIZE
    etl_runs_total.labels(flow_type=flow_type).inc()
    if not tracemalloc.is_tracing(): # Iniciar tracemalloc
        tracemalloc.start()

    try:
        # Corre\u00e7\u00e3o: Obter todos os componentes necess\u00e1rios, incluindo config_repo
        client, data_repo, etl_service, user_repo, person_repo, stage_repo, pipeline_repo, org_repo, config_repo = initialize_components()

        flow_log.info(f\"Fetching up to {test_data_size} recent deals for testing...\")
        # Usar 'client' obtido de initialize_components
        test_data = list(client.fetch_all_deals_stream(items_limit=test_data_size))
        if not test_data:
            raise ValueError(f\"No test data ({test_data_size} deals) could be fetched.\")
        flow_log.info(f\"Fetched {len(test_data)} deals for experiment.\")

        for size in batch_sizes:
            batch_log = flow_log.bind(batch_size=size)
            batch_log.info(f\"Starting experiment run with batch size: {size}\")
            # Criar um novo ETLService com o batch_size espec\u00edfico para este teste
            experiment_etl_service = ETLService(
                 client, data_repo, user_repo, person_repo, stage_repo, pipeline_repo, org_repo,
                 deal_mapper, batch_size=size # Usar 'size' aqui
            )
            run_result = {}
            try:
                run_result = experiment_etl_service.run_etl_with_data(
                    data=test_data,
                    batch_size=size, # Passar size para o m\u00e9todo tamb\u00e9m
                    flow_type=flow_type
                )
                metrics = {
                    'batch_size': size,
                    'status': run_result.get(\"status\", \"error\"),
                    'duration': run_result.get(\"duration_seconds\"),
                    'total_loaded': run_result.get(\"total_loaded\"),
                    'total_failed': (run_result.get(\"total_schema_failed\", 0) +
                                     run_result.get(\"total_domain_failed\", 0) +
                                     run_result.get(\"total_load_failed\", 0)),
                    'memory_peak': run_result.get(\"peak_memory_mb\"),
                    'data_quality_issues': (run_result.get(\"total_schema_failed\", 0) +
                                            run_result.get(\"total_domain_failed\", 0))
                }
                results.append(metrics)
                batch_log.info(\"Experiment run completed.\", extra=metrics)

            except Exception as exp_err:
                batch_log.error(\"Experiment run failed\", error=str(exp_err), exc_info=True)
                results.append({
                     'batch_size': size, 'status': 'critical_error', 'duration': None,
                     'total_loaded': 0, 'total_failed': len(test_data), 'memory_peak': None,
                     'data_quality_issues': len(test_data)
                })

            time.sleep(5) # Pausa entre os testes

        # --- Calculate Optimal Size ---
        if not results:
             flow_log.warning(\"No results collected from experiments.\")
        else:
            df = pd.DataFrame(results)
            flow_log.info(\"Experiment Results Summary:\n\" + df.to_string())
            valid_df = df[
                 (df['status'] == 'success') &
                 df['duration'].notna() & (df['duration'] > 0) &
                 df['memory_peak'].notna() & (df['memory_peak'] >= 0) &
                 df['data_quality_issues'].notna()
            ].copy()

            if valid_df.empty:
                 flow_log.error(\"No valid successful runs found to calculate optimal batch size.\")
            else:
                 # Score: Higher throughput (loaded/duration) is better, lower memory is better
                 valid_df['throughput'] = valid_df['total_loaded'] / valid_df['duration']
                 max_throughput = valid_df['throughput'].max()
                 max_memory = valid_df['memory_peak'].max()
                 valid_df['norm_throughput'] = valid_df['throughput'] / max_throughput if max_throughput > 0 else 0
                 valid_df['norm_memory'] = valid_df['memory_peak'] / max_memory if max_memory > 0 else 0
                 valid_df['score'] = (0.6 * valid_df['norm_throughput']) + (0.4 * (1 - valid_df['norm_memory']))

                 best_run = valid_df.loc[valid_df['score'].idxmax()]
                 optimal_size = int(best_run['batch_size'])
                 flow_log.info(f\"Optimal batch size calculated: {optimal_size} (Score: {best_run['score']:.3f})\")
                 flow_log.info(\"Best Run Details:\n\" + best_run.to_string())

                 # --- Save Optimal Size to Config ---
                 try:
                     config_value = {'value': optimal_size, 'source': 'experiment', 'run_id': flow_run_id, 'calculation_time': datetime.datetime.now(datetime.timezone.utc).isoformat()}
                     # Usar config_repo obtido de initialize_components
                     config_repo.save_configuration(key=\"optimal_batch_size\", value=config_value)
                     flow_log.info(f\"Saved optimal batch size {optimal_size} to configuration.\")
                 except Exception as save_err:
                      flow_log.error(\"Failed to save optimal batch size configuration\", error=str(save_err), exc_info=True)

    except Exception as e:
        etl_run_failures_total.labels(flow_type=flow_type).inc()
        flow_log.critical(\"Batch size experiment flow failed critically.\", exc_info=True)
        raise
    finally:
        peak_mem_mb = 0
        cpu_perc = 0
        thread_cnt = 0
        final_result_payload = {\"status\": \"unknown\"} # Payload para o resultado final do fluxo

        try:
            p = psutil.Process()
            with p.oneshot():
                cpu_perc = p.cpu_percent(interval=None)
                thread_cnt = p.num_threads()

            if tracemalloc.is_tracing():
                try:
                    current_mem, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop()
                    tracemalloc.clear_traces()
                    flow_log.debug(f\"Mem\u00f3ria (tracemalloc): Pico={peak_mem_mb:.2f}MB\")
                except Exception as trace_err:
                    flow_log.error(\"Erro ao obter/parar tracemalloc\", error=str(trace_err))
                    if tracemalloc.is_tracing(): tracemalloc.stop(); tracemalloc.clear_traces()

            # Tenta adicionar ao payload final
            final_result_payload = {\"status\": \"completed\", \"optimal_batch_size_calculated\": optimal_size, \"results\": results}
            final_result_payload[\"peak_memory_mb\"] = peak_mem_mb
            final_result_payload[\"cpu_percent_final\"] = cpu_perc
            final_result_payload[\"thread_count_final\"] = thread_cnt

            if peak_mem_mb > 0:
                etl_process_memory_mbytes.labels(flow_type=flow_type).set(peak_mem_mb)
            if cpu_perc >= 0:
                etl_process_cpu_percent.labels(flow_type=flow_type).set(cpu_perc)
            if thread_cnt > 0:
                etl_process_thread_count.labels(flow_type=flow_type).set(thread_cnt)

            flow_log.debug(\"M\u00e9tricas de recursos coletadas\",
                           memory_peak_mb=peak_mem_mb,
                           cpu_percent=cpu_perc,
                           threads=thread_cnt)

        except Exception as psutil_err:
            flow_log.error(\"Falha ao coletar m\u00e9tricas de recursos com psutil/tracemalloc\", error=str(psutil_err))

        flow_log.info(\"Pushing metrics to Pushgateway for batch experiment flow.\")
        push_metrics_to_gateway(job_name=\"batch_experiment\", grouping_key={'flow_run_id': flow_run_id})

    # Retorna o resultado final do fluxo
    return final_result_payload"}
,
{"path": "flows/utils/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "flows/utils/flows_utils.py", "encoding": "utf-8", "content": "from datetime import datetime, timezone
import random
from typing import Any, Dict, List
import pandas as pd
import structlog
from prefect.utilities.hashing import hash_objects

from infrastructure.repository_impl.pipedrive_repository import PipedriveRepository

log = structlog.get_logger(__name__)

def validate_loaded_data(
    repository: PipedriveRepository,
    source_data: List[Dict],
    batch_size: int
) -> Dict[str, Any]:
    \"\"\"Valida\u00e7\u00e3o completa dos dados carregados.\"\"\"
    validation_log = structlog.get_logger().bind(batch_size=batch_size)
    issues = 0
    validated = 0
    
    try:
        # 1. Verificar contagem b\u00e1sica
        expected_count = len(source_data)
        actual_count = repository.count_records()
        count_match = expected_count == actual_count
        
        # 2. Verificar IDs ausentes
        source_ids = {str(item['id']) for item in source_data if 'id' in item}
        db_ids = set(repository.get_all_ids())
        
        missing_ids = source_ids - db_ids
        extra_ids = db_ids - source_ids
        
        # 3. Amostragem de valida\u00e7\u00e3o detalhada
        sample_size = min(100, len(source_data))
        sample_records = random.sample(source_data, sample_size)
        detailed_issues = []
        
        for record in sample_records:
            db_data = repository.get_record_by_id(record['id'])
            if not db_data:
                detailed_issues.append(f\"Missing record {record['id']}\")
                continue
                
            # Verificar campos cr\u00edticos
            critical_fields = ['value', 'currency', 'status', 'stage_name']
            for field in critical_fields:
                source_val = record.get(field)
                db_val = db_data.get(field)
                
                if source_val != db_val:
                    detailed_issues.append(
                        f\"Field mismatch {field}: {source_val} vs {db_val}\"
                    )
                    issues += 1
        
        # 4. Verificar consist\u00eancia de datas
        date_issues = repository.validate_date_consistency()
        
        return {
            \"data_quality_issues\": issues + len(missing_ids) + len(extra_ids) + date_issues,
            \"count_match\": count_match,
            \"missing_ids_count\": len(missing_ids),
            \"extra_ids_count\": len(extra_ids),
            \"detailed_issues_sample\": detailed_issues[:5],
            \"date_issues\": date_issues
        }
        
    except Exception as e:
        validation_log.error(\"Data validation failed\", error=str(e))
        return {\"data_quality_issues\": -1, \"error\": str(e)}

def calculate_optimal_batch_size(results: List[Dict]) -> int:
    \"\"\"Calcula o tamanho ideal de batch com base nas m\u00e9tricas coletadas, ignorando falhas.\"\"\"
    default_batch_size = 1000 

    if not results:
        print(f\"WARN: No results provided, returning default batch size {default_batch_size}\")
        return default_batch_size

    df = pd.DataFrame(results)

    valid_df = df[
        (df['data_quality_issues'] != -1) &
        df['duration'].notna() & (df['duration'] > 0) &
        df['memory_peak'].notna() & (df['memory_peak'] >= 0)
    ].copy() 

    if valid_df.empty:
        print(f\"WARN: No valid results after filtering, returning default batch size {default_batch_size}\")
        print(\"Original results head:\n\", df.head())
        return default_batch_size

    # --- Normaliza\u00e7\u00e3o e C\u00e1lculo do Score (apenas em dados v\u00e1lidos) ---
    max_duration = valid_df['duration'].max()
    max_memory = valid_df['memory_peak'].max()
    max_quality_issues = valid_df['data_quality_issues'].max() 

    valid_df['norm_duration'] = valid_df['duration'] / max_duration if max_duration > 0 else 0
    valid_df['norm_memory'] = valid_df['memory_peak'] / max_memory if max_memory > 0 else 0

    if max_quality_issues > 0:
         valid_df['norm_quality'] = 1 - (valid_df['data_quality_issues'] / max_quality_issues)
    else:
         valid_df['norm_quality'] = 1.0 

    valid_df.fillna(0, inplace=True)

    weights = {
        'duration': 0.4,  # Menor dura\u00e7\u00e3o \u00e9 melhor (1 - norm_duration)
        'memory': 0.4,    # Menor mem\u00f3ria \u00e9 melhor (1 - norm_memory)
        'quality': 0.2    # Maior qualidade \u00e9 melhor (norm_quality)
    }

    valid_df['score'] = (
        weights['duration'] * (1 - valid_df['norm_duration']) +
        weights['memory'] * (1 - valid_df['norm_memory']) +
        weights['quality'] * valid_df['norm_quality']
    )

    best_idx = valid_df['score'].idxmax()
    best = valid_df.loc[best_idx]

    optimal_size = int(best['batch_size'])
    print(f\"INFO: Optimal batch size calculated: {optimal_size} based on score {best['score']:.3f}\")
    print(\"INFO: Scores per batch size (valid runs):\n\", valid_df[['batch_size', 'score', 'duration', 'memory_peak', 'data_quality_issues']])

    return optimal_size

def update_optimal_batch_config(repository: PipedriveRepository, optimal_size: int):
    \"\"\"Atualiza a configura\u00e7\u00e3o do tamanho \u00f3timo de batch no banco de dados.\"\"\"
    config_key = \"optimal_batch_size\"
    logger = log.bind(config_key=config_key, optimal_size=optimal_size)
    try:
        now_iso = datetime.now(timezone.utc).isoformat()
        config_value = {'value': optimal_size, 'updated_at': now_iso }
        repository.save_configuration(key=config_key, value=config_value)
        logger.info(\"Optimal batch size configuration updated in DB.\")
    except Exception as e:
        logger.error(\"Failed to update optimal batch size config in DB\", exc_info=True)
        
def get_optimal_batch_size(repository: PipedriveRepository, default_size: int = 1000) -> int:
    \"\"\"Busca o tamanho \u00f3timo de batch da configura\u00e7\u00e3o do banco de dados.\"\"\"
    config_key = \"optimal_batch_size\"
    logger = log.bind(config_key=config_key)
    try:
        config = repository.get_configuration(config_key)
        if config and isinstance(config.get(\"value\"), int) and config[\"value\"] > 0:
             size = int(config[\"value\"])
             logger.info(f\"Retrieved optimal batch size from config: {size}\")
             return size
        else:
             logger.warning(f\"Optimal batch size not found or invalid in config. Using default: {default_size}\", config_value=config)
             return default_size
    except Exception as e:
        logger.error(\"Failed to get optimal batch size from config. Using default.\", error=str(e), default_size=default_size, exc_info=True)
        return default_size
    
def backfill_cache_key_from_deal_ids(_, arguments: Dict[str, Any]) -> str:
    deal_ids = arguments.get(\"deal_ids\", [])
    return hash_objects(sorted(deal_ids))
"}
,
{"path": "flows/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "scripts/create_secret_block.py", "encoding": "utf-8", "content": "import sys
from prefect.blocks.system import Secret
import argparse
import os

# --- Configuration ---
default_block_name = \"github-access-token\"

# --- Argument Parser ---
parser = argparse.ArgumentParser(description=\"Cria ou atualiza um Bloco Prefect do tipo Secret.\")
parser.add_argument(\"-n\", \"--name\", default=default_block_name,
                    help=f\"Nome para o Bloco Secret (padr\u00e3o: {default_block_name})\")
parser.add_argument(\"token\", help=\"O valor do token secreto a ser armazenado\")

args = parser.parse_args()

block_name = args.name
secret_value = args.token

print(f\"Tentando criar/atualizar Bloco Secret chamado '{block_name}'...\")

try:
    secret_block = Secret(value=secret_value)

    block_doc_id = secret_block.save(name=block_name, overwrite=True)

    print(f\"Bloco Secret '{block_name}' salvo com sucesso!\")
    print(f\"ID do Documento do Bloco: {block_doc_id}\")

    print(\"\n--- Pr\u00f3ximo Passo ---\")
    print(\"Verifique se o seu arquivo 'prefect.yaml' est\u00e1 referenciando o nome de bloco correto:\")
    print(f\"access_token: '{{{{ prefect.blocks.secret.{block_name} }}}}'\")
    print(\"Certifique-se de que esta linha est\u00e1 correta em TODOS os deployments que precisam do token.\")


except Exception as e:
    print(f\"\nErro ao salvar o bloco: {e}\")
    print(\"\nPoss\u00edveis causas:\")
    print(\"- O servidor Prefect Orion n\u00e3o est\u00e1 acess\u00edvel.\")
    print(\"- A vari\u00e1vel de ambiente PREFECT_API_URL n\u00e3o est\u00e1 definida ou est\u00e1 incorreta.\")
    print(\"  (Certifique-se que aponta para: http://localhost:4200/api)\")
    sys.exit(1)"}
,
{"path": "scripts/create_or_update_core_blocks.py", "encoding": "utf-8", "content": "import os
from prefect.blocks.system import Secret, JSON
from prefect_kubernetes.jobs import KubernetesJob
from dotenv import load_dotenv
import structlog
from typing import Optional, Dict

try:
    structlog.configure(
        processors=[
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt=\"iso\"),
            structlog.dev.ConsoleRenderer(),
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
except structlog.exceptions.AlreadyConfiguredError:
    pass 
log = structlog.get_logger(__name__)

load_dotenv()

print(\"--- Iniciando Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o de Blocos Prefect ---\")

# --- Bloco Secret ---
secret_name = \"github-access-token\"
github_pat = os.getenv(\"GITHUB_PAT\")
print(f\"Processando Bloco Secret '{secret_name}'...\")
if github_pat:
    try:
        secret_block = Secret(value=github_pat)
        secret_block.save(name=secret_name, overwrite=True)
        print(f\"-> Bloco Secret '{secret_name}' salvo com sucesso.\")
    except Exception as e:
        print(f\"Erro ao salvar Bloco Secret '{secret_name}': {e}\")
else:
    print(f\"AVISO: Vari\u00e1vel de ambiente GITHUB_PAT n\u00e3o definida. Bloco '{secret_name}' n\u00e3o criado/atualizado.\")

# --- Bloco JSON para DB Pool ---
db_block_name = \"postgres-pool\"
db_config = {
    \"dsn\": os.getenv(\"DATABASE_URL\", \"postgresql://user:password@db:5432/pipedrive\"),
    \"minconn\": int(os.getenv(\"DB_MIN_CONN\", 1)),
    \"maxconn\": int(os.getenv(\"DB_MAX_CONN\", 10)) 
}
print(f\"Processando Bloco JSON '{db_block_name}'...\")
try:
    json_block_db = JSON(value=db_config)
    json_block_db.save(name=db_block_name, overwrite=True)
    print(f\"-> Bloco JSON '{db_block_name}' salvo com sucesso.\")
except Exception as e:
    print(f\"Erro ao salvar Bloco JSON '{db_block_name}': {e}\")


# --- Bloco JSON para Redis Cache ---
redis_block_name = \"redis-cache\"
redis_config = {
    \"connection_string\": os.getenv(\"REDIS_URL\", \"redis://redis:6379/0\")
}
print(f\"Processando Bloco JSON '{redis_block_name}'...\")
try:
    json_block_redis = JSON(value=redis_config)
    json_block_redis.save(name=redis_block_name, overwrite=True)
    print(f\"-> Bloco JSON '{redis_block_name}' salvo com sucesso.\")
except Exception as e:
    print(f\"Erro ao salvar Bloco JSON '{redis_block_name}': {e}\")


# --- Blocos KubernetesJob ---
# Configura\u00e7\u00f5es comuns que podem ser reutilizadas
default_image = \"pipedrive_metabase_integration-etl:latest\"
default_namespace = \"default\"
default_env_from = [
    {\"secretRef\": {\"name\": \"app-secrets\"}},
    {\"secretRef\": {\"name\": \"db-secrets\"}},
]
default_env = {
    \"PUSHGATEWAY_ADDRESS\": os.getenv(\"PUSHGATEWAY_ADDRESS\", \"pushgateway:9091\"),
    \"PREFECT_API_URL\": \"http://prefect-orion:4200/api\"
}
default_init_containers = [
    { 
        \"name\": \"wait-for-db\", 
        \"image\": \"busybox:1.36\", 
        \"command\": ['sh', '-c', 'echo Waiting for db...; while ! nc -z -w 1 db 5432; do sleep 2; done; echo DB ready.'] 
    },
    { 
        \"name\": \"wait-for-redis\", 
        \"image\": \"busybox:1.36\", 
        \"command\": ['sh', '-c', 'echo Waiting for redis...; while ! nc -z -w 1 redis 6379; do sleep 2; done; echo Redis ready.'] 
    },
    { 
        \"name\": \"wait-for-orion\", 
        \"image\": \"curlimages/curl:latest\", 
        \"command\": ['sh', '-c', 'echo Waiting for orion...; until curl -sf http://prefect-orion:4200/api/health > /dev/null; do echo -n \".\"; sleep 3; done; echo Orion ready.'] 
    }
]
default_job_watch_timeout = 120

# Fun\u00e7\u00e3o helper para criar o dicion\u00e1rio do Job Template
def create_job_spec_dict(image: str, resources: dict, pod_labels: Optional[dict] = None) -> dict:
    \"\"\"Cria manualmente a estrutura do Job Kubernetes.\"\"\"
    labels = {\"app.kubernetes.io/created-by\": \"prefect\"}
    if pod_labels:
        labels.update(pod_labels)

    job_spec = {
        \"metadata\": {\"labels\": labels},
        \"spec\": {
            \"template\": {
                \"spec\": {
                    \"initContainers\": default_init_containers,
                    \"containers\": [
                        {
                            \"name\": \"prefect-job\",
                            \"image\": image,
                            \"resources\": resources,
                            \"envFrom\": default_env_from,
                            \"env\": [{\"name\": k, \"value\": v} for k, v in default_env.items()],
                        }
                    ],
                }
            }
        }
    }
    return job_spec

# 1. Bloco para Infraestrutura K8s Padr\u00e3o
default_k8s_job_block_name = \"default-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{default_k8s_job_block_name}'...\")
try:
    default_resources = {
        \"requests\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"},
        \"limits\": {\"memory\": \"4Gi\", \"cpu\": \"2\"}
    }
    # Gerar o dicion\u00e1rio completo do Job
    default_job_dict = create_job_spec_dict(image=default_image, resources=default_resources)

    # Instanciar o Bloco usando o par\u00e2metro v1_job
    default_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=default_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    default_k8s_job_block.save(name=default_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{default_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{default_k8s_job_block_name}'\", exc_info=True)


# 2. Bloco para Infraestrutura K8s do Experimento
experiment_k8s_job_block_name = \"experiment-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{experiment_k8s_job_block_name}'...\")
try:
    experiment_resources = {
        \"requests\": {\"memory\": \"2Gi\", \"cpu\": \"1\"},
        \"limits\": {\"memory\": \"8Gi\", \"cpu\": \"2\"}
    }
    experiment_job_dict = create_job_spec_dict(image=default_image, resources=experiment_resources, pod_labels={\"flow\": \"experiment\"})

    experiment_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=experiment_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    experiment_k8s_job_block.save(name=experiment_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{experiment_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{experiment_k8s_job_block_name}'\", exc_info=True)


# 3. Bloco para Syncs Leves
light_sync_k8s_job_block_name = \"light-sync-k8s-job\"
print(f\"Processando Bloco KubernetesJob '{light_sync_k8s_job_block_name}'...\")
try:
    light_sync_resources = {
         \"requests\": {\"memory\": \"512Mi\", \"cpu\": \"250m\"},
         \"limits\": {\"memory\": \"1Gi\", \"cpu\": \"500m\"}
    }
    light_sync_job_dict = create_job_spec_dict(image=default_image, resources=light_sync_resources, pod_labels={\"flow\": \"light-sync\"})

    light_sync_k8s_job_block = KubernetesJob(
        namespace=default_namespace,
        v1_job=light_sync_job_dict,
        job_watch_timeout_seconds=default_job_watch_timeout
    )
    light_sync_k8s_job_block.save(name=light_sync_k8s_job_block_name, overwrite=True)
    print(f\"-> Bloco KubernetesJob '{light_sync_k8s_job_block_name}' salvo com sucesso.\")
except Exception as e:
    log.error(f\"Erro ao salvar Bloco KubernetesJob '{light_sync_k8s_job_block_name}'\", exc_info=True)


print(\"--- Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o de Blocos Prefect Conclu\u00edda ---\")
"}
,
{"path": "scripts/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/mappers/deal_mapper.py", "encoding": "utf-8", "content": "import json
from decimal import Decimal, InvalidOperation
from datetime import datetime, date, time
from typing import Any, Dict, Optional, Tuple

import structlog

from application.schemas.deal_schema import DealSchema
from core_domain.entities.deal import Deal
from core_domain.value_objects.identifiers import (
    DealId, UserId, PersonId, OrgId, StageId, PipelineId
)
from core_domain.value_objects.money import Money
from core_domain.value_objects.timestamp import Timestamp
from core_domain.value_objects.deal_status import DealStatus, DealStatusOption
from core_domain.value_objects.custom_field import CustomFieldKey, CustomFieldValue

from application.utils.column_utils import ADDRESS_COMPONENT_SUFFIX_MAP


log = structlog.get_logger(__name__)


# --- Helper para processar valores de campos customizados ---
def _process_custom_field_value(
    raw_value: Any,
    field_type: Optional[str]
) -> Tuple[Any, Optional[str]]:
    \"\"\"
    Processa o valor bruto de um campo customizado com base no tipo Pipedrive.
    Retorna o valor processado (tipo Python padr\u00e3o) e o tipo Pipedrive.
    \"\"\"
    processed_value = raw_value
    pipedrive_type_str = field_type if field_type else \"unknown\"

    if raw_value is None:
        return None, pipedrive_type_str

    try:
        if field_type in ('varchar', 'text', 'enum', 'phone', 'set', 'user', 'person', 'org', 'pipeline'):
             if isinstance(raw_value, dict) and 'id' in raw_value:
                 processed_value = int(raw_value['id'])
             elif isinstance(raw_value, dict) and 'value' in raw_value:
                 processed_value = raw_value['value']
                 if isinstance(processed_value, int):
                     processed_value = int(processed_value)
                 else:
                    processed_value = str(processed_value) if processed_value is not None else None
             elif isinstance(raw_value, (int, float)): 
                 processed_value = int(raw_value)
             else:
                 processed_value = str(raw_value)

        elif field_type in ('double', 'int'):
            processed_value = float(raw_value) if '.' in str(raw_value) else int(raw_value)
        elif field_type == 'monetary':
            if isinstance(raw_value, dict):
                amount = raw_value.get('value')
                processed_value = Decimal(amount) if amount is not None else Decimal(0)
            else:
                processed_value = Decimal(raw_value)
        elif field_type == 'date':
            processed_value = datetime.fromisoformat(str(raw_value)).date()
        elif field_type == 'time':
             processed_value = datetime.strptime(str(raw_value), '%H:%M:%S').time()
        elif field_type == 'datetime':
             dt_vo = Timestamp(datetime.fromisoformat(str(raw_value).replace('Z', '+00:00')))
             processed_value = dt_vo.value
        elif field_type == 'address':
            if isinstance(raw_value, str): 
                try:
                    processed_value = json.loads(raw_value)
                except json.JSONDecodeError:
                    log.warning(\"Could not decode address JSON string\", raw_value=raw_value)
                    processed_value = {\"formatted_address\": str(raw_value)}
            elif not isinstance(raw_value, dict):
                 processed_value = {\"formatted_address\": str(raw_value)}
            else:
                 processed_value = raw_value 
        else:
            processed_value = str(raw_value)

    except (ValueError, TypeError, InvalidOperation) as e:
        log.warning(
            \"Failed to process custom field value based on type\",
            raw_value=raw_value, field_type=field_type, error=str(e)
        )
        processed_value = str(raw_value) if raw_value is not None else None

    return processed_value, pipedrive_type_str


# --- Mapeamento API Dict -> Schema ---
def map_api_dict_to_schema(api_data: dict) -> DealSchema:
    \"\"\"
    Valida e converte um dicion\u00e1rio da API Pipedrive para um DealSchema Pydantic.
    Levanta ValidationError em caso de falha.
    \"\"\"
    try:
        return DealSchema.model_validate(api_data)
    except Exception as e:
        log.error(\"Pydantic validation failed for DealSchema\", data_preview=str(api_data)[:200], error=str(e))
        raise


# --- Mapeamento Schema -> Domain ---
def map_schema_to_domain(
    schema: DealSchema,
    custom_field_map: Dict[str, str], # { api_hash: normalized_name }
    field_definitions: Dict[str, Dict] # { api_hash: { \"key\": \"...\", \"field_type\": \"...\" } }
) -> Deal:
    \"\"\"
    Converte um DealSchema validado para a entidade Deal do dom\u00ednio.
    \"\"\"
    domain_custom_fields: Dict[CustomFieldKey, CustomFieldValue] = {}
    if schema.custom_fields:
        for api_hash_key, raw_value in schema.custom_fields.items():
            normalized_name = custom_field_map.get(api_hash_key)
            if not normalized_name:
                log.debug(\"Skipping custom field with unknown mapping\", api_key=api_hash_key)
                continue

            field_def = field_definitions.get(api_hash_key, {})
            field_type = field_def.get(\"field_type\")

            processed_value, pipedrive_type_str = _process_custom_field_value(raw_value, field_type)

            key_vo = CustomFieldKey(normalized_name)
            value_vo = CustomFieldValue(value=processed_value, pipedrive_type=pipedrive_type_str)
            domain_custom_fields[key_vo] = value_vo

    creator_user_id_val = schema.creator_user_id
    owner_id_val = schema.owner_id.get('id') if isinstance(schema.owner_id, dict) else schema.owner_id
    person_id_val = schema.person_id
    org_id_val = schema.org_id.get('value') if isinstance(schema.org_id, dict) else schema.org_id

    try:
        deal_entity = Deal(
            deal_id=DealId(schema.id),
            title=str(schema.title or \"\"),
            status=DealStatus(schema.status),
            value=Money(amount=Decimal(schema.value) if schema.value is not None else Decimal('0.0'),
                        currency=str(schema.currency or \"BRL\")),
            creator_user_id=UserId(int(creator_user_id_val)) if creator_user_id_val else None,
            owner_id=UserId(int(owner_id_val)) if owner_id_val else None, 
            pipeline_id=PipelineId(int(schema.pipeline_id)) if schema.pipeline_id else None, 
            stage_id=StageId(int(schema.stage_id)) if schema.stage_id else None, 
            # Datas/Horas - Schema j\u00e1 deve ter validado para datetime
            add_time=Timestamp(schema.add_time) if schema.add_time else None, 
            update_time=Timestamp(schema.update_time) if schema.update_time else None, 
            # Opcionais
            person_id=PersonId(int(person_id_val)) if person_id_val else None,
            org_id=OrgId(int(org_id_val)) if org_id_val else None,
            probability=Decimal(schema.probability) if schema.probability is not None else None,
            lost_reason=schema.lost_reason,
            visible_to=str(schema.visible_to) if schema.visible_to is not None else None,
            close_time=Timestamp(schema.close_time) if schema.close_time else None,
            won_time=Timestamp(schema.won_time) if schema.won_time else None,
            lost_time=Timestamp(schema.lost_time) if schema.lost_time else None,
            expected_close_date=schema.expected_close_date.date() if isinstance(schema.expected_close_date, datetime) \
                                else (datetime.strptime(schema.expected_close_date, '%Y-%m-%d').date() if isinstance(schema.expected_close_date, str) else None), # Ajuste para pegar s\u00f3 date
            custom_fields=domain_custom_fields
        )

        # Valida\u00e7\u00f5es p\u00f3s-constru\u00e7\u00e3o
        if not deal_entity.creator_user_id or not deal_entity.owner_id or \
           not deal_entity.pipeline_id or not deal_entity.stage_id or \
           not deal_entity.add_time or not deal_entity.update_time:
            log.warning(\"Deal entity created with missing core relationship IDs or timestamps\", deal_id=deal_entity.id)

        return deal_entity

    except Exception as domain_build_err:
        log.error(\"Failed to build Domain Deal entity from Schema\",
                  schema_id=schema.id, error=str(domain_build_err), exc_info=True)
        raise ValueError(f\"Could not create domain entity for deal {schema.id}\") from domain_build_err


# --- Mapeamento Domain -> Persistence Dict ---
def map_domain_to_persistence_dict(deal: Deal) -> Dict[str, Any]:
    \"\"\"
    Converte a entidade Deal do dom\u00ednio para um dicion\u00e1rio achatado
    adequado para persist\u00eancia no banco de dados.
    \"\"\"
    persistence_dict = {
        \"id\": str(deal.id.value),
        \"titulo\": deal.title,
        \"status\": deal.status.value.value,
        \"value\": deal.value.amount,
        \"currency\": deal.value.currency,
        \"add_time\": deal.add_time.value if deal.add_time else None,
        \"update_time\": deal.update_time.value if deal.update_time else None,
        \"close_time\": deal.close_time.value if deal.close_time else None,
        \"won_time\": deal.won_time.value if deal.won_time else None,
        \"lost_time\": deal.lost_time.value if deal.lost_time else None,
        \"expected_close_date\": deal.expected_close_date,
        \"creator_user_id\": deal.creator_user_id.value if deal.creator_user_id else None,
        \"owner_id\": deal.owner_id.value if deal.owner_id else None,
        \"person_id\": deal.person_id.value if deal.person_id else None,
        \"org_id\": deal.org_id.value if deal.org_id else None,
        \"stage_id\": deal.stage_id.value if deal.stage_id else None,
        \"pipeline_id\": deal.pipeline_id.value if deal.pipeline_id else None,
        \"probability\": deal.probability, 
        \"lost_reason\": deal.lost_reason,
        \"visible_to\": deal.visible_to,
        # Campos de nomes (ser\u00e3o preenchidos no Repository/ETLService antes de salvar, n\u00e3o est\u00e3o no domain)
        \"creator_user_name\": None,
        \"person_name\": None,
        \"stage_name\": None,
        \"pipeline_name\": None,
        \"owner_name\": None,
        \"org_name\": None,
         # Label (campo padr\u00e3o n\u00e3o mapeado no Domain Entity inicial)
         \"label\": None 
    }

    # Processar campos customizados
    for key_vo, value_vo in deal.custom_fields.items():
        normalized_key = key_vo.value
        value_to_persist = value_vo.value

        # L\u00f3gica especial para endere\u00e7os (achatamento)
        if value_vo.pipedrive_type == 'address' and isinstance(value_to_persist, dict):
            addr_dict = value_to_persist
            # Campo principal recebe o formatted_address
            persistence_dict[normalized_key] = addr_dict.get('formatted_address')

            # Campos de componentes (ex: local_do_acidente_numero_da_casa)
            for component_key, suffix in ADDRESS_COMPONENT_SUFFIX_MAP.items():
                subcol_name = f\"{normalized_key}_{suffix}\"
                persistence_dict[subcol_name] = addr_dict.get(component_key)
        else:
             if isinstance(value_to_persist, (set, tuple)):
                 persistence_dict[normalized_key] = list(value_to_persist)
             elif isinstance(value_to_persist, (time, date, datetime, Decimal)):
                 persistence_dict[normalized_key] = value_to_persist
             elif isinstance(value_to_persist, dict):
                  persistence_dict[normalized_key] = json.dumps(value_to_persist)
             else:
                 persistence_dict[normalized_key] = value_to_persist 

    # Remover chaves com valor None explicitamente se o DB n\u00e3o gostar?
    # persistence_dict = {k: v for k, v in persistence_dict.items() if v is not None}
    # Decis\u00e3o: Manter None, psycopg2/COPY lida bem com isso ('\N').

    return persistence_dict"}
,
{"path": "application/schemas/deal_schema.py", "encoding": "utf-8", "content": "from typing import Optional, Dict, Any, Union
from pydantic import BaseModel, Field, field_validator, confloat
from datetime import datetime, timezone

class DealSchema(BaseModel, extra='allow'):
    id: int
    title: Optional[str] = None
    creator_user_id: Optional[int] = None 
    person_id: Optional[int] = None
    stage_id: Optional[int] = None
    stage_name: Optional[str] = None
    pipeline_id: Optional[int] = None
    pipeline_name: Optional[str] = None 
    status: Optional[str] = None
    value: Optional[float] = 0.0
    currency: Optional[str] = 'BRL'
    add_time: Optional[datetime] = None
    update_time: Optional[datetime] = None
    custom_fields: Dict[str, Any] = Field({}, alias='custom_fields')

    class Config:
        populate_by_name  = True
        alias_generator = lambda x: x 

    @field_validator('creator_user_id', 'person_id', mode='before')
    def extract_id_from_dict(cls, value):
        if isinstance(value, dict):
            return value.get(\"id\")
        return value
    
    @field_validator('add_time', 'update_time')
    def parse_datetime_optional(cls, value):
        if value is None:
            return None
        if isinstance(value, datetime):
            return value.replace(tzinfo=timezone.utc) if value.tzinfo is None else value
        try:
            dt = datetime.fromisoformat(str(value).replace('Z', '+00:00'))
            return dt.astimezone(timezone.utc)
        except (ValueError, TypeError):
            print(f\"Warning: Could not parse date '{value}'. Setting to None.\")
            return None

    @field_validator('value')
    def value_to_float(cls, value):
        if value is None:
            return 0.0
        try:
            return float(value)
        except (ValueError, TypeError):
            print(f\"Warning: Could not convert value '{value}' to float. Setting to 0.0.\")
            return 0.0"}
,
{"path": "application/services/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/services/etl_service.py", "encoding": "utf-8", "content": "import json
import time
from datetime import datetime, timedelta, timezone
import tracemalloc
from typing import Any, Dict, List, Optional
import structlog
from pydantic import ValidationError 
from tenacity import RetryError
import requests

from core_domain.value_objects.timestamp import Timestamp


from application.ports.pipedrive_client_port import PipedriveClientPort
from application.ports.data_repository_port import DataRepositoryPort
from infrastructure.repository_impl.lookup_repositories import (
    UserRepository, PersonRepository, StageRepository, PipelineRepository, OrganizationRepository
)
from application.mappers import deal_mapper
from application.schemas.deal_schema import DealSchema

from infrastructure.monitoring.metrics import (
    etl_counter, etl_failure_counter, etl_duration_hist,
    records_processed_counter, memory_usage_gauge, batch_size_gauge,
    db_operation_duration_hist, etl_empty_batches_total,
    etl_batch_validation_errors_total, etl_skipped_batches_total,
)

log = structlog.get_logger(__name__)

class ETLService:
    \"\"\"
    Orchestrates the ETL process for Pipedrive deals, coordinating the API client,
    mappers, domain objects, data repository, and lookup repositories.
    \"\"\"

    def __init__(
        self,
        client: PipedriveClientPort,
        data_repository: DataRepositoryPort,
        user_repository: UserRepository,
        person_repository: PersonRepository,
        stage_repository: StageRepository,
        pipeline_repository: PipelineRepository,
        org_repository: OrganizationRepository,
        mapper_module: Any = deal_mapper, 
        batch_size: int = 1000
    ):
        self.client = client
        self.data_repository = data_repository
        self.user_repo = user_repository
        self.person_repo = person_repository
        self.stage_repo = stage_repository
        self.pipeline_repo = pipeline_repository
        self.org_repo = org_repository
        self.mapper = mapper_module
        self.process_batch_size = batch_size
        self.log = log.bind(service=\"ETLService\")

        try:
            self._custom_field_map = self.client.fetch_deal_fields_mapping()
            self._field_definitions = self._get_field_definitions_map()
            self._stage_id_to_col_map = self.data_repository.get_stage_id_to_column_map()
            self.log.info(\"ETL Service initialized with field maps and definitions.\")
        except Exception as init_err:
             self.log.error(\"Failed to fetch initial mappings/definitions for ETL Service\", error=str(init_err), exc_info=True)
             raise RuntimeError(\"ETL Service initialization failed: Could not fetch required mappings.\") from init_err

    def _get_field_definitions_map(self) -> Dict[str, Dict]:
        \"\"\"Fetches /dealFields and returns a map indexed by API hash key.\"\"\"
        try:
             all_fields_list = self.client.fetch_deal_fields() 
             if not all_fields_list:
                  self.log.warning(\"Received empty list from fetch_deal_fields\")
                  return {}
             return {field['key']: field for field in all_fields_list if 'key' in field}
        except AttributeError:
             self.log.error(\"PipedriveClientPort does not have fetch_deal_fields method.\")
             self.log.warning(\"Falling back to empty field definitions map due to missing client method.\")
             return {}
        except Exception as e:
             self.log.error(\"Failed to fetch or process deal field definitions\", error=str(e), exc_info=True)
             return {} 


    def _enrich_persistence_dicts(self, persistence_dicts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        \"\"\"
        Enriches persistence dictionaries with names fetched from lookup repositories.
        \"\"\"
        if not persistence_dicts:
            return []

        enriched_dicts = [d.copy() for d in persistence_dicts] 
        enrich_start_time = time.monotonic()
        log_ctx = self.log.bind(batch_size=len(persistence_dicts))
        log_ctx.debug(\"Starting enrichment of persistence dictionaries.\")

        try:
            # 1. Collect all unique IDs needed for lookups
            user_ids = set()
            person_ids = set()
            org_ids = set()
            stage_ids = set()
            pipeline_ids = set()

            for p_dict in enriched_dicts:
                 if p_dict.get(\"creator_user_id\"): user_ids.add(p_dict[\"creator_user_id\"])
                 if p_dict.get(\"owner_id\"): user_ids.add(p_dict[\"owner_id\"])
                 if p_dict.get(\"person_id\"): person_ids.add(p_dict[\"person_id\"])
                 if p_dict.get(\"org_id\"): org_ids.add(p_dict[\"org_id\"])
                 if p_dict.get(\"stage_id\"): stage_ids.add(p_dict[\"stage_id\"])
                 if p_dict.get(\"pipeline_id\"): pipeline_ids.add(p_dict[\"pipeline_id\"])

            # 2. Fetch name maps from lookup repositories concurrently? (For now, sequentially)
            user_map = self.user_repo.get_name_map_for_ids(user_ids) if user_ids else {}
            person_map = self.person_repo.get_name_map_for_ids(person_ids) if person_ids else {}
            org_map = self.org_repo.get_name_map_for_ids(org_ids) if org_ids else {}
            stage_map = self.stage_repo.get_name_map_for_ids(stage_ids) if stage_ids else {}
            pipeline_map = self.pipeline_repo.get_name_map_for_ids(pipeline_ids) if pipeline_ids else {}

            # 3. Apply names to the dictionaries
            for p_dict in enriched_dicts:
                 p_dict[\"creator_user_name\"] = user_map.get(p_dict.get(\"creator_user_id\"))
                 p_dict[\"owner_name\"] = user_map.get(p_dict.get(\"owner_id\"))
                 p_dict[\"person_name\"] = person_map.get(p_dict.get(\"person_id\"))
                 p_dict[\"org_name\"] = org_map.get(p_dict.get(\"org_id\"))
                 p_dict[\"stage_name\"] = stage_map.get(p_dict.get(\"stage_id\"))
                 p_dict[\"pipeline_name\"] = pipeline_map.get(p_dict.get(\"pipeline_id\"))

            duration = time.monotonic() - enrich_start_time
            log_ctx.debug(\"Enrichment completed.\", duration_sec=f\"{duration:.3f}s\")
            return enriched_dicts

        except Exception as e:
             log_ctx.error(\"Failed during persistence dictionary enrichment\", error=str(e), exc_info=True)
             return persistence_dicts 


    def run_etl(self, flow_type: str = \"sync\") -> Dict[str, Any]:
        \"\"\"
        Executes the main ETL flow: Fetch, Map (API->Schema->Domain->Persistence), Enrich, Load.
        \"\"\"
        run_start_time = time.monotonic()
        run_start_utc = datetime.now(timezone.utc)
        if not tracemalloc.is_tracing(): tracemalloc.start()
        etl_counter.labels(flow_type=flow_type).inc()
        run_log = self.log.bind(run_start_iso=run_start_utc.isoformat(), flow_type=flow_type)

        # Initialize results and counters
        result = {
            \"status\": \"started\", \"total_fetched\": 0, \"total_schema_valid\": 0,
            \"total_domain_mapped\": 0, \"total_enriched\": 0, \"total_loaded\": 0,
            \"total_schema_failed\": 0, \"total_domain_failed\": 0, \"total_enrich_failed\": 0,
            \"total_load_failed\": 0, \"start_time\": run_start_utc.isoformat(),
            \"end_time\": None, \"duration_seconds\": 0, \"message\": \"ETL process initiated.\",
            \"peak_memory_mb\": 0, \"last_processed_timestamp\": None
        }
        total_fetched = 0
        total_schema_valid = 0
        total_domain_mapped = 0
        total_loaded = 0
        total_schema_failed = 0
        total_domain_failed = 0
        total_load_failed = 0

        latest_update_time_in_run: Optional[datetime] = None
        run_log.info(f\"Starting ETL run ({flow_type})\")

        try:
            last_timestamp_str = self.client.get_last_timestamp()
            run_log.info(\"Fetching deals stream from Pipedrive\", updated_since=last_timestamp_str)
            deal_stream_iterator = self.client.fetch_all_deals_stream(updated_since=last_timestamp_str)

            batch_num = 0
            records_for_processing_batch: List[Dict] = []
            run_log.info(\"Starting ETL batch processing loop\", batch_size=self.process_batch_size)

            for api_record in deal_stream_iterator:
                total_fetched += 1
                records_for_processing_batch.append(api_record)

                update_time_str = api_record.get(\"update_time\")
                if update_time_str:
                    try:
                        current_record_time = Timestamp(datetime.fromisoformat(update_time_str.replace('Z', '+00:00'))).value
                        if latest_update_time_in_run is None or current_record_time > latest_update_time_in_run:
                            latest_update_time_in_run = current_record_time
                    except Exception:
                         run_log.warning(\"Could not parse update_time from fetched record\", record_id=api_record.get(\"id\"), time_str=update_time_str)

                # --- Process Batch ---
                if len(records_for_processing_batch) >= self.process_batch_size:
                    batch_num += 1
                    api_batch = records_for_processing_batch
                    records_for_processing_batch = []
                    batch_log = run_log.bind(batch_num=batch_num, batch_size=len(api_batch))
                    batch_log.info(\"Processing ETL batch\")
                    batch_start_time = time.monotonic()
                    batch_size_gauge.labels(flow_type=flow_type).set(len(api_batch))

                    processed_info = self._process_and_load_batch(
                        api_batch, batch_log, flow_type
                    )

                    # Update overall counters from batch results
                    total_schema_valid += processed_info[\"schema_valid\"]
                    total_domain_mapped += processed_info[\"domain_mapped\"]
                    total_loaded += processed_info[\"loaded\"]
                    total_schema_failed += processed_info[\"schema_failed\"]
                    total_domain_failed += processed_info[\"domain_failed\"]
                    total_load_failed += processed_info[\"load_failed\"]

                    batch_duration = time.monotonic() - batch_start_time
                    batch_log.info(\"ETL Batch processing complete\", duration_sec=f\"{batch_duration:.3f}s\", **processed_info)

            # --- Process Final Partial Batch ---
            if records_for_processing_batch:
                batch_num += 1
                api_batch = records_for_processing_batch
                batch_log = run_log.bind(batch_num=batch_num, batch_size=len(api_batch))
                batch_log.info(\"Processing final ETL batch\")
                batch_start_time = time.monotonic()
                batch_size_gauge.labels(flow_type=flow_type).set(len(api_batch))

                processed_info = self._process_and_load_batch(
                    api_batch, batch_log, flow_type
                )

                total_schema_valid += processed_info[\"schema_valid\"]
                total_domain_mapped += processed_info[\"domain_mapped\"]
                total_loaded += processed_info[\"loaded\"]
                total_schema_failed += processed_info[\"schema_failed\"]
                total_domain_failed += processed_info[\"domain_failed\"]
                total_load_failed += processed_info[\"load_failed\"]

                batch_duration = time.monotonic() - batch_start_time
                batch_log.info(\"Final ETL Batch processing complete\", duration_sec=f\"{batch_duration:.3f}s\", **processed_info)

            # --- Finalization ---
            run_log.info(\"ETL stream processing finished.\")
            result[\"status\"] = \"success\" if total_load_failed == 0 and total_schema_failed == 0 and total_domain_failed == 0 else \"completed_with_errors\"
            result[\"message\"] = f\"ETL run {result['status']}.\"

            if latest_update_time_in_run and total_loaded > 0:
                try:
                    buffered_time = latest_update_time_in_run + timedelta(seconds=1)
                    iso_timestamp = buffered_time.strftime('%Y-%m-%dT%H:%M:%SZ')
                    self.client.update_last_timestamp(iso_timestamp)
                    result[\"last_processed_timestamp\"] = iso_timestamp
                    run_log.info(\"Last processed timestamp updated in client/cache\", timestamp=iso_timestamp)
                except Exception as cache_err:
                    run_log.error(\"Failed to update last timestamp\", error=str(cache_err), exc_info=True)
            elif total_fetched > 0 and total_loaded == 0:
                 run_log.warning(\"ETL fetched records but loaded none. Last timestamp NOT updated.\")
                 result[\"status\"] = \"error\"
                 result[\"message\"] = \"ETL fetched records but loaded none.\"
            elif total_fetched == 0:
                 run_log.info(\"No new records fetched. Last timestamp not updated.\")
                 result[\"status\"] = \"success_no_new_data\"
                 result[\"message\"] = \"ETL completed successfully, no new data fetched.\"


        except (requests.exceptions.RequestException, RetryError) as api_err:
             run_log.critical(\"ETL failed due to API Client error\", error=str(api_err), exc_info=True)
             result[\"status\"] = \"error\"
             result[\"message\"] = f\"ETL failed: API Client Error - {api_err}\"
             etl_failure_counter.labels(flow_type=flow_type).inc()
        except Exception as e:
            run_log.critical(\"Critical ETL failure during run_etl\", error=str(e), exc_info=True)
            result[\"status\"] = \"error\"
            result[\"message\"] = f\"Critical ETL failure: {e}\"
            etl_failure_counter.labels(flow_type=flow_type).inc()
            result[\"total_load_failed\"] = total_load_failed + (total_fetched - total_schema_valid - total_domain_failed - total_loaded)

        finally:
            run_end_time = time.monotonic()
            run_end_utc = datetime.now(timezone.utc)
            duration = run_end_time - run_start_time
            result[\"duration_seconds\"] = round(duration, 3)
            result[\"end_time\"] = run_end_utc.isoformat()
            etl_duration_hist.labels(flow_type=flow_type).observe(duration)

            # Update final counts in result
            result[\"total_fetched\"] = total_fetched
            result[\"total_schema_valid\"] = total_schema_valid
            result[\"total_domain_mapped\"] = total_domain_mapped
            result[\"total_loaded\"] = total_loaded
            result[\"total_schema_failed\"] = total_schema_failed
            result[\"total_domain_failed\"] = total_domain_failed
            result[\"total_load_failed\"] = total_load_failed

            peak_mem_mb = 0
            if tracemalloc.is_tracing():
                try:
                    current_mem, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop()
                    tracemalloc.clear_traces()
                    run_log.debug(f\"Final Memory: Current={current_mem/1e6:.2f}MB, Peak={peak_mem_mb:.2f}MB\")
                except Exception as trace_err:
                    run_log.error(\"Error stopping tracemalloc\", error=str(trace_err))
                    if tracemalloc.is_tracing(): tracemalloc.stop() ; tracemalloc.clear_traces()
            result[\"peak_memory_mb\"] = peak_mem_mb
            if peak_mem_mb > 0:
                 memory_usage_gauge.labels(flow_type=flow_type).set(peak_mem_mb)

            log_level = run_log.info if result[\"status\"].startswith(\"success\") else run_log.error
            log_level(\"ETL run summary\", **result)
            print(f\"ETL_COMPLETION_METRICS: {json.dumps(result)}\")

            return result


    def _process_and_load_batch(
        self,
        api_batch: List[Dict],
        batch_log: structlog.BoundLoggerBase,
        flow_type: str
    ) -> Dict[str, int]:
        \"\"\"Maps, enriches, and loads a single batch of data.\"\"\"
        batch_results = {
            \"schema_valid\": 0, \"domain_mapped\": 0, \"loaded\": 0,
            \"schema_failed\": 0, \"domain_failed\": 0, \"load_failed\": 0
        }
        if not api_batch:
            etl_empty_batches_total.labels(flow_type=flow_type).inc()
            return batch_results

        # --- 1. API Dict -> Schema ---
        schema_batch: List[DealSchema] = []
        for api_record in api_batch:
            try:
                schema = self.mapper.map_api_dict_to_schema(api_record)
                schema_batch.append(schema)
            except ValidationError as schema_err:
                 batch_results[\"schema_failed\"] += 1
                 etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"schema\").inc()
                 batch_log.warning(\"Schema validation failed\", record_id=api_record.get(\"id\"), error=str(schema_err))
            except Exception as e:
                 batch_results[\"schema_failed\"] += 1
                 etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"schema_unexpected\").inc()
                 batch_log.error(\"Unexpected error during schema mapping\", record_id=api_record.get(\"id\"), error=str(e), exc_info=True)
        batch_results[\"schema_valid\"] = len(schema_batch)

        if not schema_batch:
             batch_log.warning(\"No records passed schema validation in this batch.\")
             etl_skipped_batches_total.labels(flow_type=flow_type).inc()
             return batch_results

        # --- 2. Schema -> Domain ---
        domain_batch: List[Any] = []
        for schema in schema_batch:
            try:
                domain_entity = self.mapper.map_schema_to_domain(schema, self._custom_field_map, self._field_definitions)
                domain_batch.append(domain_entity)
            except ValueError as domain_err:
                 batch_results[\"domain_failed\"] += 1
                 etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"domain\").inc()
                 batch_log.warning(\"Domain mapping failed\", record_id=schema.id, error=str(domain_err))
            except Exception as e:
                 batch_results[\"domain_failed\"] += 1
                 etl_batch_validation_errors_total.labels(flow_type=flow_type, error_type=\"domain_unexpected\").inc()
                 batch_log.error(\"Unexpected error during domain mapping\", record_id=schema.id, error=str(e), exc_info=True)
        batch_results[\"domain_mapped\"] = len(domain_batch)

        if not domain_batch:
             batch_log.warning(\"No records passed domain mapping in this batch.\")
             etl_skipped_batches_total.labels(flow_type=flow_type).inc()
             return batch_results

        # --- 3. Domain -> Persistence Dict ---
        persistence_batch: List[Dict[str, Any]] = []
        map_to_persist_failed = 0
        for domain_entity in domain_batch:
             try:
                 persistence_dict = self.mapper.map_domain_to_persistence_dict(domain_entity)
                 persistence_batch.append(persistence_dict)
             except Exception as e:
                  map_to_persist_failed += 1
                  batch_log.error(\"Failed mapping Domain to Persistence Dict\", record_id=domain_entity.id, error=str(e), exc_info=True)
        batch_results[\"domain_failed\"] += map_to_persist_failed
        batch_results[\"domain_mapped\"] -= map_to_persist_failed 

        if not persistence_batch:
              batch_log.warning(\"No records successfully mapped to persistence dictionary.\")
              etl_skipped_batches_total.labels(flow_type=flow_type).inc()
              return batch_results

        # --- 4. Enrich Persistence Dicts with Names ---
        enriched_batch = self._enrich_persistence_dicts(persistence_batch)

        # --- 5. Load to Repository ---
        if enriched_batch:
            try:
                with db_operation_duration_hist.labels(operation='upsert_deals').time():
                    self.data_repository.save_data_upsert(enriched_batch)
                batch_results[\"loaded\"] = len(enriched_batch)
                records_processed_counter.labels(flow_type=flow_type).inc(len(enriched_batch))
            except Exception as load_err:
                batch_results[\"load_failed\"] = len(enriched_batch)
                etl_failure_counter.labels(flow_type=flow_type).inc(len(enriched_batch))
                batch_log.error(\"Repository save_data_upsert failed for batch\", error=str(load_err), record_count=len(enriched_batch), exc_info=True)
        else:
             batch_log.warning(\"No enriched records to load for this batch (enrichment or previous steps failed).\")
             etl_skipped_batches_total.labels(flow_type=flow_type).inc()


        return batch_results


    def run_retroactive_backfill(self, deal_ids: List[str]) -> Dict[str, Any]:
        \"\"\"
        Executes the stage history backfill process for a given list of Deal IDs.
        Fetches changelogs and updates the history columns in the data repository.
        \"\"\"
        if not deal_ids:
            self.log.info(\"No deal IDs provided for retroactive backfill.\")
            return {\"status\": \"skipped\", \"processed_deals\": 0, \"updates_applied\": 0, \"api_errors\": 0, \"processing_errors\": 0}

        run_start_time = time.monotonic()
        run_log = self.log.bind(flow_type=\"backfill\", batch_deal_count=len(deal_ids))
        run_log.info(\"Starting retroactive stage history backfill run.\")

        processed_deals_count = 0
        updates_to_apply: List[Dict[str, Any]] = []
        api_errors = 0
        processing_errors = 0
        db_update_errors = 0

        if not self._stage_id_to_col_map:
             run_log.error(\"Stage ID to history column map is empty. Cannot perform backfill.\")
             return {\"status\": \"error\", \"message\": \"Stage ID map is empty.\", \"processed_deals\": 0, \"updates_applied\": 0, \"api_errors\": 0, \"processing_errors\": 1}

        for deal_id_str in deal_ids:
            deal_log = run_log.bind(deal_id=deal_id_str)
            try:
                deal_id = int(deal_id_str) 
                deal_log.debug(\"Fetching changelog for deal.\")

                changelog = self.client.fetch_deal_changelog(deal_id)
                processed_deals_count += 1

                if not changelog:
                    deal_log.debug(\"No changelog entries found.\")
                    continue

                stage_changes = []
                for entry in changelog:
                    if entry.get('field_key') == 'stage_id':
                        ts_str = entry.get('time') 
                        new_stage_id_val = entry.get('new_value')
                        if ts_str and new_stage_id_val is not None:
                             try:
                                 ts = datetime.strptime(ts_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
                                 new_stage_id = int(new_stage_id_val)
                                 stage_changes.append({'timestamp': ts, 'stage_id': new_stage_id})
                             except (ValueError, TypeError):
                                 deal_log.warning(\"Could not parse stage change entry\", entry=entry)
                                 processing_errors += 1

                if not stage_changes:
                    deal_log.debug(\"No valid 'stage_id' changes parsed from changelog.\")
                    continue

                # Find the earliest timestamp for each stage ID encountered
                stage_changes.sort(key=lambda x: x['timestamp'])
                first_entry_times: Dict[int, datetime] = {}
                for change in stage_changes:
                    stage_id = change['stage_id']
                    timestamp = change['timestamp']
                    if stage_id not in first_entry_times:
                        first_entry_times[stage_id] = timestamp

                # Prepare updates for the database
                for stage_id, first_timestamp in first_entry_times.items():
                    column_name = self._stage_id_to_col_map.get(stage_id)
                    if column_name:
                        updates_to_apply.append({
                            'deal_id': deal_id_str, 
                            'stage_column': column_name,
                            'timestamp': first_timestamp 
                        })
                    else:
                        deal_log.warning(\"Stage ID from changelog not found in current stage map\", stage_id=stage_id)
                        processing_errors += 1 

            except (requests.exceptions.RequestException, RetryError) as api_err:
                 api_errors += 1
                 deal_log.error(\"API error fetching changelog\", error=str(api_err))
            except ValueError:
                 processing_errors += 1
                 deal_log.error(\"Invalid deal_id format\", deal_id_str=deal_id_str)
            except Exception as e:
                 processing_errors += 1
                 deal_log.error(\"Error processing changelog\", error=str(e), exc_info=True)

        # Apply collected updates to the database
        updates_applied_count = 0
        if updates_to_apply:
            run_log.info(\"Applying stage history updates to database\", update_count=len(updates_to_apply))
            try:
                self.data_repository.update_stage_history(updates_to_apply)
                updates_applied_count = len(updates_to_apply)
            except Exception as db_err:
                db_update_errors += 1
                run_log.error(\"Failed during database update_stage_history\", error=str(db_err), exc_info=True)
        else:
            run_log.info(\"No stage history updates generated from this batch.\")

        duration = time.monotonic() - run_start_time
        final_status = \"success\"
        if api_errors > 0 or processing_errors > 0 or db_update_errors > 0:
             final_status = \"completed_with_errors\"
        run_log.info(\"Retroactive backfill run finished.\", status=final_status, duration_sec=f\"{duration:.3f}s\")

        return {
            \"status\": final_status,
            \"processed_deals\": processed_deals_count,
            \"updates_generated\": len(updates_to_apply), 
            \"updates_applied_attempted\": updates_applied_count,
            \"api_errors\": api_errors,
            \"processing_errors\": processing_errors + db_update_errors, 
            \"duration_seconds\": round(duration, 3)
        }

    # --- run_etl_with_data method (kept for experiments) ---
    def run_etl_with_data(self, data: List[Dict], batch_size: int, flow_type: str = \"experiment\") -> Dict:
        \"\"\" Executes ETL using provided data list instead of API stream, for testing/experiments. \"\"\"
        run_start_time = time.monotonic()
        run_start_utc = datetime.now(timezone.utc)
        if not tracemalloc.is_tracing(): tracemalloc.start()
        run_log = self.log.bind(run_start_iso=run_start_utc.isoformat(), flow_type=flow_type, batch_size=batch_size)
        run_log.info(\"Starting ETL run with provided data\", data_size=len(data))

        # Initialize results and counters
        result = {
            \"status\": \"started\", \"total_fetched\": len(data), \"total_schema_valid\": 0,
            \"total_domain_mapped\": 0, \"total_loaded\": 0, \"total_schema_failed\": 0,
            \"total_domain_failed\": 0, \"total_load_failed\": 0, \"start_time\": run_start_utc.isoformat(),
            \"end_time\": None, \"duration_seconds\": 0, \"message\": \"Experiment initiated.\",
            \"peak_memory_mb\": 0, \"success_rate\": 0.0, \"data_quality_issues\": 0 
        }
        total_schema_valid = 0
        total_domain_mapped = 0
        total_loaded = 0
        total_schema_failed = 0
        total_domain_failed = 0
        total_load_failed = 0

        try:
            num_batches = (len(data) + batch_size - 1) // batch_size
            for i in range(num_batches):
                batch_start_idx = i * batch_size
                batch_end_idx = batch_start_idx + batch_size
                api_batch = data[batch_start_idx:batch_end_idx]

                batch_log = run_log.bind(batch_num=i+1, batch_size=len(api_batch))
                batch_log.info(\"Processing experiment batch\")

                processed_info = self._process_and_load_batch(
                    api_batch, batch_log, flow_type
                )

                total_schema_valid += processed_info[\"schema_valid\"]
                total_domain_mapped += processed_info[\"domain_mapped\"]
                total_loaded += processed_info[\"loaded\"]
                total_schema_failed += processed_info[\"schema_failed\"]
                total_domain_failed += processed_info[\"domain_failed\"]
                total_load_failed += processed_info[\"load_failed\"]

            result[\"status\"] = \"success\" if total_load_failed == 0 and total_schema_failed == 0 and total_domain_failed == 0 else \"completed_with_errors\"
            result[\"message\"] = f\"Experiment {result['status']}.\"
            if len(data) > 0:
                result[\"success_rate\"] = total_loaded / len(data)

        except Exception as e:
             run_log.critical(\"Critical failure during experiment run\", error=str(e), exc_info=True)
             result[\"status\"] = \"error\"
             result[\"message\"] = f\"Critical experiment failure: {e}\"
             result[\"total_load_failed\"] = total_load_failed + (len(data) - total_loaded)

        finally:
            run_end_time = time.monotonic()
            run_end_utc = datetime.now(timezone.utc)
            duration = run_end_time - run_start_time
            result[\"duration_seconds\"] = round(duration, 3)
            result[\"end_time\"] = run_end_utc.isoformat()

            result[\"total_schema_valid\"] = total_schema_valid
            result[\"total_domain_mapped\"] = total_domain_mapped
            result[\"total_loaded\"] = total_loaded
            result[\"total_schema_failed\"] = total_schema_failed
            result[\"total_domain_failed\"] = total_domain_failed
            result[\"total_load_failed\"] = total_load_failed

            peak_mem_mb = 0
            if tracemalloc.is_tracing():
                try:
                    _, peak_mem = tracemalloc.get_traced_memory()
                    peak_mem_mb = round(peak_mem / 1e6, 2)
                    tracemalloc.stop(); tracemalloc.clear_traces()
                except Exception: pass
            result[\"peak_memory_mb\"] = peak_mem_mb

            log_level = run_log.info if result[\"status\"].startswith(\"success\") else run_log.error
            log_level(\"Experiment run summary\", **result)
            print(f\"ETL_COMPLETION_METRICS: {json.dumps(result)}\") 

            return result"}
,
{"path": "application/ports/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/ports/data_repository_port.py", "encoding": "utf-8", "content": "from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Set

class DataRepositoryPort(ABC):
    \"\"\"
    Port for interacting with the main Pipedrive deal data storage.
    \"\"\"

    @abstractmethod
    def initialize_schema(self) -> None:
        \"\"\"Ensures the main data table schema exists and is up-to-date.\"\"\"
        pass

    @abstractmethod
    def save_data_upsert(self, data: List[Dict[str, Any]]) -> None:
        \"\"\"
        Saves a batch of processed data using an upsert strategy.
        Expects data as a list of dictionaries ready for persistence.
        Handles dynamic schema updates for new columns present in the data.
        \"\"\"
        pass

    # --- Methods specific to pipedrive_data table ---
    @abstractmethod
    def get_record_by_id(self, record_id: str) -> Optional[Dict]:
        \"\"\"Fetches a single complete record by its ID.\"\"\"
        pass

    @abstractmethod
    def get_all_ids(self) -> Set[str]:
        \"\"\"Returns a set of all existing deal IDs.\"\"\"
        pass

    @abstractmethod
    def count_records(self) -> int:
        \"\"\"Counts the total number of records in the main data table.\"\"\"
        pass

    # --- Methods specific to Stage History Backfill ---
    @abstractmethod
    def get_deals_needing_history_backfill(self, limit: int) -> List[str]:
        \"\"\"Finds deal IDs potentially needing stage history backfill.\"\"\"
        pass

    @abstractmethod
    def update_stage_history(self, updates: List[Dict[str, Any]]) -> None:
        \"\"\"Applies stage history timestamp updates.\"\"\"
        pass

    @abstractmethod
    def count_deals_needing_backfill(self) -> int:
        \"\"\"Counts how many deals potentially need backfill.\"\"\"
        pass"}
,
{"path": "application/ports/pipedrive_client_port.py", "encoding": "utf-8", "content": "from abc import ABC, abstractmethod
from typing import Dict, List, Generator, Optional

class PipedriveClientPort(ABC):

    @abstractmethod
    def fetch_deal_fields_mapping(self) -> Dict[str, str]:
        pass

    @abstractmethod
    def fetch_deal_fields(self) -> List[Dict]: 
        pass

    @abstractmethod
    def fetch_all_deals_stream(self, updated_since: Optional[str] = None, items_limit: Optional[int] = None) -> Generator[Dict, None, None]: # Generator de Dict, n\u00e3o List[Dict]
        pass

    @abstractmethod
    def update_last_timestamp(self, new_timestamp: str) -> None:
        pass

    @abstractmethod
    def get_last_timestamp(self) -> Optional[str]:
        pass

    @abstractmethod
    def fetch_deal_changelog(self, deal_id: int) -> List[Dict]:
        pass

    @abstractmethod
    def fetch_all_users(self) -> List[Dict]:
        pass

    @abstractmethod
    def fetch_all_persons(self) -> List[Dict]:
        pass

    @abstractmethod
    def fetch_all_stages_details(self) -> List[Dict]:
        pass

    @abstractmethod
    def fetch_all_pipelines(self) -> List[Dict]:
        pass

    @abstractmethod
    def fetch_all_organizations(self) -> List[Dict]:
        pass"}
,
{"path": "application/utils/replace_nan_with_none_recursive.py", "encoding": "utf-8", "content": "import pandas as pd
import numpy as np

def replace_nan_with_none_recursive(obj):
    if isinstance(obj, dict):
        return {k: replace_nan_with_none_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_nan_with_none_recursive(elem) for elem in obj]
    elif isinstance(obj, float) and np.isnan(obj):
        return None
    elif pd.isna(obj): 
         return None
    else:
        return obj
"}
,
{"path": "application/utils/column_utils.py", "encoding": "utf-8", "content": "from typing import Any, Dict
import unicodedata
import re
import logging

log = logging.getLogger(__name__)

def normalize_column_name(name: str) -> str:
    if not isinstance(name, str):
        return \"\"
    name = name.lower()
    try:
        name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')
    except Exception:
        pass
    name = re.sub(r'[^\w_]+', '_', name)
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    if name and name[0].isdigit():
        name = '_' + name
    return name or \"_invalid_normalized_name\"

ADDRESS_COMPONENT_SUFFIX_MAP = {
    'street_number': 'numero_da_casa',
    'route': 'nome_da_rua',
    'sublocality': 'distrito_sub_localidade',
    'locality': 'cidade_municipio_vila_localidade',
    'admin_area_level_1': 'estado',
    'admin_area_level_2': 'regiao',
    'country': 'pais',
    'postal_code': 'cep_codigo_postal',
    'latitude': 'latitude',
    'longitude': 'longitude',
    # 'formatted_address' mapeado para a coluna principal
}

# Adicione um conjunto de chaves que indicam um campo de endere\u00e7o
ADDRESS_INDICATOR_KEYS = {'formatted_address', 'locality', 'country', 'postal_code'}

def flatten_custom_fields(custom_fields: Dict[str, Any], repo_custom_mapping: Dict[str, str]) -> Dict[str, Any]:
    \"\"\"
    Achata os campos personalizados, tratando campos de endere\u00e7o de forma especial
    para extrair seus componentes em colunas separadas.
    \"\"\"
    flat_dict = {}

    for api_key, normalized_name in repo_custom_mapping.items():
        field_data = custom_fields.get(api_key)

        if normalized_name not in flat_dict:
            flat_dict[normalized_name] = None

        if isinstance(field_data, dict):
            is_likely_address = any(key in field_data for key in ADDRESS_INDICATOR_KEYS)

            if is_likely_address:
                log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as address.\")

                # 1. Pega o valor principal 
                main_address_value = field_data.get('formatted_address') or field_data.get('value')
                flat_dict[normalized_name] = main_address_value

                # 2. Extrai os componentes
                for component_key, suffix in ADDRESS_COMPONENT_SUFFIX_MAP.items():
                    subcol_name = f\"{normalized_name}_{suffix}\"
                    component_value = field_data.get(component_key)
                    flat_dict[subcol_name] = component_value

                expected_address_cols = {f\"{normalized_name}_{suffix}\" for suffix in ADDRESS_COMPONENT_SUFFIX_MAP.values()}
                for key in flat_dict.keys():
                     if key.startswith(normalized_name + \"_\") and key not in expected_address_cols:
                          if key not in flat_dict:
                               flat_dict[key] = None


            else:
                log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as generic dictionary.\")
                flat_dict[normalized_name] = field_data.get('value')

        elif field_data is not None:
            log.debug(f\"Processing field '{normalized_name}' (API Key: {api_key}) as simple value.\")
            flat_dict[normalized_name] = field_data

    return flat_dict"}
,
{"path": "application/utils/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "application/use_cases/process_pipedrive_data.py", "encoding": "utf-8", "content": "from application.services.etl_service import ETLService
from typing import Dict

def run_pipedrive_etl_use_case(etl_service: ETLService) -> Dict[str, object]:
    \"\"\"
    Use case that processes Pipedrive data using the ETL service.

    Parameters:
        etl_service (ETLService): instance of the ETL service.

    Returns:
        Dict: A dictionary containing the results of the ETL run.
    \"\"\"
    result = etl_service.run_etl()
    return result"}
,
{"path": "application/use_cases/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "README.md", "encoding": "utf-8", "content": "# Pipeboard - Integra\u00e7\u00e3o Pipedrive + Metabase

![Status](https://img.shields.io/badge/status-em%20execu%C3%A7%C3%A3o-green)
![License](https://img.shields.io/badge/license-Propriet%C3%A1rio-red)
![Python](https://img.shields.io/badge/python-3.10+-blue)
![Kubernetes](https://img.shields.io/badge/k8s-minikube%20%7C%20prod%20ready-orange)
![Prefect](https://img.shields.io/badge/prefect-v3.x-brightgreen)
![PostgreSQL](https://img.shields.io/badge/postgres-15+-blueviolet)

---

## \ud83d\udca1 Vis\u00e3o Geral

O **Pipeboard** \u00e9 uma solu\u00e7\u00e3o robusta e extens\u00edvel de ETL desenvolvida internamente na **Pavcob**, com o objetivo de extrair, transformar e carregar dados do **Pipedrive** para um banco **PostgreSQL** com visualiza\u00e7\u00e3o no **Metabase**. Utiliza o **Prefect** para orquestra\u00e7\u00e3o e o **Kubernetes** para escalabilidade. Conta com monitoramento por **Prometheus/Grafana**, logging estruturado e cache com **Redis**.

Este reposit\u00f3rio foi projetado para ser acess\u00edvel para qualquer desenvolvedor interno da Pavcob, com execu\u00e7\u00e3o automatizada e estrutura de pastas intuitiva. Tudo pode ser reproduzido localmente com Minikube.

---

## \ud83c\udf0d Prop\u00f3sito do Projeto

- Centralizar dados do Pipedrive para BI.
- Garantir fluxo incremental e com toler\u00e2ncia a falhas.
- Executar ETL robusto com m\u00e9tricas, logs e cache.
- Permitir escala futura e manuten\u00e7\u00e3o simplificada.

---

## \ud83d\ude80 Como Executar (Resumo)

```bash
# 1. Clonar o projeto
$ git clone git@gitlab.pavcob.internal/pipeboard.git && cd pipeboard

# 2. Definir vari\u00e1veis de ambiente (.env)
$ cp .env.example .env && nano .env

# 3. Exportar token do GitHub (usado para pull dos fluxos Prefect)
$ export GITHUB_PAT=\"<seu-token-aqui>\"

# 4. Executar o deploy automatizado
$ chmod +x run_project.sh
$ ./run_project.sh
```

---

## \ud83e\uddf3 Infraestrutura Provisionada

- PostgreSQL 15
- Redis 6
- Prefect Orion + Agent
- Prometheus + Pushgateway
- Grafana
- Metabase
- Metrics Server customizado

---

## \u2696\ufe0f Tecnologias e Arquitetura

- **Python 3.10+** com **Poetry**
- **Prefect v3** para orquestra\u00e7\u00e3o
- **Kubernetes + Minikube** (com suporte a ambientes cloud)
- **PostgreSQL** para armazenamento estruturado
- **Redis** para cache incremental
- **Prometheus + Pushgateway** para m\u00e9tricas
- **Grafana** e **Metabase** para visualiza\u00e7\u00e3o

---

## \ud83d\udd0d Estrutura do Projeto

```
pipeboard/
\u251c\u2500\u2500 application/         # C\u00f3digo de neg\u00f3cio e ETLService
\u251c\u2500\u2500 infrastructure/      # Reposit\u00f3rios, clients API, cache, DB
\u251c\u2500\u2500 flows/               # Fluxos Prefect com @flow
\u251c\u2500\u2500 scripts/             # Scripts auxiliares para registrar blocos Prefect
\u251c\u2500\u2500 run_project.sh       # Script principal de execu\u00e7\u00e3o (autom\u00e1tico)
\u251c\u2500\u2500 prefect.yaml         # Defini\u00e7\u00e3o dos Deployments Prefect
\u251c\u2500\u2500 .env                 # Segredos e configura\u00e7\u00e3o (criar manualmente)
\u2514\u2500\u2500 pipedrive_metabase_integration.yaml  # Manifests K8s
```

---

## \ud83d\udd14 Observabilidade e Monitoramento

- **Prometheus:** Coleta m\u00e9tricas customizadas do ETL e Pushgateway.
- **Grafana:** Visualiza\u00e7\u00e3o de dashboards de ETL, batch, mem\u00f3ria, falhas.
- **Prefect UI:** Logs, schedules, execu\u00e7\u00f5es e status dos fluxos.
- **Logging:** Formatado com Structlog (JSON, contexto estruturado).

---

## \ud83c\udfc6 Diferenciais da Solu\u00e7\u00e3o

1. **Upsert via COPY + staging:** Alta performance e seguran\u00e7a de dados.
2. **Streaming e batching:** Mem\u00f3ria otimizada, mesmo com 1M+ registros.
3. **Campos customizados din\u00e2micos:** Schema auto-adapt\u00e1vel.
4. **Hist\u00f3rico de Stage:** Permite analisar pipeline ao longo do tempo.
5. **Cache Redis + API resiliente:** Busca incremental, circuit breaker.
6. **M\u00e9tricas granulares:** Lat\u00eancia, falhas, mem\u00f3ria, qualidade dos dados.
7. **Backfill robusto:** Para recuperar hist\u00f3rico retroativo sob demanda.
8. **Deploy 100% automatizado:** Com rollback e port-forward inclu\u00eddo.

---

## \ud83d\udd27 Manuten\u00e7\u00e3o e Acesso

- Para adicionar novos campos ou entidades:
  - Atualize os campos em `etl_service.py` e `deal_schema.py`
  - Rode `main_etl_flow` ou `backfill_stage_history_flow`
- Para execu\u00e7\u00f5es manuais:
  - Use a UI Prefect (http://localhost:4200)
  - Ou `prefect deployments run` via CLI

---

## \ud83e\udd1d Contato Interno

- **Respons\u00e1vel t\u00e9cnico:** Matheus Munhoz - mrschrodingers@gmail.com
- **Sugest\u00f5es:** Abra uma issue no reposit\u00f3rio interno GitLab

---"}
,
{"path": "pyproject.toml", "encoding": "utf-8", "content": "[tool.poetry]
name = \"pipedrive_metabase_integration\"
version = \"0.1.0\"
description = \"Integra\u00e7\u00e3o do Pipedrive com Metabase utilizando Airflow\"
authors = [\"Pavcob\"]
readme = [\"README.md\"]

[tool.poetry.dependencies]
python = \">=3.12, <3.13\"
prefect = {extras = [\"kubernetes\"], version = \"^3.3.2\"}
requests = \"2.32.3\"
python-decouple = \"3.8\"
python-dotenv = \"1.0.1\"
pydantic = '2.10.6'
prometheus-client = \"0.21.1\"
psycopg2  = '2.9.10'
httpx='0.28.1'
tenacity='9.0.0'
python-json-logger='3.3.0'
prefect-sqlalchemy = \"^0.5.2\"
pandas = \"^2.2.3\"
numpy = \"^2.2.4\"
structlog = \"^25.2.0\"
pybreaker = \"^1.3.0\"
psutil=\"7.0.0\"

[tool.poetry.group.dev.dependencies]
pytest = \"^7.2.0\"

[build-system]
requires = [\"poetry-core>=1.0.0\"]
build-backend = \"poetry.core.masonry.api\"
"}
,
{"path": "core_domain/entities/deal.py", "encoding": "utf-8", "content": "from decimal import Decimal, InvalidOperation
from time import timezone
from typing import Optional, Dict, List 
from datetime import date, datetime, timedelta

from core_domain.value_objects.identifiers import (
    DealId, UserId, PersonId, OrgId, StageId, PipelineId
)
from core_domain.value_objects.money import Money
from core_domain.value_objects.timestamp import Timestamp
from core_domain.value_objects.deal_status import DealStatus
from core_domain.value_objects.custom_field import CustomFieldKey, CustomFieldValue

class Deal:
    \"\"\"
    Represents a Deal in the Pipedrive domain.
    Holds the core attributes and custom fields.
    Entity identity is defined by `id`.
    \"\"\"
    def __init__(
        self,
        deal_id: DealId,
        title: str,
        status: DealStatus,
        value: Money,
        creator_user_id: UserId,
        owner_id: UserId,
        pipeline_id: PipelineId,
        stage_id: StageId,
        add_time: Timestamp,
        update_time: Timestamp,
        person_id: Optional[PersonId] = None,
        org_id: Optional[OrgId] = None,
        probability: Optional[Decimal] = None, 
        lost_reason: Optional[str] = None,
        visible_to: Optional[str] = None, 
        close_time: Optional[Timestamp] = None,
        won_time: Optional[Timestamp] = None,
        lost_time: Optional[Timestamp] = None,
        expected_close_date: Optional[date] = None, 
        custom_fields: Optional[Dict[CustomFieldKey, CustomFieldValue]] = None,
    ):
        # --- Core Attributes & Identity ---
        self.id: DealId = deal_id
        self.title: str = title if title is not None else \"\"
        self.status: DealStatus = status
        self.value: Money = value

        # --- Timestamps ---
        self.add_time: Timestamp = add_time
        self.update_time: Timestamp = update_time
        self.close_time: Optional[Timestamp] = close_time
        self.won_time: Optional[Timestamp] = won_time
        self.lost_time: Optional[Timestamp] = lost_time
        self.expected_close_date: Optional[date] = expected_close_date

        # --- Relationships (IDs) ---
        self.creator_user_id: UserId = creator_user_id
        self.owner_id: UserId = owner_id
        self.person_id: Optional[PersonId] = person_id
        self.org_id: Optional[OrgId] = org_id
        self.pipeline_id: PipelineId = pipeline_id
        self.stage_id: StageId = stage_id

        # --- Other Attributes ---
        self.probability: Optional[Decimal] = self._validate_probability(probability)
        self.lost_reason: Optional[str] = lost_reason
        self.visible_to: Optional[str] = visible_to

        # --- Custom Fields ---
        self.custom_fields: Dict[CustomFieldKey, CustomFieldValue] = custom_fields if custom_fields is not None else {}

    def _validate_probability(self, probability: Optional[float | Decimal]) -> Optional[Decimal]:
        \"\"\" Ensures probability is a Decimal between 0 and 100, or None. \"\"\"
        if probability is None:
            return None
        try:
            prob_decimal = Decimal(probability)
            if not (Decimal(0) <= prob_decimal <= Decimal(100)):
                 print(f\"Warning: Probability {prob_decimal} out of range [0, 100] for Deal {self.id}\")
                 pass 
            return prob_decimal
        except (InvalidOperation, TypeError, ValueError):
             print(f\"Warning: Could not convert probability '{probability}' to Decimal for Deal {self.id}\")
             return None 

    # --- Potential Business Logic Methods ---
    def is_likely_to_close(self) -> bool:
        \"\"\" Example business rule based on probability \"\"\"
        return self.probability is not None and self.probability > 75

    def time_in_pipeline(self, current_time: Optional[datetime] = None) -> timedelta:
        \"\"\" Calculates how long the deal has been open. \"\"\"
        if self.status.is_closed():
            effective_end_time = self.close_time or self.update_time
            return effective_end_time.value - self.add_time.value
        else:
            now = Timestamp(current_time or datetime.now(timezone.utc))
            return now.value - self.add_time.value

    # --- Equality and Representation ---
    def __eq__(self, other):
        \"\"\"Entities are equal if their IDs are equal.\"\"\"
        if isinstance(other, Deal):
            return self.id == other.id
        return False

    def __hash__(self):
        \"\"\"Entities are hashable based on their ID.\"\"\"
        return hash(self.id)

    def __repr__(self):
        return f\"<Deal(id={self.id}, title='{self.title[:30]}...', status={self.status})>\""}
,
{"path": "core_domain/entities/__init__.py", "encoding": "utf-8", "content": "from .deal import Deal"}
,
{"path": "core_domain/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "core_domain/value_objects/deal_status.py", "encoding": "utf-8", "content": "from dataclasses import dataclass
from enum import Enum

class DealStatusOption(Enum):
    OPEN = \"open\"
    WON = \"won\"
    LOST = \"lost\"
    DELETED = \"deleted\"
    UNKNOWN = \"unknown\" 

    @classmethod
    def from_string(cls, status_str: str | None) -> 'DealStatusOption':
        if status_str is None:
            return cls.UNKNOWN
        for status in cls:
            if status.value == status_str.lower():
                return status
        return cls.UNKNOWN

@dataclass(frozen=True)
class DealStatus:
    \"\"\"Represents the status of a Deal using an Enum.\"\"\"
    value: DealStatusOption

    def __init__(self, status_str: str | None):
        object.__setattr__(self, 'value', DealStatusOption.from_string(status_str))

    def __str__(self) -> str:
        return self.value.value

    def is_open(self) -> bool:
        return self.value == DealStatusOption.OPEN

    def is_closed(self) -> bool:
        return self.value in (DealStatusOption.WON, DealStatusOption.LOST)"}
,
{"path": "core_domain/value_objects/custom_field.py", "encoding": "utf-8", "content": "from dataclasses import dataclass
from typing import Any

@dataclass(frozen=True)
class CustomFieldKey:
    \"\"\"Represents the *normalized* key of a custom field.\"\"\"
    value: str

    def __post_init__(self):
        if not self.value or not isinstance(self.value, str):
            raise ValueError(\"CustomFieldKey value must be a non-empty string.\")

    def __str__(self) -> str:
        return self.value

@dataclass(frozen=True)
class CustomFieldValue:
    \"\"\"Represents the value of a custom field, optionally storing its original type.\"\"\"
    value: Any
    pipedrive_type: str | None = None

    def __str__(self) -> str:
         if isinstance(self.value, dict):
             return str(self.value.get('formatted_address', self.value))
         return str(self.value)"}
,
{"path": "core_domain/value_objects/money.py", "encoding": "utf-8", "content": "from dataclasses import dataclass
from decimal import Decimal, InvalidOperation

@dataclass(frozen=True)
class Money:
    \"\"\"Represents a monetary value with amount and currency.\"\"\"
    amount: Decimal
    currency: str 

    def __post_init__(self):
        if not isinstance(self.amount, Decimal):
            try:
                object.__setattr__(self, 'amount', Decimal(self.amount))
            except (InvalidOperation, TypeError, ValueError) as e:
                raise ValueError(f\"Invalid amount for Money: {self.amount}. Must be Decimal compatible.\") from e
        if not self.currency or not isinstance(self.currency, str):
            raise ValueError(\"Currency must be a non-empty string.\")

    def __str__(self) -> str:
        return f\"{self.amount:.2f} {self.currency}\""}
,
{"path": "core_domain/value_objects/identifiers.py", "encoding": "utf-8", "content": "from dataclasses import dataclass

@dataclass(frozen=True)
class BaseId:
    \"\"\"Base class for numeric identifiers. Ensures value is positive.\"\"\"
    value: int

    def __post_init__(self):
        if self.value <= 0:
            raise ValueError(f\"{self.__class__.__name__} value must be positive.\")

    def __int__(self) -> int:
        return self.value

    def __str__(self) -> str:
        return str(self.value)

class DealId(BaseId): pass
class UserId(BaseId): pass
class PersonId(BaseId): pass
class OrgId(BaseId): pass 
class StageId(BaseId): pass
class PipelineId(BaseId): pass
"}
,
{"path": "core_domain/value_objects/timestamp.py", "encoding": "utf-8", "content": "from dataclasses import dataclass
from datetime import datetime, timezone, date

@dataclass(frozen=True)
class Timestamp:
    \"\"\"Represents a specific point in time, ensuring UTC timezone.\"\"\"
    value: datetime

    def __post_init__(self):
        if not isinstance(self.value, datetime):
            raise ValueError(\"Timestamp value must be a datetime object.\")
        if self.value.tzinfo is None:
            object.__setattr__(self, 'value', self.value.replace(tzinfo=timezone.utc))
        elif self.value.tzinfo != timezone.utc:
            object.__setattr__(self, 'value', self.value.astimezone(timezone.utc))

    @property
    def date(self) -> date:
        return self.value.date()

    def isoformat(self) -> str:
        return self.value.isoformat()

    def __str__(self) -> str:
        return self.isoformat()"}
,
{"path": "core_domain/value_objects/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "run_project.sh", "encoding": "utf-8", "content": "#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

##############################
# Configura\u00e7\u00f5es
##############################
declare -A IMAGES=(
    [\"etl\"]=\"pipedrive_metabase_integration-etl:latest\"
    [\"orion\"]=\"pipedrive_metabase_integration-prefect-orion:latest\"
)

RESOURCE_TIMEOUT=1800 # 30min
MINUTES=$((RESOURCE_TIMEOUT / 60))
MINIKUBE_CPUS=4
MINIKUBE_MEMORY=10240 # 10GB
MINIKUBE_DRIVER=docker
CLEANUP_NAMESPACES=\"default,kube-system\"
PREFECT_YAML_FILE=\"./infrastructure/k8s/prefect.yaml\"

##############################
# Fun\u00e7\u00f5es Auxiliares
##############################
log() {
    local LEVEL=\"$1\"
    local MESSAGE=\"$2\"
    printf \"[%s] [%s] %s\n\" \"$(date '+%Y-%m-%d %H:%M:%S')\" \"${LEVEL^^}\" \"${MESSAGE}\"
}

fail() {
    log \"error\" \"$1\"
    exit 1
}

cleanup() {
    log \"info\" \"Limpando recursos tempor\u00e1rios...\"
    pkill -P $$ || true
    kubectl delete pods --field-selector=status.phase!=Running,status.phase!=Pending -n default --wait=false || true
}

check_dependencies() {
    declare -A DEPS=(
        [\"docker\"]=\"Docker\"
        [\"kubectl\"]=\"Kubernetes CLI\"
        [\"minikube\"]=\"Minikube\"
        [\"curl\"]=\"cURL\"
        [\"prefect\"]=\"Prefect CLI\" 
    )

    for cmd in \"${!DEPS[@]}\"; do
        if ! command -v \"${cmd}\" &> /dev/null; then
            fail \"${DEPS[$cmd]} n\u00e3o encontrado. Por favor instale/configure primeiro.\"
        fi
    done
    if [[ ! -f \"${PREFECT_YAML_FILE}\" ]]; then
        fail \"Arquivo de configura\u00e7\u00e3o Prefect n\u00e3o encontrado em: ${PREFECT_YAML_FILE}\"
    fi
}


##############################
# Fun\u00e7\u00f5es Principais
##############################
stop_resources() {
    log \"info\" \"Parando todos os recursos...\"

    declare -a KILL_PIDS=(
        \"kubectl port-forward svc/db\"       
        \"kubectl port-forward svc/prefect-orion\"
        \"kubectl port-forward svc/metabase\"
        \"kubectl port-forward svc/grafana\"
    )

    for pid_pattern in \"${KILL_PIDS[@]}\"; do
        pkill -f \"${pid_pattern}\" || true
    done

    kubectl delete --all deployments,services,jobs,hpa,pvc,secrets,configmaps -n default --wait=true --ignore-not-found=true
    minikube addons disable metrics-server || true

    log \"success\" \"Recursos parados com sucesso.\"
    exit 0
}

start_minikube() {
    local STATUS
    STATUS=$(minikube status -o json | jq -r '.Host' 2>/dev/null || echo \"Error\")

    if [[ \"${STATUS}\" != \"Running\" ]]; then
        log \"info\" \"Iniciando Minikube com ${MINIKUBE_CPUS} CPUs e ${MINIKUBE_MEMORY}MB RAM...\"
        minikube start \
            --driver=\"${MINIKUBE_DRIVER}\" \
            --cpus=\"${MINIKUBE_CPUS}\" \
            --memory=\"${MINIKUBE_MEMORY}\" \
            --addons=metrics-server \
            --embed-certs=true \
            --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \
            --extra-config=apiserver.service-account-issuer=kubernetes/serviceaccount || fail \"Falha ao iniciar Minikube\"
    else
        log \"info\" \"Minikube j\u00e1 est\u00e1 rodando. Reutilizando inst\u00e2ncia existente.\"
    fi

    eval \"$(minikube docker-env)\"
    kubectl config use-context minikube
}

build_images() {
    log \"info\" \"Construindo imagens com BuildKit...\"

    export DOCKER_BUILDKIT=1
    for image in \"${!IMAGES[@]}\"; do
        local TAG=\"${IMAGES[$image]}\"
        local BUILD_CONTEXT=\".\"

        if [[ \"${image}\" == \"orion\" ]]; then
            BUILD_CONTEXT=\"./infrastructure/prefect/orion\"
        fi

        log \"info\" \"Building ${TAG} from context ${BUILD_CONTEXT}\"
        docker build \
            --progress=plain \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --cache-from \"${TAG}\" \
            -t \"${TAG}\" \
            \"${BUILD_CONTEXT}\" || fail \"Falha ao construir imagem ${TAG}\"
    done
}

# --- Fun\u00e7\u00e3o para aplicar Deployments Prefect ---
deploy_prefect_flows() {
    local work_pool_name=\"kubernetes-pool\"
    local secret_block_name=\"github-access-token\"
    local db_block_name=\"postgres-pool\"
    local redis_block_name=\"redis-cache\"

    local prefect_api_url_ip=\"http://127.0.0.1:4200/api\"

    # --- Verifica\u00e7\u00e3o Pr\u00e9via da Conex\u00e3o com Orion ---
    log \"debug\" \"Verificando acesso \u00e0 API Prefect via IP (${prefect_api_url_ip}) antes de criar blocos...\"
    sleep 5
    local health_check_url=\"${prefect_api_url_ip%/api}/health\"
    local attempt=0
    local max_attempts=5
    while ! curl --fail --max-time 5 -s \"${health_check_url}\" > /dev/null; do
         attempt=$((attempt + 1))
         if [[ $attempt -ge $max_attempts ]]; then
              log \"error\" \"Falha no teste de conex\u00e3o com ${health_check_url} ap\u00f3s ${max_attempts} tentativas.\"
              fail \"N\u00e3o foi poss\u00edvel conectar ao Prefect API via ${prefect_api_url_ip}. Imposs\u00edvel continuar.\"
         fi
         log \"debug\" \"Tentativa ${attempt}/${max_attempts}: Falha ao conectar a ${health_check_url}. Aguardando 5s...\"
         sleep 5
    done
     log \"debug\" \"Teste de conex\u00e3o com ${health_check_url} bem-sucedido.\"

    # --- Verifica\u00e7\u00e3o das Vari\u00e1veis de Ambiente para Blocos ---
    log \"info\" \"Verificando vari\u00e1veis de ambiente para cria\u00e7\u00e3o dos blocos...\"
    local required_block_vars=(\"GITHUB_PAT\" \"POSTGRES_USER\" \"POSTGRES_PASSWORD\" \"POSTGRES_DB\")
    local missing_vars_msg=\"\"
    for var in \"${required_block_vars[@]}\"; do
         if [[ -z \"${!var:-}\" ]]; then
              missing_vars_msg+=\"- ${var}\n\"
         fi
    done
    if [[ -n \"$missing_vars_msg\" ]]; then
        log \"error\" \"Vari\u00e1veis de ambiente obrigat\u00f3rias para criar blocos n\u00e3o definidas:\n${missing_vars_msg}\"
        fail \"Exporte as vari\u00e1veis necess\u00e1rias antes de rodar o script.\"
    else
        log \"info\" \"Vari\u00e1veis de ambiente para blocos parecem estar definidas.\"
    fi

    # --- Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o dos Blocos Core E INFRA via Script Python ---
    log \"info\" \"Executando script para criar/atualizar Blocos Prefect (${secret_block_name}, ${db_block_name}, ${redis_block_name}, k8s-jobs)...\" 
    export PREFECT_API_URL=\"${prefect_api_url_ip}\"
    log \"debug\" \"PREFECT_API_URL configurada como ${PREFECT_API_URL} para script Python e CLI.\"

    # Executa o script que agora tamb\u00e9m cria os blocos K8sJob
    if ! python scripts/create_or_update_core_blocks.py; then
         fail \"Falha ao executar create_or_update_core_blocks.py. Verifique os logs do script Python acima.\"
    fi
    log \"info\" \"Blocos Prefect (incluindo K8sJob) criados/atualizados com sucesso.\"

    # --- Cria\u00e7\u00e3o do Work Pool ---
    log \"info\" \"Verificando/Criando Work Pool Prefect: ${work_pool_name}...\"

    if ! prefect work-pool inspect \"${work_pool_name}\" > /dev/null 2>&1; then
         log \"info\" \"Criando work pool '${work_pool_name}'...\"
         if prefect work-pool create --type kubernetes \"${work_pool_name}\" --overwrite; then
             log \"info\" \"Work Pool '${work_pool_name}' criado com sucesso.\"
         else
             fail \"Falha ao criar work pool '${work_pool_name}'.\"
         fi
    else
         log \"info\" \"Work Pool '${work_pool_name}' j\u00e1 existe.\"
    fi

    # --- Aplica\u00e7\u00e3o dos Deployments ---
    log \"info\" \"Aplicando/Atualizando Deployments Prefect a partir do ${PREFECT_YAML_FILE}...\"
    if prefect deploy --all; then 
        log \"success\" \"Deployments Prefect aplicados com sucesso via CLI.\"
    else
        fail \"Falha ao aplicar deployments Prefect via CLI. Verifique os logs do comando.\"
    fi
}

deploy_infra() {
    log \"info\" \"Aplicando configura\u00e7\u00f5es base...\"

    kubectl apply -f infrastructure/k8s/observability-config.yaml --server-side=true || fail \"Falha ao aplicar observability-config.yaml\"
    kubectl apply -f infrastructure/k8s/observability-config.yaml || fail \"Falha ao aplicar db-secrets.yaml\"
    if kubectl get pvc pgdata-pvc > /dev/null 2>&1 ; then
       log \"info\" \"PVC pgdata-pvc j\u00e1 existe.\"
    else
       kubectl apply -f infrastructure/k8s/persistent-volume-claim.yaml || fail \"Falha ao aplicar persistent-volume-claim.yaml\"
    fi

    kubectl apply -f infrastructure/k8s/prometheus.yml|| fail \"Falha ao aplicar prometheus.yml\"
    kubectl apply -f infrastructure/k8s/pushgateway.yaml || fail \"Falha ao aplicar pushgateway.yaml\"

    log \"info\" \"Criando/Atualizando secret 'app-secrets' a partir do .env\"
    kubectl create secret generic app-secrets \
        --from-env-file=.env \
        --dry-run=client \
        -o yaml | kubectl apply -f - || fail \"Falha ao criar/atualizar app-secrets\"

    log \"info\" \"Aplicando manifesto principal (sem o Job 'etl' est\u00e1tico)...\"
    kubectl apply -f infrastructure/k8s/pipedrive_metabase_integration.yaml || fail \"Falha ao aplicar pipedrive_metabase_integration.yaml\"
}

wait_for_rollout() {
    local DEPLOYMENTS=(\"prefect-orion\" \"prefect-agent\" \"pushgateway\" \"prometheus-deployment\" \"redis\" \"db\" \"grafana\")

    for dep in \"${DEPLOYMENTS[@]}\"; do
        if kubectl get deployment \"${dep}\" > /dev/null 2>&1; then
             log \"info\" \"Aguardando rollout do deployment/${dep}...\"
             kubectl rollout status \"deployment/${dep}\" \
                 --timeout=\"${RESOURCE_TIMEOUT}s\" \
                 --watch=true || fail \"Timeout ou erro aguardando rollout do deployment/${dep}\"
        else
             log \"warning\" \"Deployment ${dep} n\u00e3o encontrado, pulando espera do rollout.\"
        fi
    done
    log \"info\" \"Rollout de todos os deployments principais conclu\u00eddo.\"
}

setup_port_forwarding() {
     declare -A PORTS=(
        [\"prefect-orion\"]=\"4200\"
        [\"db\"]=\"5432\"
        [\"metabase\"]=\"3000\"  
        [\"grafana\"]=\"3015\"        
    )

     log \"info\" \"Iniciando port-forward para os services...\"

     for svc in \"${!PORTS[@]}\"; do
         local PORT=\"${PORTS[$svc]}\"
         if kubectl get service \"${svc}\" > /dev/null 2>&1; then
             log \"warn\" \"Removendo port-fowards existentes para ${svc}...\"
             pkill -f \"kubectl port-forward svc/${svc} ${PORT}:${PORT}\" || true
             log \"info\" \"Iniciando port-forward para ${svc} na porta ${PORT}\"
             kubectl port-forward \"svc/${svc}\" \"${PORT}:${PORT}\" &
             sleep 2 
         else
             log \"warning\" \"Servi\u00e7o ${svc} n\u00e3o encontrado, pulando port-forward.\"
         fi
     done
     log \"info\" \"Aguardando alguns segundos para estabilizar os port-forwards...\"
     sleep 10

     log \"info\" \"Port-forwards iniciados (se os servi\u00e7os existirem).\"
     for svc in \"${!PORTS[@]}\"; do
         local PORT=\"${PORTS[$svc]}\"
         if pgrep -f \"kubectl port-forward svc/${svc} ${PORT}:${PORT}\" > /dev/null; then
             log \"info\" \"[${svc}] Iniciado na porta ${PORT}, acesse: http://localhost:${PORT}\"
         fi
     done
}

##############################
# Fluxo Principal
##############################
trap cleanup EXIT # Adiciona trap para limpeza em caso de erro ou interrup\u00e7\u00e3o

case \"${1:-}\" in
    stop)
        stop_resources
        ;;
    *)
        check_dependencies
        start_minikube
        build_images
        deploy_infra            # Aplica K8s manifests (inclui Agent)
        wait_for_rollout        # Espera Deployments (inclui Agent)
        setup_port_forwarding   # Habilita acesso local (ex: Orion UI)
        deploy_prefect_flows    

        log \"success\" \"\u2705 Implanta\u00e7\u00e3o da infraestrutura conclu\u00edda!\"
        log \"info\" \"Prefect Agent est\u00e1 rodando.\"
        log \"info\" \"Fluxos agendados (como o Sync) ser\u00e3o iniciados pelo agente.\"
        log \"info\" \"Fluxos sob demanda (como o Backfill inicial) precisam ser iniciados via UI/API ou Automa\u00e7\u00e3o.\"
        log \"info\" \"Monitore em http://localhost:4200 (Orion)\"
        log \"info\" \"Para manter port-forwards ativos, este script precisa continuar rodando ou execute os port-forwards separadamente.\"
        # Mant\u00e9m o script rodando para manter os port-forwards vivos (Ctrl+C para parar)
        wait
        ;;
esac"}
,
{"path": "tests/test_use_cases.py", "encoding": "base64", "content": ""}
,
{"path": "tests/__init__.py", "encoding": "base64", "content": ""}
,
{"path": "tests/test_infrastructure.py", "encoding": "base64", "content": ""}
,
{"path": ".env", "encoding": "utf-8", "content": "##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DE BANCO DE DADOS                     #
##############################################################################
POSTGRES_DB=pipedrive_metabase_integration_db
POSTGRES_PASSWORD=pipedrive_metabase_integration_db
POSTGRES_USER=pipedrive_metabase_integration_db
POSTGRES_PORT=5432
POSTGRES_HOST=db
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

##############################################################################
#                             CONFIGURA\u00c7\u00d5ES DO REDIS                         #
##############################################################################
REDIS_URL=\"redis://redis:6379/0\"

##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DE APLICACAO                          #
##############################################################################
API_PORT=8080
APP_METRICS_PORT=8082
WEB_SERVER_PORT=8081
FERNET_KEY=\"fH7No6yfy6yhb3fPzgKURIMvA+c5hMnZSD8czvL1S/o=\"

##############################################################################
#                        CONFIGURA\u00c7\u00d5ES DO PIPEDRIVE                          #
##############################################################################
PIPEDRIVE_API_KEY=bb0cf5c38584a41fd54a90503e5767bcd9ed381c

##############################################################################
#                         CONFIGURA\u00c7\u00d5ES DO PREFECT                           #
##############################################################################
PREFECT_PORT=4200
PREFECT_API_URL=http://prefect-orion:4200/api

##############################################################################
#                         CONFIGURA\u00c7\u00d5ES DO GRAFANA                           #
##############################################################################
GF_SERVER_HTTP_PORT=3015
PUSHGATEWAY_ADDRESS=pushgateway:9091"}
]
